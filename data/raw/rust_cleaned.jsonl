{"diff": "a / src/components/storage.rs \n  b / src/components/storage.rs \n@@ -23,6 +23,7 @@ use linear_block_store::InMemBlockStore; \n pub(crate) type Storage = InMemStorage<Block>; \n +#[derive(Debug)] \n pub(crate) enum Event<S: StorageType> \n where \n <S::BlockStore as BlockStoreType>::Block: Debug, \n @@ -37,21 +38,6 @@ where \n }, \n } \n -impl<S: StorageType> Debug for Event<S> { \n - fn fmt(&self, formatter: &mut Formatter) -> fmt::Result { \n - match self { \n - Event::PutBlock { block, .. } => { \n - write!(formatter, \"Event::PutBlock {{ block: {:?} }}\", block) \n - } \n - Event::GetBlock { block_hash, .. } => write!( \n - formatter, \n - \"Event::GetBlock {{ block_hash: {:?} }}\", \n - block_hash \n - ), \n - } \n - } \n -} \n - \n impl<S: StorageType> Display for Event<S> { \n fn fmt(&self, formatter: &mut Formatter) -> fmt::Result { \n match self {", "msg": "Derive `Debug` automatically for storage events"}
{"diff": "a / src/reactor/validator.rs \n  b / src/reactor/validator.rs \n@@ -97,9 +97,9 @@ impl reactor::Reactor for Reactor { \n impl Display for Event { \n fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result { \n match self { \n - Event::Network(ev) => write!(f, \"Network[{}]\", ev), \n - Event::Storage(ev) => write!(f, \"Storage[{}]\", ev), \n - Event::StorageConsumer(ev) => write!(f, \"StorageConsumer[{}]\", ev), \n + Event::Network(ev) => write!(f, \"network: {}\", ev), \n + Event::Storage(ev) => write!(f, \"storage: {}\", ev), \n + Event::StorageConsumer(ev) => write!(f, \"storage_consumer: {}\", ev), \n } \n } \n }", "msg": "Bring `Display` impl for `reactor::validator::Event` in line with convention"}
{"diff": "a / src/components/consensus/deploy_buffer.rs \n  b / src/components/consensus/deploy_buffer.rs \n@@ -102,6 +102,8 @@ impl DeployBuffer { \n pub(crate) fn finalized_block(&mut self, block: BlockHash) { \n if let Some(deploys) = self.processed.remove(&block) { \n + self.collected_deploys \n + .retain(|deploy_hash, _| !deploys.contains_key(deploy_hash)); \n self.finalized.insert(block, deploys); \n } else { \n panic!(\"finalized block that hasn't been processed!\");", "msg": "remove finalized deploys from collected_deploys"}
{"diff": "a / node/src/effect.rs \n  b / node/src/effect.rs \n//! of the component that talks to the client and deserializes the incoming deploy though, which \n //! considers the deploy no longer its concern after it has returned an announcement effect. \n //! \n -//! **Requests** are some of the most complex effects, they represent a question of a component for \n -//! another component, for which it eventually expects an answer. \n +//! **Requests** are complex effects that are used when a component needs something from \n +//! outside of itself (typically to be provided by another component); a request requires an \n +//! eventual response. \n //! \n //! A request **must** have a `Responder` field, which a handler of a request **must** call at \n //! some point. Failing to do so will result in a resource leak.", "msg": "Provide a better description of \"Requests\" in `effects` docs."}
{"diff": "a / node/src/components/api_server.rs \n  b / node/src/components/api_server.rs \nmod config; \n mod event; \n -use std::{error::Error as StdError, net::SocketAddr, str}; \n +use std::{borrow::Cow, error::Error as StdError, net::SocketAddr, str}; \n use bytes::Bytes; \n use futures::FutureExt; \n @@ -158,8 +158,13 @@ where \n QueueKind::Api, \n ) \n .map(|text_opt| match text_opt { \n - Some(text) => Ok::<_, Rejection>(text), \n - None => todo!(), \n + Some(text) => { \n + Ok::<_, Rejection>(reply::with_status(Cow::from(text), StatusCode::OK)) \n + } \n + None => Ok(reply::with_status( \n + Cow::from(\"failed to collect metrics. sorry!\"), \n + StatusCode::INTERNAL_SERVER_ERROR, \n + )), \n }) \n });", "msg": "Return an HTTP 500 error if metrics collection failed"}
{"diff": "a / node/src/components/storage.rs \n  b / node/src/components/storage.rs \n@@ -161,9 +161,11 @@ where \n // Tell the requestor the result of storing the deploy. \n responder.respond(result).await; \n + if result.is_ok() { \n // Now that we have stored the deploy, we also want to announce it. \n effect_builder.announce_deploy_stored(deploy_id).await; \n } \n + } \n .ignore() \n } \n StorageRequest::GetDeploy {", "msg": "Only announce new deploys when they have been stored successfully"}
{"diff": "a / node/src/reactor/validator.rs \n  b / node/src/reactor/validator.rs \n@@ -10,7 +10,7 @@ use std::fmt::{self, Display, Formatter}; \n use derive_more::From; \n use rand::Rng; \n use serde::{Deserialize, Serialize}; \n -use tracing::warn; \n +use tracing::info; \n use crate::{ \n components::{ \n @@ -304,8 +304,15 @@ impl reactor::Reactor for Reactor { \n let event = deploy_gossiper::Event::DeployReceived { deploy }; \n self.dispatch_event(effect_builder, rng, Event::DeployGossiper(event)) \n } \n - Event::StorageAnnouncement(ann) => { \n - warn!(%ann, \"dropped storage announcement\"); \n + Event::StorageAnnouncement(StorageAnnouncement::StoredDeploy { \n + deploy_hash, \n + deploy_header, \n + }) => { \n + if self.deploy_buffer.add_deploy(deploy_hash, deploy_header) { \n + info!(\"Added deploy {} to the buffer.\", deploy_hash); \n + } else { \n + info!(\"Deploy {} rejected from the buffer.\", deploy_hash); \n + } \n Effects::new() \n } \n }", "msg": "Add stored deploys to the deploy buffer."}
{"diff": "a / node/src/components/deploy_buffer.rs \n  b / node/src/components/deploy_buffer.rs \n@@ -192,15 +192,21 @@ impl<REv> Component<REv> for DeployBuffer { \n max_dependencies, \n past, \n responder, \n - }) => responder \n - .respond(self.remaining_deploys( \n + }) => { \n + let deploys = self.remaining_deploys( \n current_instant, \n max_ttl, \n limits, \n max_dependencies, \n &past, \n - )) \n - .ignore(), \n + ); \n + // TODO: This is a temporary workaround because we don't call `added_block` yet. \n + // To avoid proposing the same deploys again, we remove them from the buffer. \n + for deploy in &deploys { \n + self.collected_deploys.remove(deploy); \n + } \n + responder.respond(deploys).ignore() \n + } \n } \n } \n }", "msg": "Remove deploys from the buffer when proposing them."}
{"diff": "a / node/src/components/consensus/highway_core/highway_testing.rs \n  b / node/src/components/consensus/highway_core/highway_testing.rs \n@@ -328,7 +328,14 @@ where \n .ok_or_else(|| TestRunError::NoConsensusValues)?; \n self.call_validator(&validator_id, |consensus| { \n - consensus.propose(consensus_value, block_context) \n + let mut effects = consensus.propose(consensus_value, block_context); \n + let additional_effects = match &*effects { \n + // We want to add the new vertex to creator's state immediately. \n + [Effect::NewVertex(vv)] => consensus.add_valid_vertex(vv.clone()), \n + _ => vec![], \n + }; \n + effects.extend(additional_effects); \n + effects \n })? \n } \n }", "msg": "NO-TICKET: Add new vertex immediately to creator's state."}
{"diff": "a / node/src/components/consensus/highway_core/highway_testing.rs \n  b / node/src/components/consensus/highway_core/highway_testing.rs \n@@ -792,13 +792,8 @@ impl<DS: DeliveryStrategy> HighwayTestHarnessBuilder<DS> { \n .unwrap_or_else(|| (weights_sum.0 - 1) / 3); \n // Local function creating an instance of `HighwayConsensus` for a single validator. \n - let highway_consensus = |(vid, secrets): ( \n - <TestContext as Context>::ValidatorId, \n - &mut HashMap< \n - <TestContext as Context>::ValidatorId, \n - <TestContext as Context>::ValidatorSecret, \n - >, \n - )| { \n + let highway_consensus = \n + |(vid, secrets): (ValidatorId, &mut HashMap<ValidatorId, TestSecret>)| { \n let v_sec = secrets.remove(&vid).expect(\"Secret key should exist.\"); \n let (highway, effects) = {", "msg": "Simplify type signatures."}
{"diff": "a / node/src/components/consensus/highway_core/highway_testing.rs \n  b / node/src/components/consensus/highway_core/highway_testing.rs \n@@ -73,9 +73,8 @@ impl HighwayMessage { \n let create_msg = |hwm: HighwayMessage| Message::new(creator, hwm); \n match self { \n - Timer(_) => TargetedMessage::new(create_msg(self), Target::SingleValidator(creator)), \n NewVertex(_) => TargetedMessage::new(create_msg(self), Target::AllExcept(creator)), \n - RequestBlock(_) => { \n + Timer(_) | RequestBlock(_) => { \n TargetedMessage::new(create_msg(self), Target::SingleValidator(creator)) \n } \n }", "msg": "Simplify the pattern match."}
{"diff": "a / node/src/components/block_validator/keyed_counter.rs \n  b / node/src/components/block_validator/keyed_counter.rs \n@@ -96,7 +96,7 @@ mod tests { \n } \n #[test] \n - #[should_panic] \n + #[should_panic(expected = \"tried to decrease in-flight to negative value\")] \n fn panics_on_underflow() { \n let mut kc = KeyedCounter::new(); \n assert_eq!(kc.inc(&'a'), 1); \n @@ -105,7 +105,7 @@ mod tests { \n } \n #[test] \n - #[should_panic] \n + #[should_panic(expected = \"tried to decrease in-flight to negative value\")] \n fn panics_on_immediate_underflow() { \n let mut kc = KeyedCounter::new(); \n kc.dec(&'a');", "msg": "Be more specific about how panics should occur in counter tests"}
{"diff": "a / node/src/components/consensus/highway_core/finality_detector/rewards.rs \n  b / node/src/components/consensus/highway_core/finality_detector/rewards.rs \n@@ -88,9 +88,9 @@ fn compute_rewards_for<C: Context>( \n state.params().reduced_block_reward() \n }; \n // Rewards are proportional to the quorum and to the validator's weight. \n - let num = u128::from(finality_factor) * u128::from(*quorum) * u128::from(*weight); \n + let num = u128::from(finality_factor) * u128::from(*quorum); \n let denom = u128::from(assigned_weight) * u128::from(state.params().total_weight()); \n - (num / denom) as u64 \n + ((num / denom) * u128::from(*weight)) as u64 \n }) \n .collect() \n }", "msg": "NO-TICK: Fix overflow error when computing rewards."}
{"diff": "a / grpc/test_support/src/internal/utils.rs \n  b / grpc/test_support/src/internal/utils.rs \n@@ -44,7 +44,11 @@ lazy_static! { \n .join(\"target\") \n .join(\"wasm32-unknown-unknown\") \n .join(\"release\"); \n - assert!(path.exists(), \"Rust WASM path {} does not exists\", path.display()); \n + assert!( \n + path.exists() || RUST_TOOL_WASM_PATH.exists(), \n + \"Rust Wasm path {} does not exists\", \n + path.display() \n + ); \n path \n }; \n // The location of compiled Wasm files if running from within the 'tests' crate generated by the", "msg": "NO-TICKET: allow missing Wasm path in tests for cargo-casperlabs as it uses a different one"}
{"diff": "a / node/src/effect/requests.rs \n  b / node/src/effect/requests.rs \n@@ -308,7 +308,7 @@ pub enum ApiRequest { \n /// Responder to call with the result. \n responder: Responder<Vec<DeployHash>>, \n }, \n - /// Return string formatted, prometheus compatible metrics or `None` if an error occured. \n + /// Return string formatted, prometheus compatible metrics or `None` if an error occurred. \n GetMetrics { \n /// Responder to call with the result. \n responder: Responder<Option<String>>,", "msg": "Dummy commit for CI. And fix spelling error.\nLuckily, there's always some \"occurence\" or \"occured\" when you need\nthem."}
{"diff": "a / node/src/reactor/joiner.rs \n  b / node/src/reactor/joiner.rs \n@@ -127,8 +127,9 @@ impl<R: Rng + CryptoRng + ?Sized> reactor::Reactor<R> for Reactor { \n } \n impl Reactor { \n - /// Deconstructs the reactor into config useful for creating a Validator reactor. Also shuts \n - /// down the network connection. \n + /// Deconstructs the reactor into config useful for creating a Validator reactor. Shuts down \n + /// the network, closing all incoming and outgoing connections, and frees up the listening \n + /// socket. \n pub async fn into_validator_config(self) -> ValidatorInitConfig { \n let (net, config) = ( \n self.net,", "msg": "Improve the docstring of into_validator_config"}
{"diff": "a / client/src/main.rs \n  b / client/src/main.rs \n@@ -76,6 +76,9 @@ async fn main() { \n (QueryState::NAME, Some(matches)) => QueryState::run(matches), \n (Keygen::NAME, Some(matches)) => Keygen::run(matches), \n (GenerateCompletion::NAME, Some(matches)) => GenerateCompletion::run(matches), \n - _ => panic!(\"You must choose a subcommand to execute\"), \n + _ => { \n + let _ = cli().print_long_help(); \n + println!(); \n + } \n } \n }", "msg": "have client print help if no args are passed"}
{"diff": "a / node/src/components/api_server.rs \n  b / node/src/components/api_server.rs \n@@ -263,7 +263,7 @@ where \n ); \n let status_feed = \n StatusFeed::new(last_finalized_block, peers, Some(chainspec_info)); \n - debug!(\"GetStatus --status_feed: {:?}\", status_feed); \n + info!(\"GetStatus --status_feed: {:?}\", status_feed); \n responder.respond(status_feed).await; \n } \n .ignore(),", "msg": "Changed logging from debug to info in get_status"}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -270,6 +270,19 @@ where \n highway.activate_validator(our_id, secret, timestamp.max(start_time)) \n } else { \n info!(\"not voting in era {}\", era_id.0); \n + if start_time >= self.node_start_time { \n + info!( \n + \"node was started at time {}, which is not earlier than the era start {}\", \n + self.node_start_time, start_time \n + ); \n + } else if min_end_time < timestamp { \n + info!( \n + \"era started too long ago ({}; earliest end {}), current timestamp {}\", \n + start_time, min_end_time, timestamp \n + ); \n + } else { \n + info!(\"not a validator; our ID: {}\", our_id); \n + } \n Vec::new() \n };", "msg": "Log in more detail if we are not voting in an era."}
{"diff": "a / node/src/components/consensus/protocols/highway.rs \n  b / node/src/components/consensus/protocols/highway.rs \n@@ -133,7 +133,7 @@ impl<I: NodeIdT, C: Context> HighwayProtocol<I, C> { \n ) -> Vec<CpResult<I, C>> { \n self.vertices_to_be_added_later \n .entry(future_timestamp) \n - .or_insert(vec![]) \n + .or_insert_with(Vec::new) \n .push((sender, pvv)); \n vec![ConsensusProtocolResult::ScheduleTimer(future_timestamp)] \n }", "msg": "Use .or_insert_with to make clippy pass"}
{"diff": "a / ci/casper_updater/src/main.rs \n  b / ci/casper_updater/src/main.rs \n@@ -40,6 +40,7 @@ mod regex_data; \n use std::{ \n env, \n path::{Path, PathBuf}, \n + process::Command, \n str::FromStr, \n }; \n @@ -202,4 +203,13 @@ fn main() { \n &*regex_data::grpc_cargo_casper::DEPENDENT_FILES, \n ); \n grpc_cargo_casper.update(); \n + \n + // Update Cargo.lock. \n + let status = Command::new(env!(\"CARGO\")) \n + .arg(\"generate-lockfile\") \n + .arg(\"--offline\") \n + .current_dir(root_dir()) \n + .status() \n + .expect(\"Failed to execute 'cargo generate-lockfile'\"); \n + assert!(status.success(), \"Failed to update Cargo.lock\"); \n }", "msg": "NO-TICKET: include Cargo.lock in updated files when using casper_updater tool"}
{"diff": "a / node/src/components/small_network.rs \n  b / node/src/components/small_network.rs \n@@ -436,11 +436,14 @@ where \n .peer_addr() \n .expect(\"should have peer address\"); \n - assert!( \n - self.pending.remove(&peer_address), \n - \"should always add outgoing connect attempts to pendings: {:?}\", \n - self \n + if !self.pending.remove(&peer_address) { \n + info!( \n + %peer_address, \n + \"{}: this peer's incoming connection has dropped, so don't establish an outgoing\", \n + self.our_id \n ); \n + return Effects::new(); \n + } \n // If we have connected to ourself, allow the connection to drop. \n if peer_id == self.our_id {", "msg": "NO-TICKET: avoid panic if an incoming connection drops while an outgoing is being established"}
{"diff": "a / node_macros/src/rust_type.rs \n  b / node_macros/src/rust_type.rs \n@@ -69,7 +69,7 @@ impl TryFrom<Type> for RustType { \n fn try_from(value: Type) -> core::result::Result<Self, Self::Error> { \n match value { \n Type::Path(type_path) => Ok(RustType(type_path.path)), \n - _ => Err(\"cannot convert to RustType\".to_string()), \n + broken => Err(format!(\"cannot convert {:?} input to RustType\", broken)), \n } \n } \n }", "msg": "Better error message in `RustType` conversion failure"}
{"diff": "a / node_macros/src/parse.rs \n  b / node_macros/src/parse.rs \n@@ -190,12 +190,8 @@ impl Parse for ReactorDefinition { \n Ok(ReactorDefinition { \n reactor_type_ident, \n - config_type: RustType::try_from(config.ty.as_ref().clone()).map_err(|err| { \n - syn::parse::Error::new( \n - Span::call_site(), // FIXME: Can we get a better span here? \n - err, \n - ) \n - })?, \n + config_type: RustType::try_from(config.ty.as_ref().clone()) \n + .map_err(|err| syn::parse::Error::new_spanned(config.ty, err))?, \n components, \n events, \n requests,", "msg": "Use config span for parse errors"}
{"diff": "a / node/src/components/consensus/highway_core/test_macros.rs \n  b / node/src/components/consensus/highway_core/test_macros.rs \n@@ -100,6 +100,12 @@ macro_rules! add_unit { \n /// Creates an endorsement of `vote` by `creator` and adds it to the state. \n macro_rules! endorse { \n + ($state: ident, $rng: ident, $vote: expr; $($creators: expr),*) => { \n + let creators = vec![$($creators.into()),*]; \n + for creator in creators.into_iter() { \n + endorse!($state, $rng, creator, $vote); \n + } \n + }; \n ($state: ident, $rng: ident, $creator: expr, $vote: expr) => { \n let endorsement: Endorsement<TestContext> = Endorsement::new($vote, ($creator)); \n let signature = TestSecret(($creator).0).sign(&endorsement.hash(), &mut $rng);", "msg": "Add helper macro to create multiple endorsements."}
{"diff": "a / node/src/components/consensus/highway_core/state.rs \n  b / node/src/components/consensus/highway_core/state.rs \n@@ -322,8 +322,8 @@ impl<C: Context> State<C> { \n unit: &C::Hash, \n v_ids: I, \n ) -> bool { \n - if let Some(sigs) = self.endorsements.get(unit) { \n - v_ids.into_iter().all(|v_id| sigs[*v_id].is_some()) \n + if self.endorsements.contains_key(unit) { \n + true // We have enough endorsements for this unit. \n } else if let Some(sigs) = self.incomplete_endorsements.get(unit) { \n v_ids.into_iter().all(|v_id| sigs.contains_key(v_id)) \n } else {", "msg": "Don't add more endorsements than necessary."}
{"diff": "a / node/src/components/consensus/protocols/highway/tests.rs \n  b / node/src/components/consensus/protocols/highway/tests.rs \n@@ -129,10 +129,15 @@ fn send_a_wire_unit_with_too_small_a_round_exp() { \n \"Invalid message is not message that was sent.\" \n ); \n assert_eq!(offending_sender, &sender, \"Unexpected sender.\"); \n - assert!(format!(\"{:?}\", err).starts_with( \n - \"The vertex contains an invalid unit: `The round length exponent is less than the minimum allowed by the chain-spec.`\"), \n + assert!( \n + format!(\"{:?}\", err).starts_with( \n + \"The vertex contains an invalid unit: `The round \\ \n + length exponent is less than the minimum allowed by \\ \n + the chain-spec.`\" \n + ), \n \"Error message did not start as expected: {:?}\", \n - err) \n + err \n + ) \n } \n Some(protocol_outcome) => panic!(\"Unexpected protocol outcome {:?}\", protocol_outcome), \n }", "msg": "Breaking up long string with escaped newlines"}
{"diff": "a / node/src/reactor/validator/tests.rs \n  b / node/src/reactor/validator/tests.rs \n@@ -67,8 +67,8 @@ impl TestChain { \n }) \n .collect(); \n // TODO: This is duplicated. Remove the `HighwayConfig` field. \n - // Make the genesis timestamp 30 seconds from now, to allow for all validators to start up. \n - chainspec.genesis.timestamp = Timestamp::now() + 30000.into(); \n + // Make the genesis timestamp 45 seconds from now, to allow for all validators to start up. \n + chainspec.genesis.timestamp = Timestamp::now() + 45000.into(); \n chainspec.genesis.highway_config.genesis_era_start_timestamp = chainspec.genesis.timestamp; \n chainspec.genesis.highway_config.minimum_era_height = 1;", "msg": "Delaying genesis-timestamp to allow more warm-up time"}
{"diff": "a / node/src/components/consensus/highway_core/highway_testing.rs \n  b / node/src/components/consensus/highway_core/highway_testing.rs \n@@ -244,7 +244,10 @@ impl HighwayValidator { \n // TODO: Don't send both messages to every peer. Add different \n // strategies. \n let mut wunit = swunit.wire_unit.clone(); \n - wunit.timestamp += 1.into(); \n + match wunit.value.as_mut() { \n + None => wunit.timestamp += 1.into(), \n + Some(v) => v.push(0), \n + } \n let secret = TestSecret(wunit.creator.0.into()); \n let swunit2 = SignedWireUnit::new(wunit, &secret, rng); \n vec![", "msg": "Make it more successful to equivocate."}
{"diff": "a / node/src/reactor/validator/tests.rs \n  b / node/src/reactor/validator/tests.rs \n@@ -103,6 +103,7 @@ impl TestChain { \n // Additionally set up storage in a temporary directory. \n let (storage_cfg, temp_dir) = storage::Config::default_for_tests(); \n + cfg.consensus.unit_hashes_folder = temp_dir.path().to_path_buf(); \n self.storages.push(temp_dir); \n cfg.storage = storage_cfg;", "msg": "Make simulated nodes use different dirs for unit hashes"}
{"diff": "a / node/src/components/block_executor.rs \n  b / node/src/components/block_executor.rs \n@@ -4,7 +4,7 @@ mod metrics; \n use std::{ \n collections::{BTreeMap, HashMap, VecDeque}, \n - convert::{Infallible, TryFrom}, \n + convert::{Infallible, TryInto}, \n fmt::Debug, \n }; \n @@ -549,9 +549,12 @@ impl<REv: ReactorEventT> Component<REv> for BlockExecutor { \n state.state_root_hash = post_state_hash.into(); \n let next_era_validators: BTreeMap<PublicKey, U512> = next_era_validators \n .into_iter() \n - .map(|(casper_types_public_key, weight)| { \n - let public_key = PublicKey::try_from(casper_types_public_key).expect(\"Could not convert casper_types public key into node public key\"); \n - (public_key, weight) \n + .filter_map(|(key, stake)| match key.try_into() { \n + Ok(key) => Some((key, stake)), \n + Err(error) => { \n + error!(%error, \"error converting the bonded key\"); \n + None \n + } \n }) \n .collect(); \n self.finalize_block_execution(", "msg": "Using `filter_map` for validator conversions"}
{"diff": "a / execution_engine/src/core/execution/executor.rs \n  b / execution_engine/src/core/execution/executor.rs \n@@ -39,7 +39,6 @@ macro_rules! on_fail_charge { \n Ok(res) => res, \n Err(e) => { \n let exec_err: Error = e.into(); \n - warn!(\"Execution failed: {:?}\", exec_err); \n return ExecutionResult::precondition_failure(exec_err.into()); \n } \n } \n @@ -49,7 +48,6 @@ macro_rules! on_fail_charge { \n Ok(res) => res, \n Err(e) => { \n let exec_err: Error = e.into(); \n - warn!(\"Execution failed: {:?}\", exec_err); \n return ExecutionResult::Failure { \n error: exec_err.into(), \n effect: Default::default(), \n @@ -64,7 +62,6 @@ macro_rules! on_fail_charge { \n Ok(res) => res, \n Err(e) => { \n let exec_err: Error = e.into(); \n - warn!(\"Execution failed: {:?}\", exec_err); \n return ExecutionResult::Failure { \n error: exec_err.into(), \n effect: $effect,", "msg": "NO-TICKET: Remove warning logs when smart contract execution fails."}
{"diff": "a / node/src/components/small_network.rs \n  b / node/src/components/small_network.rs \n@@ -581,7 +581,7 @@ where \n if let Some(outgoing) = self.outgoing.remove(&peer_id) { \n trace!(our_id=%self.our_id, %peer_id, \"removing peer from the outgoing connections\"); \n if add_to_blocklist { \n - info!(%peer_id, \"blacklisting peer\"); \n + info!(our_id=%self.our_id, %peer_id, \"blacklisting peer\"); \n self.blocklist.insert(outgoing.peer_address); \n } \n }", "msg": "NO-TICKET: Log our_id when we blacklist a peer."}
{"diff": "a / node/src/types/block.rs \n  b / node/src/types/block.rs \n@@ -834,6 +834,18 @@ impl Block { \n } \n } \n + /// Creates an instance of the block from the block header. \n + pub(crate) fn from_header(header: BlockHeader) -> Self { \n + let body = (); \n + let hash = header.hash(); \n + Block { \n + hash, \n + header, \n + body, \n + proofs: BTreeMap::new(), \n + } \n + } \n + \n pub(crate) fn header(&self) -> &BlockHeader { \n &self.header \n }", "msg": "Add a method to construct a block from its header."}
{"diff": "a / node/src/components/linear_chain_sync.rs \n  b / node/src/components/linear_chain_sync.rs \n@@ -549,7 +549,12 @@ where \n error!(%block_hash, \"could not download deploys from linear chain block.\"); \n panic!(\"Failed to download linear chain deploys.\") \n } \n - Some(peer) => fetch_block_deploys(effect_builder, peer, *block_header), \n + Some(peer) => { \n + let block_hash = (*block_header).hash(); \n + trace!(%block_hash, next_peer=%peer, \n + \"failed to download deploys from a peer. Trying next one\"); \n + fetch_block_deploys(effect_builder, peer, *block_header) \n + } \n }, \n Event::StartDownloadingDeploys => { \n // Start downloading deploys from the first block of the linear chain.", "msg": "NO-TICK: Log when we fail to download a deploys."}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -258,10 +258,10 @@ where \n info!(era = era_id.0, %our_id, \"not voting; not a validator\"); \n false \n } else if !self.finished_joining { \n - info!(era = era_id.0, \"not voting; still joining\"); \n + info!(era = era_id.0, %our_id, \"not voting; still joining\"); \n false \n } else { \n - info!(era = era_id.0, \"start voting\"); \n + info!(era = era_id.0, %our_id, \"start voting\"); \n true \n };", "msg": "NO-TICKET: More logging of public keys"}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -130,7 +130,7 @@ where \n let (root, config) = config.into_parts(); \n let secret_signing_key = Rc::new(config.secret_key_path.load(root)?); \n let public_signing_key = PublicKey::from(secret_signing_key.as_ref()); \n - info!(?public_signing_key, \"our own EraSupervisor pubkey\",); \n + info!(our_id = %public_signing_key, \"EraSupervisor pubkey\",); \n let bonded_eras: u64 = protocol_config.unbonding_delay - protocol_config.auction_delay; \n let metrics = ConsensusMetrics::new(registry) \n .expect(\"failure to setup and register ConsensusMetrics\");", "msg": "NO-TICKET: More consistent logging fo public key"}
{"diff": "a / node/src/components/network/tests_bulk_gossip.rs \n  b / node/src/components/network/tests_bulk_gossip.rs \n@@ -137,7 +137,10 @@ async fn send_large_message_across_network() { \n // This can, on a decent machine, be set to 30, 50, maybe even 100 nodes. The default is set to \n // 5 to avoid overloading CI. \n - let node_count: usize = 5; \n + let node_count: usize = std::env::var(\"TEST_NODE_COUNT\") \n + .expect(\"TEST_NODE_COUNT not set\") \n + .parse() \n + .expect(\"cannot parse TEST_NODE_COUNT\"); \n // Fully connecting a 20 node network takes ~ 3 seconds. This should be ample time for gossip \n // and connecting.", "msg": "Make `TEST_NODE_COUNT` configuration for number of nodes in test"}
{"diff": "a / node/src/components/network/tests_bulk_gossip.rs \n  b / node/src/components/network/tests_bulk_gossip.rs \n@@ -184,6 +184,10 @@ async fn send_large_message_across_network() { \n net.add_node_with_config(cfg, &mut rng).await.unwrap(); \n + // Hack to get network component to connect. This gives the libp2p thread (which is independent \n + // of cranking) a little time to bind to the socket. \n + std::thread::sleep(std::time::Duration::from_secs(2)); \n + \n // Create `node_count-1` additional node instances. \n for _ in 1..node_count { \n let cfg = TestReactorConfig {", "msg": "Add a little pause to allow for binding to the port"}
{"diff": "a / node/src/components/fetcher.rs \n  b / node/src/components/fetcher.rs \n@@ -6,7 +6,7 @@ use std::{collections::HashMap, convert::Infallible, fmt::Debug, time::Duration} \n use datasize::DataSize; \n use smallvec::smallvec; \n -use tracing::{debug, error}; \n +use tracing::{debug, error, info}; \n use casper_execution_engine::shared::newtypes::Blake2bHash; \n @@ -339,8 +339,14 @@ where \n } \n // We do nothing in the case of having an incoming deploy rejected. \n Event::RejectedRemotely { .. } => Effects::new(), \n - Event::AbsentRemotely { id, peer } => self.signal(id, None, peer), \n - Event::TimeoutPeer { id, peer } => self.signal(id, None, peer), \n + Event::AbsentRemotely { id, peer } => { \n + info!(%id, %peer, \"element absent on the remote node\"); \n + self.signal(id, None, peer) \n + } \n + Event::TimeoutPeer { id, peer } => { \n + info!(%id, %peer, \"request timed out\"); \n + self.signal(id, None, peer) \n + } \n } \n } \n }", "msg": "NO-TICK: Log when fetcher fails to fetch the element."}
{"diff": "a / types/src/auction/detail.rs \n  b / types/src/auction/detail.rs \n@@ -205,7 +205,7 @@ pub(crate) fn create_unbonding_purse<P: Auction + ?Sized>( \n unbonder_public_key: PublicKey, \n bonding_purse: URef, \n amount: U512, \n -) -> Result<U512> { \n +) -> Result<()> { \n if provider.get_balance(bonding_purse)?.unwrap_or_default() < amount { \n return Err(Error::UnbondTooLarge); \n } \n @@ -225,10 +225,7 @@ pub(crate) fn create_unbonding_purse<P: Auction + ?Sized>( \n .push(new_unbonding_purse); \n set_unbonding_purses(provider, unbonding_purses)?; \n - // Remaining motes in the validator's bid purse \n - let remaining_bond = provider.get_balance(bonding_purse)?.unwrap_or_default(); \n - \n - Ok(remaining_bond) \n + Ok(()) \n } \n /// Reinvests delegator reward by increasing its stake.", "msg": "Change function signature.\nThis function does not need to return a remaining balance as the balance\nis unchanged during the operation."}
{"diff": "a / node/src/components/consensus/protocols/highway/tests.rs \n  b / node/src/components/consensus/protocols/highway/tests.rs \n@@ -87,9 +87,9 @@ where \n 0, \n start_timestamp, \n ); \n - // We expect only the vertex purge timer outcome. If there are more, the tests might need to \n - // handle them. \n - assert_eq!(1, outcomes.len()); \n + // We expect only the vertex purge timer and participation log timer outcomes. \n + // If there are more, the tests might need to handle them. \n + assert_eq!(2, outcomes.len()); \n hw_proto \n }", "msg": "Update HighwayProtocol tests to expect the added timer."}
{"diff": "a / node/src/components/block_executor.rs \n  b / node/src/components/block_executor.rs \n@@ -24,7 +24,7 @@ use casper_execution_engine::{ \n }, \n storage::global_state::CommitResult, \n }; \n -use casper_types::{ExecutionResult, ProtocolVersion, PublicKey, SemVer, U512}; \n +use casper_types::{ExecutionResult, ProtocolVersion, PublicKey, U512}; \n use crate::{ \n components::{ \n @@ -436,7 +436,7 @@ impl BlockExecutor { \n state_root_hash, \n finalized_block, \n next_era_validator_weights, \n - ProtocolVersion::new(SemVer::new(1, 0, 0)), // TODO: Fix \n + self.protocol_version, \n ); \n let summary = ExecutedBlockSummary { \n hash: *block.hash(),", "msg": "Use network's protocol version when creating a block."}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -909,8 +909,15 @@ where \n // the block or seen as equivocating via the consensus protocol gets slashed. \n let era_end = terminal_block_data.map(|tbd| EraReport { \n rewards: tbd.rewards, \n - equivocators: era.accusations(), \n - inactive_validators: tbd.inactive_validators, \n + // TODO: In the first 90 days we don't slash, and we just report all \n + // equivocators as \"inactive\" instead. Change this back 90 days after launch, \n + // and put era.accusations() into equivocators instead of inactive_validators. \n + equivocators: vec![], \n + inactive_validators: tbd \n + .inactive_validators \n + .into_iter() \n + .chain(era.accusations()) \n + .collect(), \n }); \n let finalized_block = FinalizedBlock::new( \n value.into(),", "msg": "Disable slashing.\nFor the first 90 days we only evict equivocating validators instead of\nslashing them."}
{"diff": "a / node/src/components/small_network.rs \n  b / node/src/components/small_network.rs \n@@ -877,15 +877,16 @@ where \n /// Returns whether or not this node has been disconnected from all known nodes. \n fn is_not_connected_to_any_known_address(&self) -> bool { \n - for addr in self.pending.keys() { \n - if self.known_addresses.contains(addr) { \n - // We have at least one pending connection to a known node, exit early. \n + for &known_address in &self.known_addresses { \n + if self.pending.contains_key(&known_address) { \n return false; \n } \n - } \n - for outgoing in self.outgoing.values() { \n - if self.known_addresses.contains(&outgoing.peer_address) { \n + if self \n + .outgoing \n + .values() \n + .any(|outgoing_connection| outgoing_connection.peer_address == known_address) \n + { \n return false; \n } \n }", "msg": "Refactor logic determining connection to known nodes"}
{"diff": "a / node/src/components/linear_chain/pending_signatures.rs \n  b / node/src/components/linear_chain/pending_signatures.rs \n@@ -65,7 +65,8 @@ impl PendingSignatures { \n return false; \n } \n // Add the pending signature. \n - sigs.insert(block_hash, signature).is_some() \n + sigs.insert(block_hash, signature); \n + true \n } \n pub(super) fn remove(", "msg": "Return `true` when signature was added to pending collection."}
{"diff": "a / types/src/crypto/asymmetric_key.rs \n  b / types/src/crypto/asymmetric_key.rs \n@@ -209,7 +209,7 @@ impl Tagged<u8> for SecretKey { \n } \n /// A public asymmetric key. \n -#[derive(DataSize, Eq, PartialEq)] \n +#[derive(Clone, DataSize, Eq, PartialEq)] \n pub enum PublicKey { \n /// System public key. \n System, \n @@ -355,19 +355,6 @@ impl Tagged<u8> for PublicKey { \n } \n } \n -impl Clone for PublicKey { \n - fn clone(&self) -> Self { \n - match self { \n - PublicKey::System => PublicKey::System, \n - PublicKey::Ed25519(public_key) => PublicKey::Ed25519(*public_key), \n - PublicKey::Secp256k1(public_key) => { \n - let raw_bytes: [u8; SECP256K1_COMPRESSED_PUBLIC_KEY_LENGTH] = public_key.to_bytes(); \n - Self::secp256k1_from_bytes(raw_bytes).unwrap() \n - } \n - } \n - } \n -} \n - \n impl ToBytes for PublicKey { \n fn to_bytes(&self) -> Result<Vec<u8>, bytesrepr::Error> { \n let mut buffer = bytesrepr::allocate_buffer(self)?;", "msg": "No Ticket: Better clone PublicKey"}
{"diff": "a / node/src/components/consensus/protocols/highway/synchronizer.rs \n  b / node/src/components/consensus/protocols/highway/synchronizer.rs \n@@ -202,6 +202,7 @@ impl<I: NodeIdT, C: Context + 'static> Synchronizer<I, C> { \n /// Removes expired pending vertices from the queues, and schedules the next purge. \n pub(crate) fn purge_vertices(&mut self, now: Timestamp) { \n + info!(\"purging synchronizer queues\"); \n let oldest = now.saturating_sub(self.pending_vertex_timeout); \n self.vertices_no_deps.remove_expired(oldest); \n self.requests_sent.clear();", "msg": "Add a log message when purging the synchronizer queues."}
{"diff": "a / node/src/components/consensus/protocols/highway/synchronizer.rs \n  b / node/src/components/consensus/protocols/highway/synchronizer.rs \n@@ -22,7 +22,7 @@ use crate::{ \n use super::{HighwayMessage, ProtocolOutcomes, ACTION_ID_VERTEX}; \n -const MAX_REQUESTS_FOR_VERTEX: usize = 10; \n +const MAX_REQUESTS_FOR_VERTEX: usize = 2; \n #[cfg(test)] \n mod tests;", "msg": "Change number of parallel requests for the same vertex to 2."}
{"diff": "a / node/src/components/block_proposer.rs \n  b / node/src/components/block_proposer.rs \n@@ -427,6 +427,7 @@ impl BlockProposerReady { \n .iter() \n .flat_map(|block_payload| block_payload.deploys_and_transfers_iter()) \n .map(DeployOrTransferHash::into) \n + .take_while(|hash| !self.contains_finalized(hash)) \n .collect(); \n let block_timestamp = context.timestamp(); \n let mut appendable_block = AppendableBlock::new(deploy_config, block_timestamp);", "msg": "Small optimization for collecting past deploys"}
{"diff": "a / node/src/app/cli.rs \n  b / node/src/app/cli.rs \n@@ -233,6 +233,7 @@ impl Cli { \n // At this point, the joiner is shut down, so we clear the queue to ensure any \n // connections whose handshake completed but have not been registered are dropped. \n + joiner_queue.seal(); \n for event in joiner_queue.drain_queues().await { \n debug!(event=%event, \"drained event\"); \n }", "msg": "Seal queue when transitioning from joiner to validator"}
{"diff": "a / node/src/reactor/participating.rs \n  b / node/src/reactor/participating.rs \n@@ -738,10 +738,6 @@ impl reactor::Reactor for Reactor { \n Some(deploy) => { \n match Message::new_get_response(&deploy) { \n Ok(message) => { \n - if self.storage.mem_duplication_enabled() { \n - todo!(\"update cache if enabled\") \n - } \n - \n return effect_builder \n .send_message(sender, message) \n .ignore();", "msg": "Remove never-taken if condition with explicit panic"}
{"diff": "a / node/src/reactor/joiner.rs \n  b / node/src/reactor/joiner.rs \n@@ -425,7 +425,6 @@ impl reactor::Reactor for Reactor { \n panic!(\"should have trusted hash after genesis era\") \n } \n } \n - info!(\"No synchronization of the linear chain will be done.\") \n } \n Some(hash) => info!(trusted_hash=%hash, \"synchronizing linear chain\"), \n }", "msg": "Move the \"No synchronization of the linear chain will be done\" message\nto the LinearChainSync constructors."}
{"diff": "a / client/lib/deploy.rs \n  b / client/lib/deploy.rs \n@@ -252,8 +252,9 @@ impl DeployExt for Deploy { \n const TRANSFER_ARG_TARGET: &str = \"target\"; \n const TRANSFER_ARG_ID: &str = \"id\"; \n - let amount = U512::from_dec_str(amount) \n - .map_err(|err| Error::FailedToParseUint(\"amount\", UIntParseError::FromDecStr(err)))?; \n + let amount = U512::from_dec_str(amount).map_err(|err| { \n + Error::FailedToParseUint(TRANSFER_ARG_AMOUNT, UIntParseError::FromDecStr(err)) \n + })?; \n let target = parsing::get_transfer_target(target)?; \n let transfer_id = parsing::transfer_id(transfer_id)?;", "msg": "change static str to use constant."}
{"diff": "a / node/src/components/small_network.rs \n  b / node/src/components/small_network.rs \n@@ -459,7 +459,7 @@ where \n peer_consensus_public_key, \n stream, \n } => { \n - info!(\"new incoming connection established\"); \n + info!(%public_addr, \"new incoming connection established\"); \n // Learn the address the peer gave us. \n let dial_requests =", "msg": "Report public address in log when an incoming connection is made"}
{"diff": "a / node/src/effect.rs \n  b / node/src/effect.rs \n@@ -179,7 +179,11 @@ impl<T> Responder<T> { \n pub(crate) async fn respond(mut self, data: T) { \n if let Some(sender) = self.0.take() { \n if sender.send(data).is_err() { \n - error!(\"could not send response to request down oneshot channel\"); \n + let backtrace = backtrace::Backtrace::new(); \n + error!( \n + ?backtrace, \n + \"could not send response to request down oneshot channel\" \n + ); \n } \n } else { \n error!(\"tried to send a value down a responder channel, but it was already used\");", "msg": "add a backtrace to the error! log when a responder fails to respond"}
{"diff": "a / node/src/components/contract_runtime/operations.rs \n  b / node/src/components/contract_runtime/operations.rs \n@@ -83,11 +83,12 @@ pub(super) fn execute_finalized_block( \n execution_results.insert(deploy_hash, (deploy_header, execution_result)); \n state_root_hash = state_hash; \n } \n - metrics.exec_block.observe(start.elapsed().as_secs_f64()); \n // Flush once, after all deploys have been executed. \n engine_state.flush_environment()?; \n + metrics.exec_block.observe(start.elapsed().as_secs_f64()); \n + \n // If the finalized block has an era report, run the auction contract and get the upcoming era \n // validators \n let maybe_step_effect_and_upcoming_era_validators =", "msg": "Include time to flush the LMDB as part of block execution."}
{"diff": "a / node/src/components/consensus/highway_core/state/index_panorama.rs \n  b / node/src/components/consensus/highway_core/state/index_panorama.rs \n@@ -2,6 +2,7 @@ use std::fmt::Debug; \n use datasize::DataSize; \n use serde::{Deserialize, Serialize}; \n +use tracing::error; \n use crate::components::consensus::{ \n highway_core::{ \n @@ -42,11 +43,13 @@ impl IndexPanorama { \n for (vid, obs) in panorama.enumerate() { \n let index_obs = match obs { \n Observation::None => IndexObservation::None, \n - Observation::Correct(hash) => state \n - .maybe_unit(hash) \n - .map_or(IndexObservation::None, |unit| { \n - IndexObservation::Correct(unit.seq_number) \n - }), \n + Observation::Correct(hash) => state.maybe_unit(hash).map_or_else( \n + || { \n + error!(?hash, \"expected unit to exist in the local protocol state\"); \n + IndexObservation::None \n + }, \n + |unit| IndexObservation::Correct(unit.seq_number), \n + ), \n Observation::Faulty => IndexObservation::Faulty, \n }; \n validator_map[vid] = index_obs;", "msg": "Log an error when state is missing expected unit."}
{"diff": "a / node/src/components/linear_chain_sync/operations.rs \n  b / node/src/components/linear_chain_sync/operations.rs \n@@ -341,6 +341,7 @@ where \n ?parent_header, \n \"received block with wrong parent from peer\", \n ); \n + effect_builder.announce_disconnect_from_peer(peer).await; \n continue; \n } \n @@ -367,12 +368,12 @@ where \n break item; \n } \n Err(FetcherError::Absent { .. }) => { \n - warn!(height, tag = ?I::TAG, ?peer, \"Block by height absent from peer\"); \n + warn!(height, tag = ?I::TAG, ?peer, \"block by height absent from peer\"); \n // If the peer we requested doesn't have the item, continue with the next peer \n continue; \n } \n Err(FetcherError::TimedOut { .. }) => { \n - warn!(height, tag = ?I::TAG, ?peer, \"Peer timed out\"); \n + warn!(height, tag = ?I::TAG, ?peer, \"peer timed out\"); \n // Peer timed out fetching the item, continue with the next peer \n continue; \n }", "msg": "Disconnect from a peer that sent us a block with wrong parent."}
{"diff": "a / node/src/utils/round_robin.rs \n  b / node/src/utils/round_robin.rs \n@@ -17,7 +17,7 @@ use std::{ \n use enum_iterator::IntoEnumIterator; \n use serde::{ser::SerializeMap, Serialize, Serializer}; \n use tokio::sync::{Mutex, MutexGuard, Semaphore}; \n -use tracing::warn; \n +use tracing::debug; \n /// Weighted round-robin scheduler. \n /// \n @@ -243,7 +243,7 @@ where \n /// Panics if the queue identified by key `queue` does not exist. \n pub(crate) async fn push(&self, item: I, queue: K) { \n if self.sealed.load(Ordering::SeqCst) { \n - warn!(\"queue sealed, dropping item\"); \n + debug!(\"queue sealed, dropping item\"); \n return; \n }", "msg": "Lower the log level for pushing items into a sealed queue"}
{"diff": "a / ci/casper_updater/src/regex_data.rs \n  b / ci/casper_updater/src/regex_data.rs \n@@ -229,11 +229,6 @@ pub mod smart_contracts_contract { \n Regex::new(r#\"(?m)(\"casper-contract\",\\s*)\"(?:[^\"]+)\"#).unwrap(), \n replacement, \n ), \n - DependentFile::new( \n - \"execution_engine_testing/test_support/Cargo.toml\", \n - Regex::new(r#\"(?m)(^casper-contract = \\{[^\\}]*version = )\"(?:[^\"]+)\"#).unwrap(), \n - replacement, \n - ), \n DependentFile::new( \n \"smart_contracts/contract/Cargo.toml\", \n MANIFEST_VERSION_REGEX.clone(),", "msg": "update casper-updater tool to reflect changes to casper-engine-test-support crate"}
{"diff": "a / node/src/utils/work_queue.rs \n  b / node/src/utils/work_queue.rs \n@@ -79,7 +79,7 @@ use tokio::sync::Notify; \n /// # let handle = rt.handle(); \n /// # handle.block_on(test_func()); \n /// ``` \n -#[derive(Debug, Default)] \n +#[derive(Debug)] \n pub struct WorkQueue<T> { \n /// Jobs currently in the queue. \n jobs: Mutex<VecDeque<T>>, \n @@ -89,6 +89,17 @@ pub struct WorkQueue<T> { \n notify: Notify, \n } \n +// Manual default implementation, since the derivation would require a `T: Default` trait bound. \n +impl<T> Default for WorkQueue<T> { \n + fn default() -> Self { \n + Self { \n + jobs: Default::default(), \n + in_progress: Default::default(), \n + notify: Default::default(), \n + } \n + } \n +} \n + \n impl<T> WorkQueue<T> { \n /// Pop a job from the queue. \n ///", "msg": "Implement `Default` for `WorkQueue` in a matter that does not require `T: Default`"}
{"diff": "a / node/src/components/small_network/tasks.rs \n  b / node/src/components/small_network/tasks.rs \n@@ -375,8 +375,7 @@ where \n if protocol_version <= threshold { \n let mut rng = OsRng; \n - let sample = rng.gen_range(0.0f32..1.0f32); \n - if context.tarpit_chance > sample { \n + if rng.gen_bool(context.tarpit_chance as f64) { \n // If tarpitting is enabled, we hold open the connection for a specific \n // amount of time, to reduce load on other nodes and keep them from \n // reconnecting.", "msg": "Use `gen_bool` when deciding whether to tarpit a node"}
{"diff": "a / execution_engine_testing/tests/src/test/regression/host_function_metrics_size_and_gas_cost.rs \n  b / execution_engine_testing/tests/src/test/regression/host_function_metrics_size_and_gas_cost.rs \n@@ -19,8 +19,8 @@ use casper_types::{ \n const CONTRACT_HOST_FUNCTION_METRICS: &str = \"host_function_metrics.wasm\"; \n const CONTRACT_TRANSFER_TO_ACCOUNT_U512: &str = \"transfer_to_account_u512.wasm\"; \n -const HOST_FUNCTION_METRICS_STANDARD_SIZE: usize = 90_620; \n -const HOST_FUNCTION_METRICS_STANDARD_GAS_COST: u64 = 144_018_843_520; \n +const HOST_FUNCTION_METRICS_STANDARD_SIZE: usize = 116_800; \n +const HOST_FUNCTION_METRICS_STANDARD_GAS_COST: u64 = 151_323_460_380; \n /// Acceptable size regression/improvement in percentage. \n const SIZE_MARGIN: usize = 5;", "msg": "Update truth values after merging dev"}
{"diff": "a / node/src/components/console/tasks.rs \n  b / node/src/components/console/tasks.rs \n@@ -224,7 +224,9 @@ impl Session { \n } \n } \n } \n - Action::DumpQueues => match self.create_temp_file_serializer(todo!()) { \n + Action::DumpQueues => { \n + match tempfile::tempfile() { \n + Ok(tmp) => match self.create_temp_file_serializer(tmp) { \n Some(serializer) => { \n self.send_outcome(writer, &Outcome::success(\"dumping queues\")) \n .await?; \n @@ -238,6 +240,18 @@ impl Session { \n .await?; \n } \n }, \n + Err(err) => { \n + self.send_outcome( \n + writer, \n + &Outcome::failed(format!( \n + \"could not create a temporary file for queue dump: {}\", \n + err \n + )), \n + ) \n + .await?; \n + } \n + }; \n + } \n }; \n } \n Err(err) => {", "msg": "Create a tempfile for sending queue dumps"}
{"diff": "a / types/src/bytesrepr.rs \n  b / types/src/bytesrepr.rs \n@@ -1290,8 +1290,14 @@ where \n t.write_bytes(&mut written_bytes) \n .expect(\"Unable to serialize data via write_bytes\"); \n assert_eq!(serialized, written_bytes); \n + \n + let deserialized_from_slice = \n + deserialize_from_slice(&serialized).expect(\"Unable to deserialize data\"); \n + // assert!(*t == deserialized); \n + assert_eq!(*t, deserialized_from_slice); \n + \n let deserialized = deserialize::<T>(serialized).expect(\"Unable to deserialize data\"); \n - assert!(*t == deserialized); \n + assert_eq!(*t, deserialized); \n } \n #[cfg(test)] \n mod tests {", "msg": "Add deser from slice to roundtrip tests"}
{"diff": "a / node/src/components/chain_synchronizer/operations.rs \n  b / node/src/components/chain_synchronizer/operations.rs \n@@ -121,6 +121,8 @@ async fn fetch_trie_retry_forever( \n ) -> FetchedData<Trie<Key, StoredValue>> { \n loop { \n let peers = ctx.effect_builder.get_fully_connected_peers().await; \n + \n + if !peers.is_empty() { \n trace!(?id, \"attempting to fetch a trie\",); \n match ctx.effect_builder.fetch_trie(id, peers).await { \n Ok(fetched_data) => { \n @@ -131,6 +133,10 @@ async fn fetch_trie_retry_forever( \n warn!(?id, %error, \"fast sync could not fetch a trie; trying again\") \n } \n } \n + } \n + // Note: We would love to log if we have to retry, but the retry_interval is so short that \n + // it would likely spam the logs. \n + \n tokio::time::sleep(ctx.config.retry_interval()).await \n } \n }", "msg": "Do not attempt to fetch tries if there are no peers to fetch from"}
{"diff": "a / node/src/testing.rs \n  b / node/src/testing.rs \n@@ -275,6 +275,9 @@ impl<REv: 'static> ComponentHarness<REv> { \n fatal \n ) \n } \n + ControlAnnouncement::QueueDump { .. } => { \n + panic!(\"queue dumps are not supported in the test harness\") \n + } \n } \n } else { \n debug!(?ev, \"ignoring event while looking for a fatal\")", "msg": "Make tests work with now present queue dumps"}
{"diff": "a / node/src/components/chain_synchronizer/operations.rs \n  b / node/src/components/chain_synchronizer/operations.rs \n@@ -184,7 +184,7 @@ impl<'a> ChainSyncContext<'a> { \n let bad_peer_list = mem::take::<VecDeque<NodeId>>(&mut bad_peer_list); \n if !bad_peer_list.is_empty() { \n - warn!(?bad_peer_list, \"ran out of peers, redeemed all bad peers\") \n + warn!(?bad_peer_list, \"redeemed all bad peers\") \n } \n } \n }", "msg": "Better error message for bad peer redemption in sync"}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -859,7 +859,7 @@ impl EraSupervisor { \n async move { \n let peers = effect_builder.get_fully_connected_peers().await; \n if let Some(to) = peers.into_iter().next() { \n - effect_builder.send_message(to, message.into()).await; \n + effect_builder.enqueue_message(to, message.into()).await; \n } \n } \n .ignore()", "msg": "When sending message to a random peer in consensus, enqueue but do not await sending"}
{"diff": "a / node/src/components/small_network/tasks.rs \n  b / node/src/components/small_network/tasks.rs \n@@ -690,10 +690,12 @@ pub(super) async fn message_sender<P>( \n }; \n limiter.request_allowance(estimated_wire_size).await; \n - let outcome = sink.send(message).await; \n + let mut outcome = sink.send(message).await; \n // Notify via responder that the message has been buffered by the kernel. \n if let Some(responder) = opt_responder { \n + // Since someone is interested in the message, flush the socket to ensure it was sent. \n + outcome = outcome.and(sink.flush().await); \n responder.respond(()).await; \n }", "msg": "Flush message sender when a potentially awaited message is being sent"}
{"diff": "a / node/src/components/chain_synchronizer/operations.rs \n  b / node/src/components/chain_synchronizer/operations.rs \n@@ -167,6 +167,11 @@ impl<'a> ChainSyncContext<'a> { \n /// Marks a peer as bad. \n fn mark_bad_peer(&self, peer: NodeId) { \n + if self.config.redemption_interval == 0 { \n + info!(%peer, \"not marking peer as bad for syncing, redemption is disabled\"); \n + return; \n + } \n + \n let mut bad_peer_list = self \n .bad_peer_list \n .write()", "msg": "Do not mark peers as bad if they cannot redeem themselves"}
{"diff": "a / node/src/components/consensus/era_supervisor.rs \n  b / node/src/components/consensus/era_supervisor.rs \n@@ -429,11 +429,9 @@ impl EraSupervisor { \n .flat_map(|era_end| era_end.era_report().equivocators.clone()) \n .collect(); \n - let instance_id = instance_id( \n - self.chainspec.hash(), \n - era_id, \n - key_block.hash(self.verifiable_chunked_hash_activation()), \n - ); \n + let chainspec_hash = self.chainspec.hash(); \n + let key_block_hash = key_block.hash(self.verifiable_chunked_hash_activation()); \n + let instance_id = instance_id(chainspec_hash, era_id, key_block_hash); \n let now = Timestamp::now(); \n info!( \n @@ -441,6 +439,8 @@ impl EraSupervisor { \n %start_time, \n %now, \n %start_height, \n + %chainspec_hash, \n + %key_block_hash, \n %instance_id, \n %seed, \n era = era_id.value(),", "msg": "Log chainspec and key block hashes when starting era."}
{"diff": "a / node/src/utils/work_queue.rs \n  b / node/src/utils/work_queue.rs \n@@ -160,12 +160,9 @@ impl<T> WorkQueue<T> { \n /// Creates a streaming consumer of the work queue. \n #[inline] \n pub fn to_stream(self: Arc<Self>) -> impl Stream<Item = JobHandle<T>> { \n - stream::unfold((), move |_| { \n - let local_ref = self.clone(); \n - async move { \n - let next = local_ref.next_job().await; \n - next.map(|handle| (handle, ())) \n - } \n + stream::unfold(self, |work_queue| async move { \n + let next = work_queue.next_job().await; \n + next.map(|handle| (handle, work_queue)) \n }) \n }", "msg": "Improve efficiency of `WorkQueue::to_stream`"}
{"diff": "a / node/src/types/item.rs \n  b / node/src/types/item.rs \n@@ -10,7 +10,7 @@ use serde_repr::{Deserialize_repr, Serialize_repr}; \n use thiserror::Error; \n use casper_execution_engine::storage::trie::{TrieOrChunk, TrieOrChunkId}; \n -use casper_hashing::{error::ChunkWithProofVerificationError, Digest}; \n +use casper_hashing::{ChunkWithProofVerificationError, Digest}; \n use casper_types::EraId; \n use crate::types::{BlockHash, BlockHeader};", "msg": "Adjust casper-node to changes in hashing"}
{"diff": "a / utils/global-state-update-gen/src/validators/validators_manager.rs \n  b / utils/global-state-update-gen/src/validators/validators_manager.rs \n@@ -262,7 +262,9 @@ impl ValidatorsUpdateManager { \n .get_bids() \n .into_iter() \n .filter(|(pub_key, bid)| { \n - bid.staked_amount() >= min_bid && !seigniorage_recipients.contains_key(pub_key) \n + bid.total_staked_amount() \n + .map_or(true, |amount| amount >= *min_bid) \n + && !seigniorage_recipients.contains_key(pub_key) \n }) \n .map(|(pub_key, _bid)| pub_key) \n .collect()", "msg": "Take delegations into account when looking for large bids"}
{"diff": "a / node/src/components/consensus/utils.rs \n  b / node/src/components/consensus/utils.rs \n@@ -252,10 +252,14 @@ mod tests { \n // Smuggle a bogus proof in. \n let (_, pub_key) = generate_ed25519_keypair(); \n - signatures.insert_proof(pub_key, *signatures.proofs.iter().next().unwrap().1); \n + signatures.insert_proof(pub_key.clone(), *signatures.proofs.iter().next().unwrap().1); \n assert!(matches!( \n validate_finality_signatures(&signatures, &weights), \n - Err(FinalitySignatureError::BogusValidator { .. }) \n + Err(FinalitySignatureError::BogusValidator { \n + trusted_validator_weights: _, \n + block_signatures, \n + bogus_validator_public_key \n + }) if *bogus_validator_public_key == pub_key && *block_signatures == signatures \n )); \n }", "msg": "Add more detailed validation of the `BogusValidator` error"}
{"diff": "a / node/src/components/small_network/metrics.rs \n  b / node/src/components/small_network/metrics.rs \n@@ -420,7 +420,7 @@ impl Metrics { \n } \n /// Records an outgoing payload. \n - pub(crate) fn record_payload_out(this: &mut Weak<Self>, kind: MessageKind, size: u64) { \n + pub(crate) fn record_payload_out(this: &Weak<Self>, kind: MessageKind, size: u64) { \n if let Some(metrics) = this.upgrade() { \n match kind { \n MessageKind::Protocol => { \n @@ -466,7 +466,7 @@ impl Metrics { \n } \n /// Records an incoming payload. \n - pub(crate) fn record_payload_in(this: &mut Weak<Self>, kind: MessageKind, size: u64) { \n + pub(crate) fn record_payload_in(this: &Weak<Self>, kind: MessageKind, size: u64) { \n if let Some(metrics) = this.upgrade() { \n match kind { \n MessageKind::Protocol => {", "msg": "Payload recording methods of small network metrics need not take mutable `this` args"}
{"diff": "a / node/src/components/chain_synchronizer/operations.rs \n  b / node/src/components/chain_synchronizer/operations.rs \n@@ -431,12 +431,13 @@ where \n let new_peer_list = get_peers(T::can_use_syncing_nodes(), ctx).await; \n if new_peer_list.is_empty() && total_attempts % 100 == 0 { \n - error!( \n + warn!( \n total_attempts, \n + retry_count, \n + has_connected_to_network, \n item_type = ?T::TAG, \n ?id, \n can_use_syncing_nodes = %T::can_use_syncing_nodes(), \n - has_connected_to_network, \n \"failed to attempt to fetch item due to no fully-connected peers\" \n ); \n }", "msg": "Improve the log message used for tracking the fetch attempts"}
{"diff": "a / node/src/types/block.rs \n  b / node/src/types/block.rs \n@@ -1968,9 +1968,17 @@ impl Item for BlockEffectsOrChunk { \n } \n }, \n BlockEffectsOrChunk::BlockEffects(value) => match value { \n - ValueOrChunk::Value(execution_results) => BlockEffectsOrChunkId::new( \n - Chunkable::hash(&execution_results).expect(\"hashing to succeed.\"), //TODO \n - ), \n + ValueOrChunk::Value(execution_results) => { \n + match Chunkable::hash(&execution_results) { \n + Ok(chunked_hash) => BlockEffectsOrChunkId::new(chunked_hash), \n + Err(err) => { \n + error!(?err, \"error when calculating hash of block effects. Will likely lead to a sync process being stuck.\"); \n + // To appease compiler we have to return `Self::Id`. This will have no \n + // effect and will be dropped in the fetcher. \n + BlockEffectsOrChunkId::new(Digest::default()) \n + } \n + } \n + } \n ValueOrChunk::ChunkWithProof(chunk_with_proof) => { \n BlockEffectsOrChunkId::BlockEffectsOrChunkId { \n chunk_index: chunk_with_proof.proof().index(),", "msg": "Do not call expect on failed id calculation."}
{"diff": "a / node/src/components/deploy_buffer.rs \n  b / node/src/components/deploy_buffer.rs \n@@ -127,6 +127,7 @@ impl DeployBuffer { \n let footprint = match deploy.footprint() { \n Ok(deploy_footprint) => deploy_footprint, \n Err(_) => { \n + error!(%deploy_hash, \"invalid deploy in the proposable set\"); \n self.dead.insert(deploy_hash); \n continue; \n }", "msg": "Log error for invalid deploy in proposable set"}
{"diff": "a / node/src/reactor/main_reactor.rs \n  b / node/src/reactor/main_reactor.rs \n@@ -367,12 +367,19 @@ impl reactor::Reactor for MainReactor { \n // ROUTE ALL INTENT TO SHUTDOWN TO THIS EVENT \n // (DON'T USE FATAL ELSEWHERE WITHIN REACTOR OR COMPONENTS) \n - MainEvent::Shutdown(msg) => fatal!( \n + MainEvent::Shutdown(msg) => { \n + if self.consensus.is_active_validator() { \n + info!(%msg, \"consensus is active, not shutting down\"); \n + Effects::new() \n + } else { \n + fatal!( \n effect_builder, \n \"reactor should shut down due to error: {}\", \n msg, \n ) \n - .ignore(), \n + .ignore() \n + } \n + } \n // LOCAL I/O BOUND COMPONENTS \n MainEvent::UpgradeWatcher(event) => reactor::wrap_effects(", "msg": "Don't shut down if consensus is actively validating"}
{"diff": "a / node/src/components/fetcher/item_fetcher.rs \n  b / node/src/components/fetcher/item_fetcher.rs \n@@ -76,12 +76,16 @@ pub(crate) trait ItemFetcher<T: FetcherItem + 'static> { \n let peer_timeout = self.peer_timeout(); \n // Capture responder for later signalling. \n let responders = self.responders(); \n - responders \n + let entry = responders \n .entry(id.clone()) \n .or_default() \n .entry(peer) \n - .or_default() \n - .push(responder); \n + .or_default(); \n + entry.push(responder); \n + if entry.len() != 1 { \n + // Avoid sending a new get request if we already have one in flight. \n + return Effects::new(); \n + } \n match Message::new_get_request::<T>(&id) { \n Ok(message) => { \n self.metrics().fetch_total.inc();", "msg": "change fetcher to avoid duplicating requests"}
{"diff": "a / node/src/components/block_synchronizer/block_acquisition.rs \n  b / node/src/components/block_synchronizer/block_acquisition.rs \n@@ -967,6 +967,7 @@ impl BlockAcquisitionAction { \n } \n } \n + // NOT WIRED - NEEDS TO BE WIRED TO `strict_finality_signatures` actions. \n pub(super) fn strict_finality_signatures( \n peer_list: &PeerList, \n rng: &mut NodeRng,", "msg": "Add not wired comment to previously added fn"}
{"diff": "a / node/src/components/small_network/insights.rs \n  b / node/src/components/small_network/insights.rs \n//! insights should neither be abused just because they are available. \n use std::{ \n - collections::{BTreeSet, HashMap, HashSet}, \n + collections::{BTreeSet, HashSet}, \n fmt::{self, Debug, Display, Formatter}, \n net::SocketAddr, \n sync::atomic::Ordering, \n @@ -49,9 +49,9 @@ pub(crate) struct NetworkInsights { \n /// The amount of bandwidth allowance currently buffered, ready to be spent. \n unspent_bandwidth_allowance_bytes: Option<i64>, \n /// Map of outgoing connections, along with their current state. \n - outgoing_connections: HashMap<SocketAddr, OutgoingInsight>, \n + outgoing_connections: Vec<(SocketAddr, OutgoingInsight)>, \n /// Map of incoming connections. \n - connection_symmetries: HashMap<NodeId, ConnectionSymmetryInsight>, \n + connection_symmetries: Vec<(NodeId, ConnectionSymmetryInsight)>, \n } \n /// Insight into an outgoing connection.", "msg": "Do not use hashmaps to return data in networking insights, to ensure JSON encodability"}
{"diff": "a / node/src/components/block_synchronizer/trie_accumulator.rs \n  b / node/src/components/block_synchronizer/trie_accumulator.rs \n@@ -5,6 +5,7 @@ use std::{ \n use datasize::DataSize; \n use derive_more::From; \n +use rand::seq::SliceRandom; \n use serde::Serialize; \n use thiserror::Error; \n use tracing::{debug, error, trace, warn}; \n @@ -220,7 +221,7 @@ where \n fn handle_event( \n &mut self, \n effect_builder: EffectBuilder<REv>, \n - _rng: &mut NodeRng, \n + rng: &mut NodeRng, \n event: Self::Event, \n ) -> Effects<Self::Event> { \n trace!(?event, \"TrieAccumulator: handling event\"); \n @@ -228,8 +229,9 @@ where \n Event::Request(TrieAccumulatorRequest { \n hash, \n responder, \n - peers, \n + mut peers, \n }) => { \n + peers.shuffle(rng); \n let trie_id = TrieOrChunkId(0, hash); \n let peer = match peers.last() { \n Some(peer) => *peer,", "msg": "Shuffle peers before choosing one for downloading a trie"}
{"diff": "a / node/src/components/block_synchronizer/trie_accumulator.rs \n  b / node/src/components/block_synchronizer/trie_accumulator.rs \n@@ -36,6 +36,8 @@ pub(crate) enum Error { \n Bytesrepr(bytesrepr::Error), \n #[error(\"trie accumulator couldn't fetch trie chunk ({0}, {1})\")] \n Absent(Digest, u64), \n + #[error(\"request contained no peers; trie = {0}\")] \n + NoPeers(Digest), \n } \n #[derive(DataSize, Debug)] \n @@ -237,7 +239,7 @@ where \n Some(peer) => *peer, \n None => { \n error!(%hash, \"tried to fetch trie with no peers available\"); \n - return Effects::new(); \n + return responder.respond(Err(Error::NoPeers(hash))).ignore(); \n } \n }; \n let partial_chunks = PartialChunks {", "msg": "Don't drop the responder when no peers were supplied"}
{"diff": "a / node/src/components/block_synchronizer/block_acquisition.rs \n  b / node/src/components/block_synchronizer/block_acquisition.rs \n@@ -653,8 +653,7 @@ impl BlockAcquisitionState { \n ) { \n Ok(new_effects) => match new_effects { \n ExecutionResultsAcquisition::Needed { .. } \n - | ExecutionResultsAcquisition::Pending { .. } \n - | ExecutionResultsAcquisition::Acquiring { .. } => return Ok(None), \n + | ExecutionResultsAcquisition::Pending { .. } => return Ok(None), \n ExecutionResultsAcquisition::Complete { ref results, .. } => ( \n BlockAcquisitionState::HaveGlobalState( \n block.clone(), \n @@ -664,6 +663,15 @@ impl BlockAcquisitionState { \n ), \n Some(results.clone()), \n ), \n + ExecutionResultsAcquisition::Acquiring { .. } => ( \n + BlockAcquisitionState::HaveGlobalState( \n + block.clone(), \n + signatures.clone(), \n + deploys.clone(), \n + new_effects, \n + ), \n + None, \n + ), \n }, \n Err(error) => { \n warn!(%error, \"failed to apply execution results\");", "msg": "Allow ExecutionResultsAcquisition to collect next chunk"}
{"diff": "a / node/src/reactor/main_reactor/tests.rs \n  b / node/src/reactor/main_reactor/tests.rs \n@@ -539,7 +539,7 @@ async fn dont_upgrade_without_switch_block() { \n } \n // Run until the nodes shut down for the upgrade. \n - let timeout = Duration::from_secs(30); \n + let timeout = Duration::from_secs(90); \n net.settle_on_exit(&mut rng, ExitCode::Success, timeout) \n .await; \n @@ -590,7 +590,7 @@ async fn should_store_finalized_approvals() { \n net.settle_on( \n &mut rng, \n has_completed_era(EraId::from(0)), \n - Duration::from_secs(30), \n + Duration::from_secs(90), \n ) \n .await; \n @@ -668,7 +668,7 @@ async fn should_store_finalized_approvals() { \n } \n // Run until the deploy gets executed. \n - let timeout = Duration::from_secs(30); \n + let timeout = Duration::from_secs(90); \n net.settle_on( \n &mut rng, \n |nodes| {", "msg": "Increase timeouts for main_reactor tests."}
{"diff": "a / node/src/reactor/main_reactor/keep_up.rs \n  b / node/src/reactor/main_reactor/keep_up.rs \n@@ -301,9 +301,12 @@ impl MainReactor { \n leap_status \n ); \n match leap_status { \n - LeapStatus::Idle => { \n - self.sync_back_leaper_idle(effect_builder, rng, parent_hash, Duration::ZERO) \n - } \n + LeapStatus::Idle => self.sync_back_leaper_idle( \n + effect_builder, \n + rng, \n + parent_hash, \n + self.control_logic_default_delay.into(), \n + ), \n LeapStatus::Awaiting { .. } => KeepUpInstruction::CheckLater( \n \"historical sync leaper is awaiting response\".to_string(), \n self.control_logic_default_delay.into(),", "msg": "Prevent log flooding by introducing delays between sync leaps when there are no peers available"}
{"diff": "a / node/src/components/block_synchronizer/block_builder.rs \n  b / node/src/components/block_synchronizer/block_builder.rs \n@@ -515,7 +515,10 @@ impl BlockBuilder { \n self.touch(); \n self.promote_peer(maybe_peer); \n } \n - Ok(Some(Acceptance::HadIt)) | Ok(None) => (), \n + Ok(Some(Acceptance::HadIt)) => { \n + self.in_flight_latch = None; \n + } \n + Ok(None) => (), \n Err(error) => { \n self.disqualify_peer(maybe_peer); \n return Err(Error::BlockAcquisition(error));", "msg": "release the block builder latch even if the data was already registered"}
{"diff": "a / node/src/reactor/main_reactor/control.rs \n  b / node/src/reactor/main_reactor/control.rs \n@@ -445,12 +445,9 @@ impl MainReactor { \n } \n fn refresh_contract_runtime(&mut self) -> Result<(), String> { \n - // Note: we don't want to read the highest COMPLETE block, as an immediate switch block is \n - // only marked complete after we receive enough signatures from validators. Using the \n - // highest stored block ensures the ContractRuntime's `exec_queue` isn't set to a block \n - // height we already executed but haven't yet marked complete. \n - match self.storage.read_highest_block_header() { \n - Ok(Some(block_header)) => { \n + match self.storage.read_highest_complete_block() { \n + Ok(Some(block)) => { \n + let block_header = block.header(); \n let block_height = block_header.height(); \n let state_root_hash = block_header.state_root_hash(); \n let block_hash = block_header.id();", "msg": "Use highest complete block to initialize contract runtime"}
{"diff": "a / config/src/network_config.rs \n  b / config/src/network_config.rs \n@@ -87,7 +87,9 @@ impl Default for NetworkRpcQuotaConfiguration { \n pub struct NetworkConfig { \n // The address that this node is listening on for new connections. \n pub listen: Multiaddr, \n + #[serde(default)] \n pub seeds: Vec<MultiaddrWithPeerId>, \n + #[serde(default)] \n pub enable_mdns: bool, \n //TODO skip this field, do not persistence this flag to config. this change will break network config. \n pub disable_seed: bool,", "msg": "[config] Add default to network config enable_mdns."}
{"diff": "a / vm/types/src/genesis_config.rs \n  b / vm/types/src/genesis_config.rs \n@@ -268,8 +268,6 @@ impl BuiltinNetworkID { \n pub fn boot_nodes_domain(self) -> String { \n match self { \n BuiltinNetworkID::Test | BuiltinNetworkID::Dev => \"localhost\".to_string(), \n - BuiltinNetworkID::Halley => \"halley1.seed.starcoin.org\".to_string(), \n - BuiltinNetworkID::Proxima => \"proxima1.seed.starcoin.org\".to_string(), \n _ => format!(\"{}.seed.starcoin.org\", self), \n } \n }", "msg": "[config] Change boot nodes domain to load balance domain."}
{"diff": "a / cmd/starcoin/src/view.rs \n  b / cmd/starcoin/src/view.rs \n@@ -36,12 +36,11 @@ pub struct TransactionOptions { \n /// otherwise please let cli to auto get sequence_number from onchain and txpool. \n pub sequence_number: Option<u64>, \n - #[clap(short = 'g', name = \"max-gas-amount\")] \n + #[clap(long = \"max-gas-amount\")] \n /// max gas used to deploy the module \n pub max_gas_amount: Option<u64>, \n #[clap( \n - short = 'p', \n long = \"gas-unit-price\", \n alias = \"gas-price\", \n name = \"price of gas unit\"", "msg": "deprecated short option for TransactionOptions::max_gas_amount and TransactionOptions::gas_unit_price"}
{"diff": "a / src/lib.rs \n  b / src/lib.rs \n@@ -1147,9 +1147,6 @@ impl Glfw { \n /// Wrapper for `glfwGetInstanceProcAddress` \n #[cfg(feature = \"vulkan\")] \n pub fn get_instance_proc_address_raw(&self, instance: VkInstance, procname: &str) -> VkProc { \n - //TODO: Determine if this assertion is required? It doesn't seem to be required for vkCreateInstance, \n - //TODO: but it might be needed for other pointers. \n - debug_assert!(unsafe { ffi::glfwGetCurrentContext() } != std::ptr::null_mut()); \n with_c_str(procname, |procname| { \n unsafe { ffi::glfwGetInstanceProcAddress(instance, procname) } \n })", "msg": "Remove context check from get_instance_proc_address_raw for Vulkan"}
{"diff": "a / src/base.rs \n  b / src/base.rs \n@@ -23,13 +23,16 @@ pub fn trans_mono_item<'a, 'tcx: 'a>( \n match inst.def { \n InstanceDef::Item(_) \n | InstanceDef::DropGlue(_, _) \n - | InstanceDef::Virtual(_, _) => { \n + | InstanceDef::Virtual(_, _) if inst.def_id().krate == LOCAL_CRATE => { \n let mut mir = ::std::io::Cursor::new(Vec::new()); \n ::rustc_mir::util::write_mir_pretty(tcx, Some(inst.def_id()), &mut mir) \n .unwrap(); \n String::from_utf8(mir.into_inner()).unwrap() \n } \n - InstanceDef::FnPtrShim(_, _) \n + InstanceDef::Item(_) \n + | InstanceDef::DropGlue(_, _) \n + | InstanceDef::Virtual(_, _) \n + | InstanceDef::FnPtrShim(_, _) \n | InstanceDef::ClosureOnceShim { .. } \n | InstanceDef::CloneShim(_, _) => { \n // FIXME fix write_mir_pretty for these instances", "msg": "Don't use write_mir_pretty for non local mir"}
{"diff": "a / src/abi.rs \n  b / src/abi.rs \n@@ -49,7 +49,7 @@ fn get_pass_mode<'a, 'tcx: 'a>( \n PassMode::ByVal(ret_ty) \n } else { \n if abi == Abi::C { \n - unimplemented!(\"Non scalars are not yet supported for \\\"C\\\" abi\"); \n + unimpl!(\"Non scalars are not yet supported for \\\"C\\\" abi ({:?}) is_return: {:?}\", ty, is_return); \n } \n PassMode::ByRef \n }", "msg": "Better error message for unsupported \"C\" abi args"}
{"diff": "a / example/mini_core.rs \n  b / example/mini_core.rs \n@@ -268,6 +268,7 @@ pub trait FnMut<Args>: FnOnce<Args> { \n #[lang = \"panic\"] \n pub fn panic(&(_msg, _file, _line, _col): &(&'static str, &'static str, u32, u32)) -> ! { \n unsafe { \n + libc::puts(\"Panicking\\0\" as *const str as *const u8); \n intrinsics::abort(); \n } \n }", "msg": "Print a message when panicking from mini_core"}
{"diff": "a / src/base.rs \n  b / src/base.rs \n@@ -563,12 +563,6 @@ fn is_fat_ptr<'a, 'tcx: 'a>(fx: &FunctionCx<'a, 'tcx, impl Backend>, ty: Ty<'tcx \n ) \n } else if from_clif_ty.is_int() && to_clif_ty.is_float() { \n // int-like -> float \n - // FIXME missing encoding for fcvt_from_sint.f32.i8 \n - let from = if from_clif_ty == types::I8 || from_clif_ty == types::I16 { \n - fx.bcx.ins().uextend(types::I32, from) \n - } else { \n - from \n - }; \n if signed { \n fx.bcx.ins().fcvt_from_sint(to_clif_ty, from) \n } else {", "msg": "Remove workaround for previously missing encoding"}
{"diff": "a / src/constant.rs \n  b / src/constant.rs \n@@ -321,8 +321,7 @@ fn define_all_allocs( \n // Don't push a `TodoItem::Static` here, as it will cause statics used by \n // multiple crates to be duplicated between them. It isn't necessary anyway, \n // as it will get pushed by `codegen_static` when necessary. \n - let linkage = crate::linkage::get_static_ref_linkage(tcx, def_id); \n - data_id_for_static(tcx, module, def_id, linkage) \n + data_id_for_static(tcx, module, def_id, Linkage::Import) \n } \n };", "msg": "Always use Linkage::Import for relocations targeting a static"}
{"diff": "a / src/base.rs \n  b / src/base.rs \n@@ -162,6 +162,13 @@ fn codegen_fn_content(fx: &mut FunctionCx<'_, '_, impl Backend>) { \n target, \n cleanup: _, \n } => { \n + if !fx.tcx.sess.overflow_checks() { \n + if let mir::interpret::PanicInfo::OverflowNeg = *msg { \n + let target = fx.get_ebb(*target); \n + fx.bcx.ins().jump(target, &[]); \n + continue; \n + } \n + } \n let cond = trans_operand(fx, cond).load_scalar(fx); \n // TODO HACK brz/brnz for i8/i16 is not yet implemented \n let cond = fx.bcx.ins().uextend(types::I32, cond);", "msg": "Don't perform neg overflow checks when they are disabled"}
{"diff": "a / src/value_and_place.rs \n  b / src/value_and_place.rs \n@@ -334,7 +334,15 @@ pub fn to_addr_maybe_unsized( \n fx.bcx.ins().stack_addr(fx.pointer_type, stack_slot, 0), \n None, \n ), \n - CPlaceInner::NoPlace => (fx.bcx.ins().iconst(fx.pointer_type, 45), None), \n + CPlaceInner::NoPlace => { \n + ( \n + fx.bcx.ins().iconst( \n + fx.pointer_type, \n + i64::try_from(self.layout.align.pref.bytes()).unwrap(), \n + ), \n + None \n + ) \n + } \n CPlaceInner::Var(_) => bug!(\"Expected CPlace::Addr, found CPlace::Var\"), \n } \n }", "msg": "Correctly align returned addr for to_addr on NoPlace"}
{"diff": "a / src/debuginfo/mod.rs \n  b / src/debuginfo/mod.rs \n@@ -257,6 +257,10 @@ pub(crate) fn define( \n source_info_set: &indexmap::IndexSet<SourceInfo>, \n local_map: FxHashMap<mir::Local, CPlace<'tcx>>, \n ) { \n + if isa.get_mach_backend().is_some() { \n + return; // The AArch64 backend doesn't support line debuginfo yet. \n + } \n + \n let end = self.create_debug_lines(context, isa, source_info_set); \n self.debug_context", "msg": "Disable line debuginfo for the AArch64 backend"}
{"diff": "a / src/intrinsics/mod.rs \n  b / src/intrinsics/mod.rs \nmacro validate_atomic_type($fx:ident, $intrinsic:ident, $span:ident, $ty:expr) { \n match $ty.kind() { \n - ty::Uint(_) | ty::Int(_) => {} \n + ty::Uint(_) | ty::Int(_) | ty::RawPtr(..) => {} \n _ => { \n $fx.tcx.sess.span_err( \n $span, \n &format!( \n - \"`{}` intrinsic: expected basic integer type, found `{:?}`\", \n + \"`{}` intrinsic: expected basic integer or raw pointer type, found `{:?}`\", \n $intrinsic, $ty \n ), \n );", "msg": "Allow cranelift to handle atomic pointers"}
{"diff": "a / src/value_and_place.rs \n  b / src/value_and_place.rs \n@@ -480,17 +480,19 @@ fn assert_assignable<'tcx>( \n // fn(&T) -> for<'l> fn(&'l T) is allowed \n } \n (&ty::Dynamic(from_traits, _), &ty::Dynamic(to_traits, _)) => { \n - let from_traits = fx \n + for (from, to) in from_traits.iter().zip(to_traits) { \n + let from = fx \n .tcx \n - .normalize_erasing_late_bound_regions(ParamEnv::reveal_all(), from_traits); \n - let to_traits = fx \n + .normalize_erasing_late_bound_regions(ParamEnv::reveal_all(), from); \n + let to = fx \n .tcx \n - .normalize_erasing_late_bound_regions(ParamEnv::reveal_all(), to_traits); \n + .normalize_erasing_late_bound_regions(ParamEnv::reveal_all(), to); \n assert_eq!( \n - from_traits, to_traits, \n + from, to, \n \"Can't write trait object of incompatible traits {:?} to place with traits {:?}\\n\\n{:#?}\", \n from_traits, to_traits, fx, \n ); \n + } \n // dyn for<'r> Trait<'r> -> dyn Trait<'_> is allowed \n } \n _ => {", "msg": "Move binder for dyn to each list item"}
{"diff": "a / src/pretty_clif.rs \n  b / src/pretty_clif.rs \n@@ -201,9 +201,7 @@ pub(crate) fn add_comment<S: Into<String> + AsRef<str>, E: Into<AnyEntity>>( \n } \n pub(crate) fn should_write_ir(tcx: TyCtxt<'_>) -> bool { \n - cfg!(debug_assertions) \n - || tcx \n - .sess \n + tcx.sess \n .opts \n .output_types \n .contains_key(&OutputType::LlvmAssembly)", "msg": "Don't write clif ir by default when debug assertions are enabled"}
{"diff": "a / src/abi/returning.rs \n  b / src/abi/returning.rs \n@@ -44,9 +44,9 @@ pub(crate) fn can_return_to_ssa_var<'tcx>( \n FnAbi::of_fn_ptr(&RevealAllLayoutCx(fx.tcx), fn_ty.fn_sig(fx.tcx), &extra_args) \n }; \n match fn_abi.ret.mode { \n - PassMode::Ignore | PassMode::Direct(_) | PassMode::Pair(_, _) => true, \n - // FIXME Make it possible to return Cast and Indirect to an ssa var. \n - PassMode::Cast(_) | PassMode::Indirect { .. } => false, \n + PassMode::Ignore | PassMode::Direct(_) | PassMode::Pair(_, _) | PassMode::Cast(_) => true, \n + // FIXME Make it possible to return Indirect to an ssa var. \n + PassMode::Indirect { .. } => false, \n } \n }", "msg": "Allow returning PassMode::Cast directly to an ssa var"}
{"diff": "a / build_system/build_sysroot.rs \n  b / build_system/build_sysroot.rs \n@@ -193,8 +193,6 @@ fn build_clif_sysroot_for_triple( \n \"RUSTC\", \n env::current_dir().unwrap().join(target_dir).join(\"bin\").join(\"cg_clif_build_sysroot\"), \n ); \n - // FIXME Enable incremental again once rust-lang/rust#74946 is fixed \n - build_cmd.env(\"CARGO_INCREMENTAL\", \"0\").env(\"__CARGO_DEFAULT_LIB_METADATA\", \"cg_clif\"); \n spawn_and_wait(build_cmd); \n // Copy all relevant files to the sysroot", "msg": "Re-enable incremental compilation for the sysroot\nrust-lang/rust#74946 for fixed"}
{"diff": "a / src/inline_asm.rs \n  b / src/inline_asm.rs \n@@ -108,7 +108,7 @@ pub(crate) fn codegen_inline_asm<'tcx>( \n let mut asm_gen = InlineAssemblyGenerator { \n tcx: fx.tcx, \n - arch: InlineAsmArch::X86_64, \n + arch: fx.tcx.sess.asm_arch.unwrap(), \n template, \n operands, \n options, \n @@ -306,12 +306,8 @@ fn allocate_stack_slots(&mut self) { \n let mut slots_output = vec![None; self.operands.len()]; \n let new_slot_fn = |slot_size: &mut Size, reg_class: InlineAsmRegClass| { \n - let reg_size = reg_class \n - .supported_types(InlineAsmArch::X86_64) \n - .iter() \n - .map(|(ty, _)| ty.size()) \n - .max() \n - .unwrap(); \n + let reg_size = \n + reg_class.supported_types(self.arch).iter().map(|(ty, _)| ty.size()).max().unwrap(); \n let align = rustc_target::abi::Align::from_bytes(reg_size.bytes()).unwrap(); \n let offset = slot_size.align_to(align); \n *slot_size = offset + reg_size;", "msg": "Dispatch inline asm to the correct arch"}
{"diff": "a / src/inline_asm.rs \n  b / src/inline_asm.rs \n@@ -18,10 +18,7 @@ pub(crate) fn codegen_inline_asm<'tcx>( \n ) { \n // FIXME add .eh_frame unwind info directives \n - if template.is_empty() { \n - // Black box \n - return; \n - } else if template[0] == InlineAsmTemplatePiece::String(\"int $$0x29\".to_string()) { \n + if template[0] == InlineAsmTemplatePiece::String(\"int $$0x29\".to_string()) { \n let true_ = fx.bcx.ins().iconst(types::I32, 1); \n fx.bcx.ins().trapnz(true_, TrapCode::User(1)); \n return;", "msg": "Remove black box inline asm fallback\nIt isn't used anymore since the introduction of the black_box intrinsic"}
{"diff": "a / scripts/cargo.rs \n  b / scripts/cargo.rs \n@@ -42,7 +42,7 @@ fn main() { \n \"RUSTFLAGS\", \n env::var(\"RUSTFLAGS\").unwrap_or(String::new()) + \" -Cprefer-dynamic\", \n ); \n - std::array::IntoIter::new([\"rustc\".to_string()]) \n + IntoIterator::into_iter([\"rustc\".to_string()]) \n .chain(env::args().skip(2)) \n .chain([ \n \"--\".to_string(), \n @@ -56,7 +56,7 @@ fn main() { \n \"RUSTFLAGS\", \n env::var(\"RUSTFLAGS\").unwrap_or(String::new()) + \" -Cprefer-dynamic\", \n ); \n - std::array::IntoIter::new([\"rustc\".to_string()]) \n + IntoIterator::into_iter([\"rustc\".to_string()]) \n .chain(env::args().skip(2)) \n .chain([ \n \"--\".to_string(),", "msg": "Use IntoIterator for array impl everywhere."}
{"diff": "a / src/archive.rs \n  b / src/archive.rs \n@@ -105,8 +105,6 @@ fn add_archive<F>(&mut self, archive_path: &Path, mut skip: F) -> std::io::Resul \n Ok(()) \n } \n - fn update_symbols(&mut self) {} \n - \n fn build(mut self) { \n enum BuilderKind { \n Bsd(ar::Builder<File>),", "msg": "Unconditionally update symbols\nAll paths to an ArchiveBuilder::build call update_symbols first."}
{"diff": "a / src/archive.rs \n  b / src/archive.rs \n@@ -61,19 +61,6 @@ fn new(sess: &'a Session, output: &Path, input: Option<&Path>) -> Self { \n } \n } \n - fn src_files(&mut self) -> Vec<String> { \n - self.entries.iter().map(|(name, _)| String::from_utf8(name.clone()).unwrap()).collect() \n - } \n - \n - fn remove_file(&mut self, name: &str) { \n - let index = self \n - .entries \n - .iter() \n - .position(|(entry_name, _)| entry_name == name.as_bytes()) \n - .expect(\"Tried to remove file not existing in src archive\"); \n - self.entries.remove(index); \n - } \n - \n fn add_file(&mut self, file: &Path) { \n self.entries.push(( \n file.file_name().unwrap().to_str().unwrap().to_string().into_bytes(),", "msg": "Remove src_files and remove_file\nThey only apply to the main source archive and their role can be\nfulfilled through the skip argument of add_archive too."}
{"diff": "a / src/value_and_place.rs \n  b / src/value_and_place.rs \n@@ -324,6 +324,12 @@ pub(crate) fn new_stack_slot( \n }; \n } \n + if layout.size.bytes() >= u64::from(u32::MAX - 16) { \n + fx.tcx \n + .sess \n + .fatal(&format!(\"values of type {} are too big to store on the stack\", layout.ty)); \n + } \n + \n let stack_slot = fx.bcx.create_stack_slot(StackSlotData { \n kind: StackSlotKind::ExplicitSlot, \n // FIXME Don't force the size to a multiple of 16 bytes once Cranelift gets a way to", "msg": "Don't crash when local variables are too big to store on the stack"}
{"diff": "a / src/abi/pass_mode.rs \n  b / src/abi/pass_mode.rs \n@@ -193,7 +193,7 @@ pub(super) fn from_casted_value<'tcx>( \n // larger alignment than the integer. \n size: (std::cmp::max(abi_param_size, layout_size) + 15) / 16 * 16, \n }); \n - let ptr = Pointer::new(fx.bcx.ins().stack_addr(pointer_ty(fx.tcx), stack_slot, 0)); \n + let ptr = Pointer::stack_slot(stack_slot); \n let mut offset = 0; \n let mut block_params_iter = block_params.iter().copied(); \n for param in abi_params {", "msg": "Use `stack_store` instead of `stack_addr`+`store` when building structs"}
{"diff": "a / src/driver/aot.rs \n  b / src/driver/aot.rs \n@@ -108,6 +108,8 @@ pub(crate) fn join( \n self.concurrency_limiter.finished(); \n + sess.abort_if_errors(); \n + \n ( \n CodegenResults { \n modules, \n @@ -411,8 +413,6 @@ pub(crate) fn run_aot( \n .collect::<Vec<_>>() \n }); \n - tcx.sess.abort_if_errors(); \n - \n let mut allocator_module = make_module(tcx.sess, &backend_config, \"allocator_shim\".to_string()); \n let mut allocator_unwind_context = UnwindContext::new(allocator_module.isa(), true); \n let created_alloc_shim =", "msg": "Ensure Cranelift errors are reported deterministically\nThis may also have been the root cause of"}
{"diff": "a / y.rs \n  b / y.rs \n# This block is ignored by rustc \n set -e \n echo \"[BUILD] y.rs\" 1>&2 \n -rustc $0 -o ${0/.rs/.bin} -Cdebuginfo=1 --edition 2021 \n +rustc $0 -o ${0/.rs/.bin} -Cdebuginfo=1 --edition 2021 -Cpanic=abort \n exec ${0/.rs/.bin} $@ \n */", "msg": "Set panic=abort for the build system\nThis saves about 60ms of build time"}
{"diff": "a / build_system/build_sysroot.rs \n  b / build_system/build_sysroot.rs \n@@ -103,6 +103,10 @@ struct SysrootTarget { \n impl SysrootTarget { \n fn install_into_sysroot(&self, sysroot: &Path) { \n + if self.libs.is_empty() { \n + return; \n + } \n + \n let target_rustlib_lib = sysroot.join(\"lib\").join(\"rustlib\").join(&self.triple).join(\"lib\"); \n fs::create_dir_all(&target_rustlib_lib).unwrap();", "msg": "Skip creating sysroot target dir if it will be empty"}
{"diff": "a / src/command/utils.rs \n  b / src/command/utils.rs \n@@ -16,15 +16,10 @@ pub fn find_pkg_directory(guess_path: &str) -> Option<PathBuf> { \n } \n path.read_dir().ok().and_then(|entries| { \n - for entry in entries { \n - if entry.is_ok() { \n - let p = entry.unwrap().path(); \n - if is_pkg_directory(&p) { \n - return Some(p); \n - } \n - } \n - } \n - None \n + entries \n + .filter(|x| x.is_ok()) \n + .map(|x| x.unwrap().path()) \n + .find(|x| is_pkg_directory(&x)) \n }) \n }", "msg": "Write as more ideomatic rust"}
{"diff": "a / tests/manifest/main.rs \n  b / tests/manifest/main.rs \n@@ -86,7 +86,6 @@ fn it_creates_a_package_json_provided_path() { \n assert_eq!(pkg.name, \"js-hello-world\"); \n assert_eq!(pkg.main, \"js_hello_world.js\"); \n - \n let actual_files: HashSet<String> = pkg.files.into_iter().collect(); \n let expected_files: HashSet<String> = [ \n \"js_hello_world_bg.wasm\",", "msg": "One more rustfmt run!"}
{"diff": "a / src/main.rs \n  b / src/main.rs \n@@ -16,7 +16,7 @@ mod installer; \n fn main() { \n setup_panic!(); \n if let Err(e) = run() { \n - eprintln!(\"{}\", e); \n + eprintln!(\"Error: {}\", e); \n for cause in e.iter_causes() { \n eprintln!(\"Caused by: {}\", cause); \n }", "msg": "Add \"Error:\" prefix to error messages"}
{"diff": "a / src/command/build.rs \n  b / src/command/build.rs \n@@ -184,7 +184,15 @@ type BuildStep = fn(&mut Build) -> Result<(), Error>; \n impl Build { \n /// Construct a build command from the given options. \n - pub fn try_from_opts(build_opts: BuildOptions) -> Result<Self, Error> { \n + pub fn try_from_opts(mut build_opts: BuildOptions) -> Result<Self, Error> { \n + if let Some(path) = &build_opts.path { \n + if path.to_string_lossy().starts_with(\"--\") { \n + let path = build_opts.path.take().unwrap(); \n + build_opts \n + .extra_options \n + .insert(0, path.to_string_lossy().into_owned().to_string()); \n + } \n + } \n let crate_path = get_crate_path(build_opts.path)?; \n let crate_data = manifest::CrateData::new(&crate_path, build_opts.out_name.clone())?; \n let out_dir = crate_path.join(PathBuf::from(build_opts.out_dir));", "msg": "Only parse first build argument as path if it does not start with hyphens"}
{"diff": "a / src/new.rs \n  b / src/new.rs \n@@ -96,7 +96,7 @@ pub fn create_new_document(doc_type: &str, name: &str, config: &Config) -> Resul \n match doc_type { \n \"page\" => create_file(name, index_liquid)?, \n \"post\" => create_file(full_path, post_1_md)?, \n - _ => bail!(\"Can only create post or page\") \n + _ => bail!(\"Unsupported document type {}\", doc_type) \n } \n Ok(())", "msg": "Better bail message"}
{"diff": "a / src/bin/cobalt/serve.rs \n  b / src/bin/cobalt/serve.rs \n@@ -32,6 +32,14 @@ pub fn serve_command_args() -> clap::App<'static, 'static> { \n .default_value(\"3000\") \n .takes_value(true), \n ) \n + .arg( \n + clap::Arg::with_name(\"host\") \n + .long(\"host\") \n + .value_name(\"host-name/IP\") \n + .help(\"Host to serve from\") \n + .default_value(\"localhost\") \n + .takes_value(true), \n + ) \n .arg( \n clap::Arg::with_name(\"no-watch\") \n .long(\"no-watch\") \n @@ -42,8 +50,9 @@ pub fn serve_command_args() -> clap::App<'static, 'static> { \n } \n pub fn serve_command(matches: &clap::ArgMatches) -> Result<()> { \n + let host = matches.value_of(\"host\").unwrap().to_string(); \n let port = matches.value_of(\"port\").unwrap().to_string(); \n - let ip = format!(\"127.0.0.1:{}\", port); \n + let ip = format!(\"{}:{}\", host, port); \n let mut config = args::get_config(matches)?; \n debug!(\"Overriding config `site.base_url` with `{}`\", ip);", "msg": "add option to bind the serve to a different host\nThis is useful when you are testing/coding your site from a remote host\nfor example over ssh."}
{"diff": "a / build.rs \n  b / build.rs \n@@ -32,10 +32,12 @@ fn gen_swizzle_functions(variables: &'static str, upto: usize) -> String { \n let nn = (variables.len()+1).pow(upto as u32); \n for i in 1..nn { \n if let Some((swizzle_name, swizzle_impl)) = gen_swizzle_nth(variables, i, upto) { \n - let vec_type = format!(\"$vector_type{}\", swizzle_name.len()); \n + let dim = format!(\"{}\", swizzle_name.len()); \n result.push_str( \n - &format!(\" #[inline] pub fn {0}(&self) -> {2}<$S> {{ {2}::new({1}) }}\\n\", \n - swizzle_name, swizzle_impl, vec_type)); \n + &format!(\" \n + /// Swizzle operator that creates a new type with dimension {2} from variables `{0}`. \n + #[inline] pub fn {0}(&self) -> $vector_type{2}<$S> {{ $vector_type{2}::new({1}) }}\\n\", \n + swizzle_name, swizzle_impl, dim)); \n } \n } \n result", "msg": "Add a doc explanation to each swizzle function."}
{"diff": "a / src/vector.rs \n  b / src/vector.rs \n@@ -99,7 +99,7 @@ macro_rules! impl_vector { \n impl<S> $VectorN<S> { \n /// Construct a new vector, using the provided values. \n #[inline] \n - pub fn new($($field: S),+) -> $VectorN<S> { \n + pub const fn new($($field: S),+) -> $VectorN<S> { \n $VectorN { $($field: $field),+ } \n } \n @@ -115,7 +115,7 @@ macro_rules! impl_vector { \n /// The short constructor. \n #[inline] \n - pub fn $constructor<S>($($field: S),+) -> $VectorN<S> { \n + pub const fn $constructor<S>($($field: S),+) -> $VectorN<S> { \n $VectorN::new($($field),+) \n }", "msg": "Declare vector constructors to be const\nThis makes it easier to create vectors in constants."}
{"diff": "a / src/parse/mod.rs \n  b / src/parse/mod.rs \n@@ -19,7 +19,7 @@ impl FromStr for ReplCommand { \n type Err = ParseError; \n fn from_str(src: &str) -> Result<ReplCommand, ParseError> { \n - grammar::parse_ReplCommand(src).map_err(|e| ParseError(format!(\"{:?}\", e))) \n + grammar::parse_ReplCommand(src).map_err(|e| ParseError(format!(\"{}\", e))) \n } \n } \n @@ -39,7 +39,7 @@ impl FromStr for Term { \n type Err = ParseError; \n fn from_str(src: &str) -> Result<Term, ParseError> { \n - grammar::parse_Term(src).map_err(|e| ParseError(format!(\"{:?}\", e))) \n + grammar::parse_Term(src).map_err(|e| ParseError(format!(\"{}\", e))) \n } \n }", "msg": "Use display for error message printing"}
{"diff": "a / pikelet/src/core/mod.rs \n  b / pikelet/src/core/mod.rs \n@@ -112,18 +112,6 @@ impl Term { \n } \n } \n -/// The local value environment. \n -pub struct Locals { \n - // TODO: values, \n -} \n - \n -impl Locals { \n - /// Create a new local environment. \n - pub fn new() -> Locals { \n - Locals {} \n - } \n -} \n - \n /// Values in the core language. \n #[derive(Clone, Debug, PartialEq)] \n pub enum Value { \n @@ -263,6 +251,18 @@ impl Default for Globals { \n } \n } \n +/// The local value environment. \n +pub struct Locals { \n + // TODO: values, \n +} \n + \n +impl Locals { \n + /// Create a new local environment. \n + pub fn new() -> Locals { \n + Locals {} \n + } \n +} \n + \n pub trait HasType { \n fn r#type() -> Arc<Value>; \n }", "msg": "Move locals to a more logical place in module"}
{"diff": "a / circuits/plonk-15-wires/src/wires.rs \n  b / circuits/plonk-15-wires/src/wires.rs \n@@ -5,6 +5,7 @@ This source file implements Plonk circuit gate wires primitive. \n *****************************************************************************************************************/ \n use algebra::bytes::{FromBytes, ToBytes}; \n +use array_init::array_init; \n use std::io::{Read, Result as IoResult, Write}; \n pub const GENERICS: usize = 3; \n @@ -18,6 +19,12 @@ pub struct Wire { \n pub col: usize, \n } \n +impl Wire { \n + /// Creates a new set of wires for a given row. \n + pub fn new(row: usize) -> [Self; COLUMNS] { \n + array_init(|col| Self { row, col }) \n + } \n +} \n pub type GateWires = [Wire; COLUMNS]; \n impl ToBytes for Wire {", "msg": "[15-wires] add a helper function for creating wires quickly"}
{"diff": "a / dlog/kimchi/src/prover.rs \n  b / dlog/kimchi/src/prover.rs \n@@ -308,7 +308,7 @@ where \n let (lookup_aggreg_coeffs, lookup_aggreg_comm, lookup_aggreg8) = \n // compute lookup aggregation polynomial \n - match (index.cs.lookup_constraint_system.as_ref(), lookup_sorted.as_ref()) { \n + match (index.cs.lookup_constraint_system.as_ref(), lookup_sorted) { \n (None, None) | (None, Some(_)) | (Some(_), None) => (None, None, None), \n (Some(lcs), Some(lookup_sorted)) => { \n let iter_lookup_table = || (0..d1_size).map(|i| {", "msg": "Move value rather than passing reference, to ensure early drop"}
{"diff": "a / kimchi/src/index.rs \n  b / kimchi/src/index.rs \n@@ -417,3 +417,23 @@ where \n .map_err(|e| e.to_string()) \n } \n } \n + \n +pub mod testing { \n + use super::*; \n + use crate::circuits::gate::CircuitGate; \n + use commitment_dlog::srs::endos; \n + use mina_curves::pasta::{pallas::Affine as Other, vesta::Affine, Fp}; \n + \n + pub fn new_index_for_test(gates: Vec<CircuitGate<Fp>>, public: usize) -> Index<Affine> { \n + let fp_sponge_params = oracle::pasta::fp::params(); \n + let cs = ConstraintSystem::<Fp>::create(gates, vec![], fp_sponge_params, public).unwrap(); \n + \n + let mut srs = SRS::<Affine>::create(cs.domain.d1.size as usize); \n + srs.add_lagrange_basis(cs.domain.d1); \n + let srs = Arc::new(srs); \n + \n + let fq_sponge_params = oracle::pasta::fq::params(); \n + let (endo_q, _endo_r) = endos::<Other>(); \n + Index::<Affine>::create(cs, fq_sponge_params, endo_q, srs) \n + } \n +}", "msg": "[kimchi][index] handy function to create an index based on a circuit"}
{"diff": "a / kimchi/src/linearization.rs \n  b / kimchi/src/linearization.rs \n@@ -73,6 +73,14 @@ pub fn constraints_expr<F: FftField + SquareRootField>( \n expr += combined; \n } \n + // the generic gate must be associated with alpha^0 \n + // to make the later addition with the public input work \n + if cfg!(debug_assertions) { \n + let mut generic_alphas = \n + powers_of_alpha.get_exponents(ArgumentType::Gate(GateType::Generic), 1); \n + assert_eq!(generic_alphas.next(), Some(0)); \n + } \n + \n // return the expression \n (expr, powers_of_alpha) \n }", "msg": "enforce that alpha^0 is used for generic gate"}
{"diff": "a / kimchi/src/proof.rs \n  b / kimchi/src/proof.rs \n@@ -13,12 +13,12 @@ use serde_with::serde_as; \n //~ spec:startcode \n #[serde_as] \n #[derive(Clone, Serialize, Deserialize)] \n -pub struct LookupEvaluations<Field> { \n - /// sorted lookup table polynomial \n #[serde(bound( \n serialize = \"Vec<o1_utils::serialization::SerdeAs>: serde_with::SerializeAs<Field>\", \n deserialize = \"Vec<o1_utils::serialization::SerdeAs>: serde_with::DeserializeAs<'de, Field>\" \n ))] \n +pub struct LookupEvaluations<Field> { \n + /// sorted lookup table polynomial \n #[serde_as(as = \"Vec<Vec<o1_utils::serialization::SerdeAs>>\")] \n pub sorted: Vec<Field>, \n /// lookup aggregation polynomial", "msg": "Move LookupEvaluations serde bounds to the top level"}
{"diff": "a / kimchi/src/snarky/constraint_system.rs \n  b / kimchi/src/snarky/constraint_system.rs \n@@ -12,7 +12,7 @@ pub trait GateVector<Field: FftField> { \n fn create() -> Self; \n fn add(&mut self, gate: CircuitGate<Field>); \n fn get(&self, idx: usize) -> CircuitGate<Field>; \n - fn digest(&self) -> Vec<u8>; \n + fn digest(&self) -> [u8; 32]; \n } \n /** A row indexing in a constraint system. \n @@ -206,7 +206,7 @@ enum Circuit<Field, RustGates> { \n /** Once finalized, a circuit is represented as a digest \n and a list of gates that corresponds to the circuit. \n */ \n - Compiled(Vec<u8>, RustGates), \n + Compiled([u8; 32], RustGates), \n } \n /** The constraint system. */", "msg": "[snarky] use [u8; 32] for the digest of the circuit"}
{"diff": "a / utils/src/field_helpers.rs \n  b / utils/src/field_helpers.rs \n@@ -37,9 +37,7 @@ pub trait FieldHelpers<F> { \n where \n F: PrimeField \n { \n - let mut bytes = big.to_bytes_le(); \n - bytes.resize(F::size_in_bytes(), 0); \n - F::from_bytes(&bytes) \n + big.try_into().map_err(|_| FieldHelpersError::DeserializeBytes) \n } \n /// Serialize to bytes", "msg": "Switch to simpler method in field helpers for from_biguint"}
{"diff": "a / poly-commitment/src/evaluation_proof.rs \n  b / poly-commitment/src/evaluation_proof.rs \n@@ -130,9 +130,11 @@ impl<G: CommitmentCurve> SRS<G> { \n scale *= &polyscale; \n offset += self.g.len(); \n if let Some(m) = degree_bound { \n + if offset >= *m { \n if offset > *m { \n // mixing in the shifted segment since degree is bounded \n plnm.add_shifted(scale, self.g.len() - m % self.g.len(), segment); \n + } \n omega += &(omegas.shifted.unwrap() * scale); \n scale *= &polyscale; \n }", "msg": "Mix in blinding factors even when the shifted polynomial is zero"}
{"diff": "a / kimchi/src/tests/xor.rs \n  b / kimchi/src/tests/xor.rs \n@@ -326,6 +326,7 @@ fn test_prove_and_verify_one_not_gnrc() { \n let next_row = CircuitGate::<Fp>::extend_not_gnrc_gadget(&mut gates, 1, 0, 1); \n (next_row, gates) \n }; \n + \n // Temporary workaround for lookup-table/domain-size issue \n for _ in 0..(1 << 13) { \n gates.push(CircuitGate::zero(Wire::for_row(next_row))); \n @@ -333,7 +334,16 @@ fn test_prove_and_verify_one_not_gnrc() { \n } \n // Create witness and random inputs \n - let witness = xor::create_not_gnrc_witness(&vec![big_random(bits)], bits); \n + let witness: [Vec<PallasField>; 15] = not_gnrc_witness( \n + &vec![ \n + big_random(bits), \n + big_random(bits), \n + big_random(bits), \n + big_random(bits), \n + big_random(bits), \n + ], \n + bits, \n + ); \n TestFramework::default() \n .gates(gates)", "msg": "use the corresponding witness creator for end-to-end test"}
{"diff": "a / wallet-crypto/src/address.rs \n  b / wallet-crypto/src/address.rs \n@@ -130,6 +130,9 @@ mod cbor { \n pub fn cbor_array_start(nb_elems: usize, buf: &mut Vec<u8>) { \n write_length_encoding(MajorType::ARRAY, nb_elems, buf); \n } \n + pub fn cbor_map_start(nb_elems: usize, buf: &mut Vec<u8>) { \n + write_length_encoding(MajorType::MAP, nb_elems, buf); \n + } \n } \n @@ -346,7 +349,11 @@ impl Attributes { \n } \n impl ToCBOR for Attributes { \n fn encode(&self, buf: &mut Vec<u8>) { \n + cbor::cbor_map_start(2, buf); \n + // TODO \n + cbor::cbor_uint_small(0, buf); \n self.derivation_path.encode(buf); \n + cbor::cbor_uint_small(1, buf); \n self.stake_distribution.encode(buf) \n } \n }", "msg": "use a map for Attributes"}
{"diff": "a / wallet-crypto/src/address.rs \n  b / wallet-crypto/src/address.rs \n@@ -323,7 +323,9 @@ impl HDAddressPayload { \n } \n impl ToCBOR for HDAddressPayload { \n fn encode(&self, buf: &mut Vec<u8>) { \n - cbor::cbor_bs(self.as_ref(),buf) \n + let mut vec = vec![]; \n + cbor::cbor_bs(self.as_ref(), &mut vec); \n + cbor::cbor_bs(&vec , buf); \n } \n } \n @@ -352,9 +354,9 @@ impl ToCBOR for Attributes { \n cbor::cbor_map_start(2, buf); \n // TODO \n cbor::cbor_uint_small(0, buf); \n - self.derivation_path.encode(buf); \n + self.stake_distribution.encode(buf); \n cbor::cbor_uint_small(1, buf); \n - self.stake_distribution.encode(buf) \n + self.derivation_path.encode(buf); \n } \n }", "msg": "fix order and serialise in cborg before serialising in cborg"}
{"diff": "a / wallet-wasm/src/lib.rs \n  b / wallet-wasm/src/lib.rs \n@@ -9,6 +9,7 @@ use self::rcw::digest::{Digest}; \n use self::wallet_crypto::hdwallet; \n use self::wallet_crypto::paperwallet; \n +use self::wallet_crypto::address; \n use std::mem; \n use std::ffi::{CStr, CString}; \n @@ -174,3 +175,15 @@ pub extern \"C\" fn blake2b_256(msg_ptr: *const c_uchar, msg_sz: usize, out: *mut \n b2b.result(&mut outv); \n unsafe { write_data(&outv, out) } \n } \n + \n +#[no_mangle] \n +pub extern \"C\" fn wallet_public_to_address(xpub_ptr: *const c_uchar, out: *mut c_uchar) { \n + let xpub = unsafe { read_xpub(xpub_ptr) }; \n + let addr_type = address::AddrType::ATPubKey; \n + let hdap = address::HDAddressPayload::new(&[1,2,3,4,5]); // FIXME \n + let sd = address::SpendingData::PubKeyASD(xpub.clone()); \n + let attrs = address::Attributes::new_single_key(&xpub, Some(hdap)); \n + let ea = address::ExtendedAddr::new(addr_type, sd, attrs); \n + \n + // FIXME return \n +}", "msg": "add wasm export placeholder for address generation"}
{"diff": "a / wallet-crypto/src/bip44.rs \n  b / wallet-crypto/src/bip44.rs \n@@ -29,6 +29,19 @@ impl Addressing { \n Path::new(vec![BIP44_PURPOSE, BIP44_COIN_TYPE, self.account, self.change, self.index]) \n } \n + pub fn from_path(path: Path) -> Option<Self> { \n + if path.as_ref().len() != 5 { return None; } \n + if path.as_ref()[0] != BIP44_PURPOSE { return None; } \n + if path.as_ref()[1] != BIP44_COIN_TYPE { return None; } \n + if path.as_ref()[2] < 0x80000000 { return None; } \n + \n + Some(Addressing { \n + account: path.as_ref()[2], \n + change: path.as_ref()[3], \n + index: path.as_ref()[4], \n + }) \n + } \n + \n pub fn incr(&self, incr: u32) -> Option<Self> { \n if incr >= 0x80000000 { return None; } \n let mut addr = self.clone();", "msg": "add function to retrieve an Addressing from a Path"}
{"diff": "a / wallet-cli/src/storage/mod.rs \n  b / wallet-cli/src/storage/mod.rs \n@@ -125,6 +125,7 @@ pub fn block_location(storage: &Storage, hash: &BlockHash) -> Option<BlockLocati \n match nb { \n pack::FanoutNb(0) => {}, \n _ => { \n + if lookup.bloom.search(hash) { \n let idx_filepath = storage.config.get_index_filepath(packref); \n let mut idx_file = fs::File::open(idx_filepath).unwrap(); \n match pack::search_index(&mut idx_file, hash, start, nb) { \n @@ -134,6 +135,7 @@ pub fn block_location(storage: &Storage, hash: &BlockHash) -> Option<BlockLocati \n } \n } \n } \n + } \n if blob::exist(storage, hash) { \n return Some(BlockLocation::Loose); \n }", "msg": "search in bloom before looking for index in hashes"}
{"diff": "a / blockchain/src/block.rs \n  b / blockchain/src/block.rs \n@@ -19,6 +19,13 @@ impl BlockHeader { \n &BlockHeader::MainBlockHeader(ref blo) => blo.previous_header.clone(), \n } \n } \n + \n + pub fn get_slotid(&self) -> SlotId { \n + match self { \n + &BlockHeader::GenesisBlockHeader(ref blo) => SlotId { epoch: blo.consensus.epoch, slotid: 0 }, \n + &BlockHeader::MainBlockHeader(ref blo) => blo.consensus.slot_id.clone(), \n + } \n + } \n } \n impl fmt::Display for BlockHeader {", "msg": "add ability for block header to report their slotid"}
{"diff": "a / blockchain/src/block.rs \n  b / blockchain/src/block.rs \n@@ -46,6 +46,15 @@ pub enum Block { \n GenesisBlock(genesis::Block), \n MainBlock(normal::Block), \n } \n +impl Block { \n + pub fn get_header(&self) -> BlockHeader { \n + match self { \n + Block::GenesisBlock(blk) => BlockHeader::GenesisBlockHeader(blk.header.clone()), \n + Block::MainBlock(blk) => BlockHeader::MainBlockHeader(blk.header.clone()), \n + } \n + } \n +} \n + \n impl fmt::Display for Block { \n fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result { \n match self {", "msg": "add a way to generate the block header from the block"}
{"diff": "a / blockchain/src/block.rs \n  b / blockchain/src/block.rs \n@@ -26,6 +26,11 @@ impl BlockHeader { \n &BlockHeader::MainBlockHeader(ref blo) => blo.consensus.slot_id.clone(), \n } \n } \n + \n + pub fn compute_hash(&self) -> HeaderHash { \n + let v = cbor::encode_to_cbor(self).unwrap(); \n + HeaderHash::new(&v[..]) \n + } \n } \n impl fmt::Display for BlockHeader {", "msg": "add a way to compute hash from block header"}
{"diff": "a / wallet-cli/src/command/wallet/config.rs \n  b / wallet-cli/src/command/wallet/config.rs \n@@ -70,9 +70,9 @@ pub struct Config { \n impl Config { \n /// construct a wallet configuration from the given wallet and blockchain name \n /// \n - pub fn from_wallet(wallet: Wallet, blockchain: PathBuf) -> Self { \n + pub fn from_wallet<P: Into<PathBuf>>(wallet: Wallet, blockchain: P) -> Self { \n Config { \n - blockchain: blockchain, \n + blockchain: blockchain.into(), \n selection_fee_policy: wallet.selection_policy, \n cached_root_key: wallet.cached_root_key \n }", "msg": "allow more generic function inputs"}
{"diff": "a / exe-common/src/config.rs \n  b / exe-common/src/config.rs \n@@ -194,6 +194,7 @@ pub mod net { \n pub fn mainnet() -> Self { \n let mut peers = Peers::new(); \n peers.push(\"iohk-hosts\".to_string(), Peer::native(\"relays.cardano-mainnet.iohk.io:3000\".to_string())); \n + peers.push(\"hermes\".to_string(), Peer::http(\"http://hermes.dev.iohkdev.io\".to_string())); \n Config { \n genesis: HeaderHash::from_hex(&\"89D9B5A5B8DDC8D7E5A6795E9774D97FAF1EFEA59B2CAF7EAF9F8C5B32059DF4\").unwrap(), \n genesis_prev: HeaderHash::from_hex(&\"5f20df933584822601f9e3f8c024eb5eb252fe8cefb24d1317dc3d432e940ebb\").unwrap(),", "msg": "allow mainnet to download epochs from mainnet"}
{"diff": "a / wallet-crypto/src/address.rs \n  b / wallet-crypto/src/address.rs \n@@ -294,6 +294,11 @@ impl ExtendedAddr { \n } \n } \n + // bootstrap era + no hdpayload address \n + pub fn new_simple(xpub: XPub) -> Self { \n + ExtendedAddr::new(AddrType::ATPubKey, SpendingData::PubKeyASD(xpub), Attributes::new_bootstrap_era(None)) \n + } \n + \n /// encode an `ExtendedAddr` to cbor with the extra details and `crc32` \n /// \n /// ```", "msg": "add a way to create simple addresses"}
{"diff": "a / wallet-cli/src/command/wallet/state/lookup.rs \n  b / wallet-cli/src/command/wallet/state/lookup.rs \n@@ -58,6 +58,9 @@ pub struct State<T: AddrLookup> { \n } \n impl <T: AddrLookup> State<T> { \n + pub fn new(ptr: StatePtr, lookup_struct: T, utxos: Utxos) -> Self { \n + State { ptr, lookup_struct, utxos } \n + } \n /// update a given state with a set of blocks. \n /// \n /// The blocks need to be in blockchain order,", "msg": "add smart constructor for State"}
{"diff": "a / wallet-cli/src/command/wallet/state/lookup.rs \n  b / wallet-cli/src/command/wallet/state/lookup.rs \n@@ -70,6 +70,9 @@ impl <T: AddrLookup> State<T> { \n for block in blocks { \n let hdr = block.get_header(); \n let date = hdr.get_blockdate(); \n + if date.is_genesis() { \n + info!(\"skipping genesis block: {}\", date); \n + } else { \n if self.ptr.latest_addr >= date { \n return Err(Error::BlocksInvalidDate) \n } \n @@ -91,13 +94,15 @@ impl <T: AddrLookup> State<T> { \n } \n } \n - let found_outputs = self.lookup_struct.lookup(&all_outputs[..]); \n - println!(\"found_outputs: {:?}\", found_outputs) \n + let found_outputs = self.lookup_struct.lookup(&all_outputs[..])?; \n + if ! found_outputs.is_empty() { \n + info!(\"found_outputs: {:?}\", found_outputs) \n + } \n // utxo \n }, \n } \n - \n + } \n // update the state \n self.ptr.latest_known_hash = hdr.compute_hash(); \n self.ptr.latest_addr = date;", "msg": "skip the lookup if the block is a genesis block"}
{"diff": "a / wallet-crypto/src/hdwallet.rs \n  b / wallet-crypto/src/hdwallet.rs \n@@ -77,6 +77,9 @@ pub enum DerivationScheme { \n V1, \n V2, \n } \n +impl Default for DerivationScheme { \n + fn default() -> Self { DerivationScheme::V2 } \n +} \n /// Seed used to generate the root private key of the HDWallet. \n ///", "msg": "add default instance for derivation scheme"}
{"diff": "a / wallet-crypto/src/hdwallet.rs \n  b / wallet-crypto/src/hdwallet.rs \n@@ -164,6 +164,31 @@ impl XPrv { \n Self::from_bytes(out) \n } \n + pub fn generate_from_daedalus_seed(seed: &Seed) -> Self { \n + let bytes = cbor::encode_to_cbor(&cbor::Value::Bytes(cbor::Bytes::from_slice(seed.as_ref()))).unwrap(); \n + let mut mac = Hmac::new(Sha512::new(), &bytes); \n + \n + let mut iter = 1; \n + let mut out = [0u8; XPRV_SIZE]; \n + \n + loop { \n + let s = format!(\"Root Seed Chain {}\", iter); \n + mac.reset(); \n + mac.input(s.as_bytes()); \n + let mut block = [0u8; 64]; \n + mac.raw_result(&mut block); \n + mk_ed25519_extended(&mut out[0..64], &block[0..32]); \n + \n + if (out[31] & 0x20) == 0 { \n + out[64..96].clone_from_slice(&block[32..64]); \n + break; \n + } \n + iter = iter + 1; \n + } \n + \n + Self::from_bytes(out) \n + } \n + \n pub fn generate_from_bip39(bytes: &bip39::Seed) -> Self { \n let mut out = [0u8; XPRV_SIZE];", "msg": "add a function that will generate a xprv from the equivalent daedalus method"}
{"diff": "a / cbor/src/se.rs \n  b / cbor/src/se.rs \n@@ -7,6 +7,11 @@ use len::Len; \n pub trait Serialize { \n fn serialize(&self, serializer: Serializer) -> Result<Serializer>; \n } \n +impl<'a, T: Serialize> Serialize for &'a T { \n + fn serialize(&self, serializer: Serializer) -> Result<Serializer> { \n + serializer.serialize(*self) \n + } \n +} \n impl Serialize for u32 { \n fn serialize(&self, serializer: Serializer) -> Result<Serializer> { \n serializer.write_unsigned_integer((*self) as u64)", "msg": "add support for double references"}
{"diff": "a / hermes/src/service.rs \n  b / hermes/src/service.rs \n-use std::sync::Arc; \n -use iron::Iron; \n +use config::{Config, Networks}; \n use handlers; \n +use iron::Iron; \n use router::Router; \n -use config::Config; \n +use std::sync::Arc; \n pub fn start(cfg: Config) { \n - let mut router = Router::new(); \n let networks = Arc::new(cfg.get_networks().unwrap()); \n + // start background thread to refresh sync blocks \n + start_http_server(cfg, networks); \n +} \n + \n +fn start_http_server(cfg: Config, networks: Arc<Networks>) { \n + let mut router = Router::new(); \n handlers::block::Handler::new(networks.clone()).route(&mut router); \n handlers::pack::Handler::new(networks.clone()).route(&mut router); \n handlers::epoch::Handler::new(networks.clone()).route(&mut router);", "msg": "separate http-serving part of service to own start function"}
{"diff": "a / hermes/src/service.rs \n  b / hermes/src/service.rs \nuse config::{Config, Networks}; \n use exe_common::sync; \n use handlers; \n -use iron::Iron; \n +use iron; \n use router::Router; \n use std::sync::Arc; \n @@ -11,13 +11,14 @@ pub fn start(cfg: Config) { \n start_http_server(cfg, networks); \n } \n -fn start_http_server(cfg: Config, networks: Arc<Networks>) { \n +fn start_http_server(cfg: &Config, networks: Arc<Networks>) -> iron::Listening { \n let mut router = Router::new(); \n handlers::block::Handler::new(networks.clone()).route(&mut router); \n handlers::pack::Handler::new(networks.clone()).route(&mut router); \n handlers::epoch::Handler::new(networks.clone()).route(&mut router); \n info!(\"listenting to port {}\", cfg.port); \n - Iron::new(router) \n + iron::Iron::new(router) \n .http(format!(\"0.0.0.0:{}\", cfg.port)) \n - .unwrap(); \n + .expect(\"start http server\") \n +} \n }", "msg": "include a message when the http server fails to start"}
{"diff": "a / wallet-crypto/src/fee.rs \n  b / wallet-crypto/src/fee.rs \n@@ -70,6 +70,18 @@ impl LinearFee { \n Ok(Fee(coin)) \n } \n } \n + \n +pub trait FeeAlgorithm { \n + fn calculate_for_tx(&self, tx: &Tx) -> Result<Fee>; \n +} \n + \n +impl FeeAlgorithm for LinearFee { \n + fn calculate_for_tx(&self, tx: &Tx) -> Result<Fee> { \n + let txbytes = cbor!(tx).unwrap(); \n + self.estimate(txbytes.len()) \n + } \n +} \n + \n impl Default for LinearFee { \n fn default() -> Self { LinearFee::new(155381.0, 43.946) } \n }", "msg": "add an api to calculate fee from Tx"}
{"diff": "a / wallet-crypto/src/tx.rs \n  b / wallet-crypto/src/tx.rs \n@@ -6,7 +6,7 @@ use raw_cbor::{self, de::RawCbor, se::{Serializer}}; \n use config::{Config}; \n use redeem; \n -use hdwallet::{Signature, XPub, XPrv}; \n +use hdwallet::{Signature, XPub, XPrv, XPUB_SIZE, SIGNATURE_SIZE}; \n use address::{ExtendedAddr, SpendingData}; \n use coin::{Coin}; \n @@ -78,6 +78,11 @@ impl fmt::Display for TxInWitness { \n } \n } \n impl TxInWitness { \n + /// this is used to create a fake signature useful for fee evaluation \n + pub fn fake() -> Self { \n + let fakesig = Signature::from_bytes([0u8;SIGNATURE_SIZE]); \n + TxInWitness::PkWitness(XPub::from_bytes([0u8;XPUB_SIZE]), fakesig) \n + } \n /// create a TxInWitness from a given private key `XPrv` for the given transaction id `TxId`. \n pub fn new(cfg: &Config, key: &XPrv, txid: &TxId) -> Self {", "msg": "add a way to create a fake signature"}
{"diff": "a / cardano/src/block/block.rs \n  b / cardano/src/block/block.rs \n@@ -151,10 +151,16 @@ impl BlockHeader { \n } \n pub fn to_raw(&self) -> RawBlockHeader { \n + // the only reason this would fail is if there was no more memory \n + // to allocate. This would be the users' last concern if it was the \n + // case \n RawBlockHeader(cbor!(self).unwrap()) \n } \n pub fn compute_hash(&self) -> HeaderHash { \n + // the only reason this would fail is if there was no more memory \n + // to allocate. This would be the users' last concern if it was the \n + // case \n let v = cbor!(self).unwrap(); \n HeaderHash::new(&v[..]) \n }", "msg": "document why it is _acceptable_ to have an unwrap here.\nrelates to"}
{"diff": "a / cardano/src/bip/bip44.rs \n  b / cardano/src/bip/bip44.rs \n@@ -254,6 +254,11 @@ pub struct Addressing { \n pub change: u32, \n pub index: Index, \n } \n +impl fmt::Display for Addressing { \n + fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result { \n + write!(f, \"{}.{}.{}\", self.account.0, self.change, self.index.0) \n + } \n +} \n #[derive(Clone, Copy, Serialize, Deserialize, Debug, PartialEq, Eq)] \n pub enum AddrType {", "msg": "add Display instance for Addressing"}
{"diff": "a / cardano-cli/src/utils/term/mod.rs \n  b / cardano-cli/src/utils/term/mod.rs \n@@ -62,6 +62,19 @@ impl Term { \n Progress::new_tick(self, 0) \n } \n + pub fn prompt(&mut self, prompt: &str) -> io::Result<String> { \n + use ::std::io::BufRead; \n + \n + let mut out = self.stdout.lock(); \n + write!(&mut out, \"{}\", prompt)?; \n + out.flush()?; \n + let stdin = io::stdin(); \n + let mut lock = stdin.lock(); \n + let mut output = String::new(); \n + lock.read_line(&mut output)?; \n + Ok(output) \n + } \n + \n pub fn password(&mut self, prompt: &str) -> io::Result<String> { \n let mut out = self.stdout.lock(); \n write!(&mut out, \"{}\", prompt)?;", "msg": "add function to require the prompt from user"}
{"diff": "a / cardano-cli/src/wallet/state/utxo.rs \n  b / cardano-cli/src/wallet/state/utxo.rs \n@@ -38,7 +38,7 @@ pub struct UTxO<A> { \n pub credited_address: A, \n /// the amount credited in this `UTxO` \n - credited_value: Coin, \n + pub credited_value: Coin, \n } \n impl<A> UTxO<A> { \n /// extract the `TxIn` from the `UTxO`. The output `TxIn` is meant to", "msg": "public accessfor for the utxo's structure"}
{"diff": "a / cardano-cli/src/blockchain/mod.rs \n  b / cardano-cli/src/blockchain/mod.rs \n@@ -115,6 +115,10 @@ impl Blockchain { \n let tag = format!(\"wallet/{}\", wallet_name); \n tag::remove_tag(&self.storage, &tag); \n } \n + pub fn get_wallet_tag(&self, wallet_name: &str) -> Option<block::HeaderHash> { \n + let tag = format!(\"wallet/{}\", wallet_name); \n + tag::read_hash(&self.storage, &tag) \n + } \n pub fn load_tip(&self) -> (BlockRef, bool) { \n let genesis_ref = (BlockRef {", "msg": "add function to retrieve the wallet tag"}
{"diff": "a / cardano-cli/src/wallet/state/lookup/mod.rs \n  b / cardano-cli/src/wallet/state/lookup/mod.rs \n@@ -4,9 +4,9 @@ pub mod randomindex; \n pub mod sequentialindex; \n pub trait AddressLookup { \n - type Error; \n - type AddressInput; \n - type AddressOutput; \n + type Error : ::std::fmt::Debug; \n + type AddressInput : ::std::fmt::Display; \n + type AddressOutput: ::std::fmt::Display; \n /// the implementor will attempt the recognize the given UTxO's credited_address. \n ///", "msg": "add some constraints to the AddressLookup trait's associated types"}
{"diff": "a / hermes/src/main.rs \n  b / hermes/src/main.rs \n@@ -61,7 +61,7 @@ fn main() { \n .required(false) \n .multiple(true) \n .default_value(\"mainnet\") \n - .possible_values(&[\"mainnet\", \"testnet\"]) \n + .possible_values(&[\"mainnet\", \"staging\", \"testnet\"]) \n ) \n .arg(Arg::with_name(\"no-sync\") \n .long(\"no-sync\") \n @@ -87,6 +87,7 @@ fn main() { \n for template in args.values_of(\"TEMPLATE\").unwrap() { \n let net_cfg = match template { \n \"mainnet\" => { net::Config::mainnet() }, \n + \"staging\" => { net::Config::staging() }, \n \"testnet\" => { net::Config::testnet() }, \n _ => { \n // we do not support custom template yet.", "msg": "add support for staging and testnet in hermes"}
{"diff": "a / cardano-cli/src/wallet/state/state.rs \n  b / cardano-cli/src/wallet/state/state.rs \nuse super::utxo::{UTxO, UTxOs}; \n use super::log::{Log}; \n use super::{lookup::{AddressLookup}, ptr::StatePtr}; \n -use cardano::{tx::TxIn}; \n +use cardano::{tx::TxIn, coin::{self, Coin}}; \n use std::{fmt}; \n #[derive(Debug)] \n @@ -18,6 +18,15 @@ impl<T: AddressLookup> State<T> { \n pub fn ptr<'a>(&'a self) -> &'a StatePtr { &self.ptr } \n + pub fn total(&self) -> coin::Result<Coin> { \n + self.utxos \n + .iter() \n + .map(|(_, v)| v.credited_value) \n + .fold(Ok(Coin::zero()), |acc, v| { \n + acc.and_then(|acc| acc + v) \n + }) \n + } \n + \n /// update the wallet state with the given logs \n /// This function is for initializing the State by recovering the logs. \n ///", "msg": "add function for the State to display the total"}
{"diff": "a / cardano/src/coin.rs \n  b / cardano/src/coin.rs \n@@ -93,10 +93,7 @@ impl cbor_event::se::Serialize for Coin { \n impl cbor_event::de::Deserialize for Coin { \n fn deserialize<'a>(raw: &mut RawCbor<'a>) -> cbor_event::Result<Self> { \n Coin::new(raw.unsigned_integer()?).map_err(|err| { \n - match err { \n - Error::OutOfBound(v) => cbor_event::Error::CustomError(format!(\"coin ({}) out of bound, max: {}\", v, MAX_COIN)), \n - Error::Negative => cbor_event::Error::CustomError(\"coin cannot hold negative value\".to_owned()), \n - } \n + cbor_event::Error::CustomError(format!(\"{}\", err)) \n }) \n } \n }", "msg": "simplify error report to the cbor_event deserializer"}
{"diff": "a / cardano/src/address.rs \n  b / cardano/src/address.rs \n@@ -378,6 +378,21 @@ impl ExtendedAddr { \n cbor_event::de::Deserialize::deserialize(&mut raw) \n } \n } \n +#[derive(Debug)] \n +pub enum ParseExtendedAddrError { \n + EncodingError(cbor_event::Error), \n + Base58Error(base58::Error) \n +} \n +impl ::std::str::FromStr for ExtendedAddr { \n + type Err = ParseExtendedAddrError; \n + fn from_str(s: &str) -> Result<Self, Self::Err> { \n + let bytes = base58::decode(s) \n + .map_err(ParseExtendedAddrError::Base58Error)?; \n + \n + Self::from_bytes(&bytes) \n + .map_err(ParseExtendedAddrError::EncodingError) \n + } \n +} \n impl cbor_event::se::Serialize for ExtendedAddr { \n fn serialize<W: ::std::io::Write>(&self, serializer: Serializer<W>) -> cbor_event::Result<Serializer<W>> { \n cbor::hs::util::encode_with_crc32_(&(&self.addr, &self.attributes, &self.addr_type), serializer)", "msg": "add instance of FromStr to provide standard string parsing"}
{"diff": "a / cardano-cli/src/transaction/core/operation.rs \n  b / cardano-cli/src/transaction/core/operation.rs \n@@ -105,3 +105,19 @@ pub struct Output { \n /// The desired amount to send to the associated address \n pub amount: Coin \n } \n +impl From<Output> for TxOut { \n + fn from(o: Output) -> Self { \n + TxOut { \n + address: o.address, \n + value: o.amount \n + } \n + } \n +} \n +impl<'a> From<&'a Output> for TxOut { \n + fn from(o: &'a Output) -> Self { \n + TxOut { \n + address: o.address.clone(), \n + value: o.amount \n + } \n + } \n +}", "msg": "add conversion function to retrieve the TxOut of an Output"}
{"diff": "a / cardano/src/bip/bip44.rs \n  b / cardano/src/bip/bip44.rs \n//! ``` \n use hdpayload::{Path}; \n -use std::{fmt, result}; \n +use std::{fmt, result, error}; \n #[cfg(feature = \"generic-serialization\")] \n use serde; \n @@ -74,6 +74,7 @@ impl fmt::Display for Error { \n } \n } \n } \n +impl error::Error for Error {} \n pub type Result<T> = result::Result<T, Error>;", "msg": "implement Error trait for bip44 related Errors"}
{"diff": "a / cardano/src/txbuild.rs \n  b / cardano/src/txbuild.rs \n@@ -343,6 +343,19 @@ mod tests { \n } \n } \n + fn fee_is_acceptable(coindiff: CoinDiff) { \n + match coindiff { \n + CoinDiff::Zero => {}, \n + CoinDiff::Positive(c) => { \n + let max_fee_overhead = 5_000u32.into(); \n + assert!(c < max_fee_overhead, \"fee is much greater than expected {}, expected less than {}\", c, max_fee_overhead); \n + }, \n + CoinDiff::Negative(c) => { \n + assert!(false, \"fee is negative {}, expecting zero or positive\", c) \n + } \n + } \n + } \n + \n fn fake_id() -> TxId { Blake2b256::new(&[1,2]) } \n fn fake_txopointer_val(coin: Coin) -> (TxoPointer, Coin) { \n (TxoPointer::new(fake_id(), 1), coin) \n @@ -387,6 +400,7 @@ mod tests { \n }, \n Err(Error::TxOutputPolicyNotEnoughCoins(c)) => { \n // here we don't check that the fee is minimal, since we need to burn extra coins \n + fee_is_acceptable(builder.balance(&alg).unwrap()) \n }, \n Err(e) => panic!(\"{}\", e), \n }", "msg": "verify fee is acceptable if the add_output_policy failed to add policy"}
{"diff": "a / protocol-tokio/src/protocol/outbound_sink.rs \n  b / protocol-tokio/src/protocol/outbound_sink.rs \n@@ -5,7 +5,7 @@ use std::{ \n }; \n use tokio_io::AsyncWrite; \n -use super::{nt, ConnectionState, LightWeightConnectionState, Message, NodeId}; \n +use super::{nt, ConnectionState, LightWeightConnectionState, Message, NodeId, KeepAlive}; \n pub type Outbound = Message; \n @@ -70,6 +70,18 @@ impl<T: AsyncWrite> OutboundSink<T> { \n }) \n } \n + /// initialize a subscription from the given outbound halve. \n + pub fn subscribe( \n + self, \n + keep_alive: KeepAlive, \n + ) -> impl Future<Item = (nt::LightWeightConnectionId, Self), Error = OutboundError> { \n + self.new_light_connection() \n + .and_then(move |(lwcid, connection)| { \n + connection.send(Message::Subscribe(lwcid, keep_alive)) \n + .map(move |connection| (lwcid, connection)) \n + }) \n + } \n + \n /// close a light connection that has been created with \n /// `new_light_connection`. \n ///", "msg": "add handy function to enable subscription from connected clients"}
{"diff": "a / cardano/src/block/date.rs \n  b / cardano/src/block/date.rs \n@@ -86,6 +86,16 @@ impl fmt::Display for BlockDate { \n } \n } \n } \n +impl From<EpochSlotId> for BlockDate { \n + fn from(esi: EpochSlotId) -> Self { \n + BlockDate::Normal(esi) \n + } \n +} \n +impl From<EpochId> for BlockDate { \n + fn from(ei: EpochId) -> Self { \n + BlockDate::Boundary(ei) \n + } \n +} \n impl str::FromStr for BlockDate { \n type Err = BlockDateParseError;", "msg": "add impl of From<EpochSlotId> and From<EpochId> for BlockDate\nthis will make type conversion more standard in the library"}
{"diff": "a / cardano/src/wallet/rindex.rs \n  b / cardano/src/wallet/rindex.rs \n@@ -22,7 +22,7 @@ use super::scheme; \n pub struct Addressing(u32, u32); \n impl Addressing { \n pub fn new(account: u32, index: u32) -> Self { \n - Addressing(account | 0x80000000, index | 0x80000000) \n + Addressing(account, index) \n } \n } \n impl ::std::fmt::Display for Addressing {", "msg": "Removed forcing derivation levels to be hardened on `Addressing` instantiation"}
{"diff": "a / chain-addr/src/lib.rs \n  b / chain-addr/src/lib.rs \n@@ -75,7 +75,7 @@ impl KindType { \n /// An unstructured address including the \n /// discrimination and the kind of address \n #[derive(Debug, Clone, PartialEq, Eq, Hash)] \n -pub struct Address(Discrimination, Kind); \n +pub struct Address(pub Discrimination, pub Kind); \n #[derive(Debug)] \n pub enum Error { \n @@ -162,6 +162,13 @@ impl Address { \n } \n unsafe { String::from_utf8_unchecked(out) } \n } \n + \n + pub fn public_key<'a>(&'a self) -> &'a PublicKey { \n + match self.1 { \n + Kind::Single(ref pk) => pk, \n + Kind::Group(ref pk, _) => pk, \n + } \n + } \n } \n fn get_kind_value(first_byte: u8) -> u8 {", "msg": "expose Address constructor and allow to access the inner public key"}
{"diff": "a / chain-impl-mockchain/src/leadership/bft.rs \n  b / chain-impl-mockchain/src/leadership/bft.rs \n@@ -56,14 +56,14 @@ impl<LeaderId: Eq + Clone> BftLeaderSelection<LeaderId> { \n /// get the party leader id elected for a given slot \n #[inline] \n - pub fn get_leader_at(&self, slotid: u64) -> &LeaderId { \n + fn get_leader_at(&self, slotid: u64) -> &LeaderId { \n let BftRoundRobinIndex(ofs) = self.offset(slotid); \n &self.leaders[ofs] \n } \n /// check if this party is elected for a given slot \n #[inline] \n - pub fn am_leader_at(&self, slotid: u64) -> IsLeading { \n + fn is_leader_at(&self, slotid: u64) -> IsLeading { \n match self.my { \n None => IsLeading::No, \n Some(my_index) => { \n @@ -123,7 +123,7 @@ impl LeaderSelection for BftLeaderSelection<PublicKey> { \n &self, \n date: <Self::Block as property::Block>::Date, \n ) -> Result<bool, Self::Error> { \n - Ok(self.am_leader_at(date.block_number()) == IsLeading::Yes) \n + Ok(self.is_leader_at(date.block_number()) == IsLeading::Yes) \n } \n }", "msg": "change name of the am_leader_at to is_leader_at\nalso make the function private"}
{"diff": "a / exe-common/examples/generate_genesis_data.rs \n  b / exe-common/examples/generate_genesis_data.rs \n@@ -97,7 +97,7 @@ fn main() { \n start_time: SystemTime::UNIX_EPOCH + Duration::from_secs(1548089245), \n slot_duration: Duration::from_millis(20000), \n protocol_magic, \n - fee_policy: fee::LinearFee::new(fee::Milli::new(43, 946), fee::Milli::integral(155381)), \n + fee_policy: fee::LinearFee::new(fee::Milli::integral(155381), fee::Milli::new(43, 946)), \n avvm_distr: BTreeMap::new(), \n non_avvm_balances, \n boot_stakeholders,", "msg": "Flip fee constants in example to match mainnet"}
{"diff": "a / chain-impl-mockchain/src/setting.rs \n  b / chain-impl-mockchain/src/setting.rs \n@@ -257,6 +257,7 @@ mod tests { \n impl Arbitrary for SettingsDiff { \n fn arbitrary<G: Gen>(g: &mut G) -> SettingsDiff { \n SettingsDiff { \n + block_version: ValueDiff::None, \n block_id: ValueDiff::Replace(Arbitrary::arbitrary(g), Arbitrary::arbitrary(g)), \n bootstrap_key_slots_percentage: ValueDiff::Replace( \n Arbitrary::arbitrary(g),", "msg": "add arbitrary setting for the diff of the block version"}
{"diff": "a / chain-impl-mockchain/src/value.rs \n  b / chain-impl-mockchain/src/value.rs \nuse chain_core::property; \n use std::ops; \n +use std::slice::Iter; \n /// Unspent transaction value. \n #[cfg_attr(feature = \"generic-serialization\", derive(serde_derive::Serialize))] \n @@ -10,6 +11,13 @@ impl Value { \n pub fn zero() -> Self { \n Value(0) \n } \n + \n + pub fn sum<I>(values: I) -> Result<Self, ValueError> \n + where \n + I: Iterator<Item = Self>, \n + { \n + values.fold(Ok(Value::zero()), |acc, v| acc? + v) \n + } \n } \n #[derive(Debug, Copy, Clone, PartialEq, Eq)]", "msg": "add ability to sum Value with an Iterator"}
{"diff": "a / chain-impl-mockchain/src/txbuilder.rs \n  b / chain-impl-mockchain/src/txbuilder.rs \n@@ -101,6 +101,12 @@ impl TransactionBuilder<Address> { \n balance(&self.tx, Value(0)) \n } \n + /// Create transaction finalizer without performing any \n + /// checks or output balancing. \n + pub fn unchecked_finalize(mut self) -> TransactionFinalizer { \n + TransactionFinalizer::new(self.tx, self.cert) \n + } \n + \n /// We finalize the transaction by passing fee rule and return \n /// policy. Then after all calculations were made we can get \n /// the information back to us.", "msg": "Add unchecked_finalize function for the transaction builder."}
{"diff": "a / chain-impl-mockchain/src/ledger.rs \n  b / chain-impl-mockchain/src/ledger.rs \n@@ -64,7 +64,7 @@ impl Ledger { \n } \n pub fn apply_transaction( \n - &mut self, \n + &self, \n signed_tx: &SignedTransaction<Address>, \n allow_account_creation: bool, \n linear_fees: &LinearFee,", "msg": "don't allow a mutable reference for apply transaction"}
{"diff": "a / chain-impl-mockchain/src/ledger.rs \n  b / chain-impl-mockchain/src/ledger.rs \n@@ -223,6 +223,10 @@ impl Ledger { \n allow_account_creation: self.settings.allow_account_creation, \n } \n } \n + \n + pub fn utxos<'a>(&'a self) -> utxo::Iter<'a, Address> { \n + self.utxos.iter() \n + } \n } \n fn apply_old_declaration(", "msg": "explose the utxos() function to retrieve the utxos of the UTxO ledger"}
{"diff": "a / chain-impl-mockchain/src/block/version.rs \n  b / chain-impl-mockchain/src/block/version.rs \n@@ -8,6 +8,10 @@ impl BlockVersion { \n pub const fn new(v: u16) -> Self { \n BlockVersion(v) \n } \n + \n + pub const fn as_u16(&self) -> u16 { \n + self.0 \n + } \n } \n #[derive(Debug, Clone, Copy, FromPrimitive, PartialEq, Eq)]", "msg": "add function to retrieve the internal u16 of a block version"}
{"diff": "a / chain-impl-mockchain/src/leadership/bft.rs \n  b / chain-impl-mockchain/src/leadership/bft.rs \n@@ -75,6 +75,12 @@ impl BftLeaderSelection { \n } \n } \n +impl LeaderId { \n + pub fn as_public_key(&self) -> &PublicKey<SIGNING_ALGORITHM> { \n + &self.0 \n + } \n +} \n + \n impl property::Serialize for LeaderId { \n type Error = std::io::Error; \n fn serialize<W: std::io::Write>(&self, writer: W) -> Result<(), Self::Error> {", "msg": "add a function to retrieve the internal PublicKey of the LeaderId"}
{"diff": "a / chain-impl-mockchain/src/config.rs \n  b / chain-impl-mockchain/src/config.rs \n@@ -119,6 +119,18 @@ pub fn entity_from_string<T: ConfigParam>(tag: &str, value: &str) -> Result<T, E \n T::from_string(value) \n } \n +impl std::fmt::Display for Error { \n + fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result { \n + match self { \n + Error::InvalidTag => write!(f, \"Invalid tag\"), \n + Error::SizeInvalid => write!(f, \"Invalid payload size\"), \n + Error::StructureInvalid => write!(f, \"Invalid payload structure\"), \n + Error::UnknownString(s) => write!(f, \"Invalid payload string: {}\", s), \n + } \n + } \n +} \n +impl std::error::Error for Error {} \n + \n #[cfg(test)] \n mod test { \n use super::*;", "msg": "impl Error trait for the block config Error"}
{"diff": "a / chain-impl-mockchain/src/stake/distribution.rs \n  b / chain-impl-mockchain/src/stake/distribution.rs \n@@ -39,6 +39,10 @@ impl StakeDistribution { \n self.0.get(poolid).map(|psd| psd.total_stake) \n } \n + pub fn get_distribution(&self, stake_pool_id: &StakePoolId) -> Option<&PoolStakeDistribution> { \n + self.0.get(stake_pool_id) \n + } \n + \n /// Place the stake pools on the interval [0, total_stake) (sorted \n /// by ID), then return the ID of the one containing 'point' \n /// (which must be in the interval). This is used to randomly", "msg": "get distribution for the given pool id"}
{"diff": "a / chain-impl-mockchain/src/stake/delegation.rs \n  b / chain-impl-mockchain/src/stake/delegation.rs \n@@ -209,7 +209,8 @@ impl DelegationState { \n output = CertificateApplyOutput::CreateAccount(reg.stake_key_id.clone()); \n } \n CertificateContent::StakeKeyDeregistration(ref reg) => { \n - new_state = new_state.deregister_stake_key(&reg.stake_key_id)? \n + new_state = new_state.deregister_stake_key(&reg.stake_key_id)?; \n + // don't delete account \n } \n CertificateContent::StakePoolRegistration(ref reg) => { \n new_state = new_state.register_stake_pool(reg.clone())?", "msg": "leave a comment not to try to remove an account"}
{"diff": "a / chain-impl-mockchain/src/accounting/account.rs \n  b / chain-impl-mockchain/src/accounting/account.rs \n@@ -104,6 +104,11 @@ impl<Extra: Clone> AccountState<Extra> { \n } \n } \n + pub fn value(&self) -> Value { \n + self.value \n + } \n + \n + // deprecated use value() \n pub fn get_value(&self) -> Value { \n self.value \n }", "msg": "add \"rust compliant\" accessor for account state value"}
{"diff": "a / cardano-legacy-address/src/address.rs \n  b / cardano-legacy-address/src/address.rs \n@@ -181,6 +181,21 @@ impl Addr { \n let mut raw = Deserializer::from(std::io::Cursor::new(&self.0)); \n cbor_event::de::Deserialize::deserialize(&mut raw).unwrap() // unwrap should never fail from addr to extended addr \n } \n + \n + /// Check if the Addr can be reconstructed with a specific xpub \n + pub fn identical_with_pubkey(&self, xpub: &XPub) -> bool { \n + let ea = self.deconstruct(); \n + let newea = ExtendedAddr::new(xpub, ea.attributes); \n + self == &newea.to_address() \n + } \n + \n + /// mostly helper of the previous function, so not to have to expose the xpub construction \n + pub fn identical_with_pubkey_raw(&self, xpub: &[u8]) -> bool { \n + match XPub::from_slice(xpub) { \n + Ok(xpub) => self.identical_with_pubkey(&xpub), \n + _ => false, \n + } \n + } \n } \n impl AsRef<[u8]> for Addr {", "msg": "introduce a simple way to check for equivalence after rebuild"}
{"diff": "a / src/monitor.rs \n  b / src/monitor.rs \n@@ -33,7 +33,7 @@ pub fn monitor_signal(handle: &Handle, plugins: Vec<Plugin>) { \n .and_then(|sigterm| { \n sigterm.take(1) \n .for_each(|_| -> Result<(), io::Error> { \n - info!(\"Received SIGTERM, aborting process\"); \n + info!(\"Received SIGTERM, exiting.\"); \n Ok(()) \n }) \n .map(|_| libc::SIGTERM) \n @@ -47,7 +47,7 @@ pub fn monitor_signal(handle: &Handle, plugins: Vec<Plugin>) { \n .and_then(|sigint| { \n sigint.take(1) \n .for_each(|_| -> Result<(), io::Error> { \n - error!(\"Received SIGINT, aborting process\"); \n + info!(\"Received SIGINT, exiting.\"); \n Ok(()) \n }) \n .map(|_| libc::SIGINT) \n @@ -69,8 +69,8 @@ pub fn monitor_signal(handle: &Handle, plugins: Vec<Plugin>) { \n drop(plugins); \n match r { \n - Ok(exit_code) => { \n - process::exit(128 + exit_code); \n + Ok(_signo) => { \n + process::exit(0); \n } \n Err(..) => Err(()), \n }", "msg": "exit with 0 on SIGTERM / SIGINT\nso that the manager / supervisor knows that we're exiting successfully."}
{"diff": "a / src/lib.rs \n  b / src/lib.rs \n#![crate_name = \"shadowsocks\"] \n #![recursion_limit = \"128\"] \n +use std::io; \n + \n +use tokio::runtime::Handle; \n + \n /// ShadowSocks version \n pub const VERSION: &str = env!(\"CARGO_PKG_VERSION\"); \n @@ -91,3 +95,13 @@ mod context; \n pub mod crypto; \n pub mod plugin; \n pub mod relay; \n + \n +/// Start a ShadowSocks' server \n +/// \n +/// For `config.config_type` in `Socks5Local`, `HttpLocal` and `TunnelLocal`, server will run in Local mode. \n +pub async fn run(config: Config, rt: Handle) -> io::Result<()> { \n + match config.config_type { \n + ConfigType::Socks5Local | ConfigType::HttpLocal | ConfigType::TunnelLocal => run_local(config, rt).await, \n + ConfigType::Server => run_server(config, rt).await, \n + } \n +}", "msg": "Add a unified entrance for starting server"}
{"diff": "a / src/config.rs \n  b / src/config.rs \n@@ -807,6 +807,7 @@ impl RedirType { \n for e in Self::iter() { \n match e { \n RedirType::NotSupported => continue, \n + #[allow(unreachable_patterns)] \n _ => v.push(e.name()), \n } \n }", "msg": "Suppress warning for platforms that doesnt have redir support"}
{"diff": "a / src/relay/tcprelay/proxy_stream.rs \n  b / src/relay/tcprelay/proxy_stream.rs \n@@ -508,9 +508,11 @@ async fn connect_proxy_server(context: &Context, svr_cfg: &ServerConfig) -> io:: \n } \n Err(err) => { \n // Connection failure, retry \n - debug!( \n + trace!( \n \"failed to connect {}, retried {} times (last err: {})\", \n - svr_addr, retry_time, err \n + svr_addr, \n + retry_time, \n + err \n ); \n last_err = Some(err); \n @@ -523,7 +525,7 @@ async fn connect_proxy_server(context: &Context, svr_cfg: &ServerConfig) -> io:: \n } \n let last_err = last_err.unwrap(); \n - error!( \n + debug!( \n \"failed to connect {}, retried {} times, last_err: {}\", \n svr_addr, RETRY_TIMES, last_err \n );", "msg": "Lower proxy connection errors to debug level"}
{"diff": "a / src/relay/dnsrelay/mod.rs \n  b / src/relay/dnsrelay/mod.rs \n@@ -367,6 +367,7 @@ where \n Ok(x) => x, \n Err(e) => { \n error!(\"DNS relay read from UDP socket error: {}\", e); \n + time::sleep(Duration::from_secs(1)).await; \n continue; \n } \n };", "msg": "DNS UDP listener sleep 1s if error occurs"}
{"diff": "a / crates/shadowsocks-service/src/local/redir/udprelay/mod.rs \n  b / crates/shadowsocks-service/src/local/redir/udprelay/mod.rs \n@@ -489,7 +489,16 @@ impl UdpAssociationContext { \n // Create a socket binds to destination addr \n // This only works for systems that supports binding to non-local addresses \n - let inbound = UdpRedirSocket::bind(self.redir_ty, addr)?; \n + let inbound = match UdpRedirSocket::bind(self.redir_ty, addr) { \n + Ok(s) => s, \n + Err(err) => { \n + error!( \n + \"failed to bind to dest {} for sending back to {}, error: {}\", \n + addr, self.peer_addr, err \n + ); \n + continue; \n + } \n + }; \n // Send back to client \n if let Err(err) = inbound.send_to(data, self.peer_addr).await { \n @@ -497,6 +506,7 @@ impl UdpAssociationContext { \n \"udp failed to send back to client {}, from target {}, error: {}\", \n self.peer_addr, addr, err \n ); \n + continue; \n } \n trace!(\"udp relay {} <- {} with {} bytes\", self.peer_addr, addr, data.len());", "msg": "ignore bind transparent ip error and continue receiving"}
{"diff": "a / crates/shadowsocks-service/src/local/dns/server.rs \n  b / crates/shadowsocks-service/src/local/dns/server.rs \n@@ -77,10 +77,10 @@ impl Dns { \n tokio::pin!(tcp_fut, udp_fut); \n - let _ = future::select(tcp_fut, udp_fut).await; \n - \n - let err = io::Error::new(ErrorKind::Other, \"dns server exited unexpectly\"); \n - Err(err) \n + match future::select(tcp_fut, udp_fut).await { \n + Either::Left((res, ..)) => res, \n + Either::Right((res, ..)) => res, \n + } \n } \n async fn run_tcp_server(&self, bind_addr: &ClientConfig, client: Arc<DnsClient>) -> io::Result<()> {", "msg": "pass-through tcp & udp dns server's result to upper caller"}
{"diff": "a / crates/shadowsocks/src/config.rs \n  b / crates/shadowsocks/src/config.rs \n@@ -327,7 +327,7 @@ impl ServerConfig { \n let mut sp2 = account.splitn(2, ':'); \n let (method, pwd) = match (sp2.next(), sp2.next()) { \n (Some(m), Some(p)) => (m, p), \n - _ => panic!(\"Malformed input\"), \n + _ => return Err(UrlParseError::InvalidUserInfo) \n }; \n let addr = match addr.parse::<ServerAddr>() {", "msg": "don't panic when sip002 account is invalid"}
{"diff": "a / aws/sdk/examples/sts/src/bin/credentials-provider.rs \n  b / aws/sdk/examples/sts/src/bin/credentials-provider.rs \n@@ -8,6 +8,8 @@ use std::sync::{Arc, Mutex}; \n use std::time::{Duration, SystemTime}; \n use sts::Credentials; \n +/// Implements a basic version of ProvideCredentials with AWS STS \n +/// and lists the tables in the region based on those credentials. \n #[tokio::main] \n async fn main() -> Result<(), dynamodb::Error> { \n tracing_subscriber::fmt::init(); \n @@ -26,7 +28,7 @@ async fn main() -> Result<(), dynamodb::Error> { \n Ok(()) \n } \n -/// This is a rough example of how you could implement ProvideCredentials with Sts \n +/// This is a rough example of how you could implement ProvideCredentials with Amazon STS. \n /// \n /// Do not use this in production! A high quality implementation is in the roadmap. \n #[derive(Clone)]", "msg": "Added doc comment to STS credentials-provider code example"}
{"diff": "a / tools/ci-cdk/canary-lambda/src/main.rs \n  b / tools/ci-cdk/canary-lambda/src/main.rs \n@@ -104,7 +104,8 @@ async fn lambda_main(clients: canary::Clients) -> Result<Value, Error> { \n let canaries = get_canaries_to_run(clients, env); \n let join_handles = canaries \n .into_iter() \n - .map(|(name, future)| (name, tokio::spawn(future))); \n + .map(|(name, future)| (name, tokio::spawn(future))) \n + .collect::<Vec<_>>(); \n // Wait for and aggregate results \n let mut failures = BTreeMap::new();", "msg": "Collect canary tasks so that they are run in parallel"}
{"diff": "a / rust-runtime/aws-smithy-http-server/examples/pokemon_service/tests/benchmark.rs \n  b / rust-runtime/aws-smithy-http-server/examples/pokemon_service/tests/benchmark.rs \n@@ -37,9 +37,9 @@ async fn benchmark() -> Result<(), Box<dyn std::error::Error>> { \n // Run a single benchmark with 8 threads and 64 connections for 60 seconds. \n let benches = vec![BenchmarkBuilder::default() \n - .duration(Duration::from_secs(60)) \n - .threads(8) \n - .connections(64) \n + .duration(Duration::from_secs(90)) \n + .threads(2) \n + .connections(32) \n .build()?]; \n wrk.bench(&benches)?;", "msg": "Try to play a little with benchmark params to increase reliability by decreasing contention"}
{"diff": "a / rust-runtime/aws-smithy-http-server/src/runtime_error.rs \n  b / rust-runtime/aws-smithy-http-server/src/runtime_error.rs \n@@ -32,6 +32,7 @@ pub enum RuntimeErrorKind { \n /// As of writing, this variant can only occur upon failure to extract an \n /// [`crate::extension::Extension`] from the request. \n InternalFailure(crate::Error), \n + // TODO(https://github.com/awslabs/smithy-rs/issues/1663) \n // UnsupportedMediaType, \n NotAcceptable, \n }", "msg": "Reference issue for unsupported media type"}
{"diff": "a / src/bin/aggregator.rs \n  b / src/bin/aggregator.rs \n@@ -23,11 +23,16 @@ async fn main() { \n Arg::with_name(\"config\") \n .short(\"c\") \n .takes_value(true) \n + .required(true) \n .help(\"path to the config file\"), \n ) \n .get_matches(); \n let config_file = matches.value_of(\"config\").unwrap(); \n - let settings = Settings::new(config_file).unwrap(); \n + \n + let settings = Settings::new(config_file).unwrap_or_else(|err| { \n + eprintln!(\"Problem parsing configuration file: {}\", err); \n + process::exit(1); \n + }); \n env::set_var(\"RUST_LOG\", &settings.log_level); \n env_logger::init();", "msg": "Improve settings error handling"}
{"diff": "a / src/coordinator/core/protocol.rs \n  b / src/coordinator/core/protocol.rs \n@@ -315,7 +315,8 @@ impl Protocol { \n pub fn end_aggregation(&mut self, success: bool) { \n if !self.waiting_for_aggregation { \n - panic!(\"not waiting for aggregation\"); \n + error!(\"not waiting for aggregation\"); \n + return; \n } \n self.waiting_for_aggregation = false; \n if success {", "msg": "don't panic upon un-expected end aggregation message from the aggregator\nthis can actually happen, if the coordinator's acknowledgement of the\nfirst message is lost for some reason, for instance"}
{"diff": "a / rust/src/aggregator/service.rs \n  b / rust/src/aggregator/service.rs \n@@ -124,7 +124,7 @@ where \n { \n let _ = response_tx.send(self.global_weights.clone()); \n } else { \n - debug!(\"rejecting download request\"); \n + warn!(\"rejecting download request\"); \n } \n } \n @@ -138,7 +138,7 @@ where \n .unwrap_or(false); \n if !accept_upload { \n - debug!(\"rejecting upload request\"); \n + warn!(\"rejecting upload request\"); \n return; \n }", "msg": "aggregator: log a warning when rejecting a download or upload"}
{"diff": "a / rust/xaynet-client/src/mobile_client/mod.rs \n  b / rust/xaynet-client/src/mobile_client/mod.rs \n@@ -147,6 +147,16 @@ impl MobileClient { \n }) \n } \n + /// Returns the current state of the client. \n + pub fn get_current_state(&self) -> ClientStateName { \n + match self.client_state { \n + ClientStateMachine::Awaiting(_) => ClientStateName::Awaiting, \n + ClientStateMachine::Sum(_) => ClientStateName::Sum, \n + ClientStateMachine::Update(_) => ClientStateName::Update, \n + ClientStateMachine::Sum2(_) => ClientStateName::Sum2, \n + } \n + } \n + \n /// Sets the local model. \n /// \n /// The local model is only sent if the client has been selected as an update client. \n @@ -177,6 +187,13 @@ impl MobileClient { \n } \n } \n +pub enum ClientStateName { \n + Awaiting, \n + Sum, \n + Update, \n + Sum2, \n +} \n + \n struct LocalModelCache(Option<Model>); \n impl LocalModelCache {", "msg": "export current state of the client\nprovide method that returns the current state of the client"}
{"diff": "a / src/indexer/index_writer.rs \n  b / src/indexer/index_writer.rs \n@@ -702,7 +702,7 @@ mod tests { \n index.load_searchers().unwrap(); \n assert_eq!(num_docs_containing(\"a\"), 200); \n - assert_eq!(index.searchable_segments().unwrap().len(), 1); \n + assert!(index.searchable_segments().unwrap().len() < 8); \n } \n }", "msg": "issue/96 Fixed unit test condition to something reasonable"}
{"diff": "a / src/schema/term.rs \n  b / src/schema/term.rs \n@@ -18,7 +18,7 @@ pub struct Term(Vec<u8>); \n impl Term { \n /// Set the content of the term. \n - pub fn set_content(&mut self, content: &[u8]) { \n + pub(crate) fn set_content(&mut self, content: &[u8]) { \n assert!(content.len() >= 4); \n self.0.resize(content.len(), 0u8); \n (&mut self.0[..]).clone_from_slice(content); \n @@ -114,7 +114,7 @@ impl Term { \n /// \n /// If you want to build a field for a given `str`, \n /// you want to use `from_field_text`. \n - pub fn from_bytes(data: &[u8]) -> Term { \n + pub(crate) fn from_bytes(data: &[u8]) -> Term { \n Term(Vec::from(data)) \n }", "msg": "NOBUG Hiding methods making it possible to build a incorrect Term."}
{"diff": "a / src/indexer/merger.rs \n  b / src/indexer/merger.rs \n@@ -240,7 +240,7 @@ impl IndexMerger { \n .field_type() \n .get_segment_postings_option() \n .expect(\"Encounterred a field that is not supposed to be \n - indexed. Have you modified the index?\"); \n + indexed. Have you modified the schema?\"); \n last_field = Some(current_field); \n // it is perfectly safe to call `.new_field`", "msg": "issue/17 Slightly more explicit error message"}
{"diff": "a / src/snippet/mod.rs \n  b / src/snippet/mod.rs \n@@ -55,10 +55,8 @@ impl FragmentCandidate { \n if let Some(score) = terms.get(&token.text.to_lowercase()) { \n self.score += score; \n - self.highlighted.push(HighlightSection { \n - start: token.offset_from, \n - stop: token.offset_to, \n - }); \n + self.highlighted \n + .push(HighlightSection::new(token.offset_from, token.offset_to)); \n } \n } \n } \n @@ -163,9 +161,11 @@ fn select_best_fragment_combination<'a>( \n let highlighted = fragment \n .highlighted \n .iter() \n - .map(|item| HighlightSection { \n - start: item.start - fragment.start_offset, \n - stop: item.stop - fragment.start_offset, \n + .map(|item| { \n + HighlightSection::new( \n + item.start - fragment.start_offset, \n + item.stop - fragment.start_offset, \n + ) \n }).collect(); \n Snippet { \n fragments: fragment_text.to_owned(),", "msg": "Use HighlightSection::new rather than just directly creating the object"}
{"diff": "a / src/query/term_query/term_weight.rs \n  b / src/query/term_query/term_weight.rs \n@@ -4,7 +4,7 @@ use crate::docset::DocSet; \n use crate::postings::SegmentPostings; \n use crate::query::bm25::BM25Weight; \n use crate::query::explanation::does_not_match; \n -use crate::query::weight::{for_each_pruning_scorer, for_each_scorer}; \n +use crate::query::weight::for_each_scorer; \n use crate::query::Weight; \n use crate::query::{Explanation, Scorer}; \n use crate::schema::IndexRecordOption; \n @@ -73,8 +73,8 @@ impl Weight for TermWeight { \n reader: &SegmentReader, \n callback: &mut dyn FnMut(DocId, Score) -> Score, \n ) -> crate::Result<()> { \n - let mut scorer = self.scorer(reader, 1.0)?; \n - for_each_pruning_scorer(&mut scorer, threshold, callback); \n + let scorer = self.specialized_scorer(reader, 1.0)?; \n + crate::query::boolean_query::block_wand(vec![scorer], threshold, callback); \n Ok(()) \n } \n }", "msg": "Using block wand for term queries too."}
{"diff": "a / src/schema/document.rs \n  b / src/schema/document.rs \n@@ -74,9 +74,8 @@ impl Document { \n } \n /// Add a text field. \n - pub fn add_text(&mut self, field: Field, text: &str) { \n - let value = Value::Str(String::from(text)); \n - self.add(FieldValue::new(field, value)); \n + pub fn add_text<S: ToString>(&mut self, field: Field, text: S) { \n + self.add(FieldValue::new(field, Value::Str(text.to_string()))); \n } \n /// Add a pre-tokenized text field. \n @@ -110,8 +109,8 @@ impl Document { \n } \n /// Add a bytes field \n - pub fn add_bytes(&mut self, field: Field, value: Vec<u8>) { \n - self.add(FieldValue::new(field, Value::Bytes(value))) \n + pub fn add_bytes<T: Into<Vec<u8>>>(&mut self, field: Field, value: T) { \n + self.add(FieldValue::new(field, Value::Bytes(value.into()))) \n } \n /// Add a field value", "msg": "Make field types less strict when populating documents."}
{"diff": "a / src/query/boolean_query/block_wand.rs \n  b / src/query/boolean_query/block_wand.rs \n@@ -268,7 +268,7 @@ mod tests { \n } \n fn nearly_equals(left: Score, right: Score) -> bool { \n - (left - right).abs() < 0.00001 * (left + right).abs() \n + (left - right).abs() < 0.0001 * (left + right).abs() \n } \n fn compute_checkpoints_for_each_pruning(", "msg": "Making block wand test more robusts"}
{"diff": "a / src/docset.rs \n  b / src/docset.rs \n@@ -129,6 +129,14 @@ impl<'a> DocSet for &'a mut dyn DocSet { \n fn size_hint(&self) -> u32 { \n (**self).size_hint() \n } \n + \n + fn count(&mut self, delete_bitset: &DeleteBitSet) -> u32 { \n + (**self).count(delete_bitset) \n + } \n + \n + fn count_including_deleted(&mut self) -> u32 { \n + (**self).count_including_deleted() \n + } \n } \n impl<TDocSet: DocSet + ?Sized> DocSet for Box<TDocSet> {", "msg": "Added specialized implementation for count/count_including... in &mut DocSet"}
{"diff": "a / src/query/query_parser/query_parser.rs \n  b / src/query/query_parser/query_parser.rs \n@@ -157,7 +157,8 @@ fn trim_ast(logical_ast: LogicalAST) -> Option<LogicalAST> { \n /// a word lexicographically between `a` and `c` (inclusive lower bound, exclusive upper bound). \n /// Inclusive bounds are `[]`, exclusive are `{}`. \n /// \n -/// * date values: The query parser supports rfc3339 formatted dates. For example \"2002-10-02T15:00:00.05Z\" \n +/// * date values: The query parser supports rfc3339 formatted dates. For example `\"2002-10-02T15:00:00.05Z\"` \n +/// or `some_date_field:[2002-10-02T15:00:00Z TO 2002-10-02T18:00:00Z}` \n /// \n /// * all docs query: A plain `*` will match all documents in the index. \n ///", "msg": "Add a date range query example to QueryParser documentation"}
{"diff": "a / src/lib.rs \n  b / src/lib.rs \n@@ -237,6 +237,7 @@ pub fn version_string() -> &'static str { \n pub mod merge_policy { \n pub use crate::indexer::DefaultMergePolicy; \n pub use crate::indexer::LogMergePolicy; \n + pub use crate::indexer::MergeCandidate; \n pub use crate::indexer::MergePolicy; \n pub use crate::indexer::NoMergePolicy; \n }", "msg": "Making MergeCandidate public in order to allow the usage of custom merge\npolicies.\nCloses"}
{"diff": "a / src/collector/multi_collector.rs \n  b / src/collector/multi_collector.rs \n@@ -87,6 +87,10 @@ pub struct FruitHandle<TFruit: Fruit> { \n } \n impl<TFruit: Fruit> FruitHandle<TFruit> { \n + // Extract a typed fruit off a multifruit. \n + // \n + // This function involves downcasting and can panic if the multifruit was \n + // created using faulty code. \n pub fn extract(self, fruits: &mut MultiFruit) -> TFruit { \n let boxed_fruit = fruits.sub_fruits[self.pos].take().expect(\"\"); \n *boxed_fruit", "msg": "Added rustdoc for MultiFruit extract function"}
{"diff": "a / crates/rune-cli/src/main.rs \n  b / crates/rune-cli/src/main.rs \n@@ -110,7 +110,7 @@ struct Args { \n /// macros[=<true/false>] - Enable or disable macros (experimental). \n /// \n /// bytecode[=<true/false>] - Enable or disable bytecode caching (experimental). \n - #[structopt(name = \"option\", short = \"O\")] \n + #[structopt(name = \"option\", short = \"O\", number_of_values = 1)] \n compiler_options: Vec<String>, \n }", "msg": "Don't parse more than one argument to -O"}
{"diff": "a / crates/rune/src/query.rs \n  b / crates/rune/src/query.rs \n@@ -227,6 +227,8 @@ pub(crate) struct Query { \n /// These items are associated with AST elements, and encodoes the item path \n /// that the AST element was indexed. \n pub(crate) items: HashMap<Id, Item>, \n + /// Reverse lookup for items to reduce the number of items used. \n + pub(crate) items_rev: HashMap<Item, Id>, \n /// Compiled constant functions. \n pub(crate) const_fns: HashMap<Id, Rc<ir::IrFn>>, \n } \n @@ -243,13 +245,19 @@ impl Query { \n indexed: HashMap::new(), \n templates: HashMap::new(), \n items: HashMap::new(), \n + items_rev: HashMap::new(), \n const_fns: HashMap::new(), \n } \n } \n /// Insert an item and return its Id. \n pub(crate) fn insert_item(&mut self, item: Item) -> Option<Id> { \n + if let Some(id) = self.items_rev.get(&item) { \n + return Some(*id); \n + } \n + \n let id = self.next_id.next()?; \n + self.items_rev.insert(item.clone(), id); \n self.items.insert(id, item); \n Some(id) \n }", "msg": "Use reverse lookup of items to reduce memory use"}
{"diff": "a / crates/rune/src/compiling/v1/assemble/expr_call.rs \n  b / crates/rune/src/compiling/v1/assemble/expr_call.rs \n@@ -31,6 +31,7 @@ impl Assemble for ast::ExprCall { \n log::trace!(\"ExprCall(ExprFieldAccess) => {:?}\", c.source.source(span)); \n expr.assemble(c, Needs::Value)?.apply(c)?; \n + c.scopes.decl_anon(span)?; \n for (expr, _) in &self.args { \n expr.assemble(c, Needs::Value)?.apply(c)?;", "msg": "Make sure anon var is declared for called field expr"}
{"diff": "a / starlark/src/values/layout/heap.rs \n  b / starlark/src/values/layout/heap.rs \n@@ -241,7 +241,7 @@ impl Heap { \n self.alloc_raw(ValueMem::Immutable(box x)) \n } \n - pub fn alloc_thaw_on_write<'v>(&'v self, x: FrozenValue) -> Value<'v> { \n + pub(crate) fn alloc_thaw_on_write<'v>(&'v self, x: FrozenValue) -> Value<'v> { \n if x.get_ref().naturally_mutable() { \n self.alloc_raw(ValueMem::ThawOnWrite(ThawableCell::new(x))) \n } else {", "msg": "Don't allow users to allocate thaw-on-write things\nSummary: It's an internal optimisation that isn't widely applicable, so keep it internal."}
{"diff": "a / starlark/src/values/mod.rs \n  b / starlark/src/values/mod.rs \n//! https://github.com/google/skylark/blob/a0e5de7e63b47e716cca7226662a4c95d47bf873/doc/spec.md#sequence-types). \n //! We also use the term _container_ for denoting any of those type that can \n //! hold several values. \n -pub use crate::values::{ \n - error::*, interpolation::*, iter::*, layout::*, owned::*, traits::*, types::*, unpack::*, \n -}; \n +pub use crate::values::{error::*, iter::*, layout::*, owned::*, traits::*, types::*, unpack::*}; \n use crate::{ \n collections::{Hashed, SmallHashResult}, \n values::types::function::FunctionInvoker,", "msg": "Don't expose the interpolation\nSummary: Not required at the moment, and I suspect it might not be for a while, or ever. There are 2 forms of interpolation, a 3rd planned, so no point exposing the least favoured one."}
{"diff": "a / starlark/src/stdlib/string.rs \n  b / starlark/src/stdlib/string.rs \n@@ -216,14 +216,18 @@ pub(crate) fn string_methods(builder: &mut GlobalsBuilder) { \n ref start @ NoneOr::None: NoneOr<i32>, \n ref end @ NoneOr::None: NoneOr<i32>, \n ) -> i32 { \n + if start.is_none() && end.is_none() { \n + if needle.len() == 1 { \n + let b = needle.as_bytes()[0]; \n + Ok(this.as_bytes().iter().filter(|x| **x == b).count() as i32) \n + } else { \n + Ok(this.matches(needle).count() as i32) \n + } \n + } else { \n let (start, end) = convert_indices(this.len() as i32, start, end); \n - let mut counter = 0i32; \n - let mut s = this.get(start..end).unwrap(); \n - while let Some(offset) = s.find(needle) { \n - counter += 1; \n - s = s.get(offset + needle.len()..).unwrap_or(\"\"); \n + // FIXME: This unwrap can be triggered by users, should bubble up an error \n + Ok(this.get(start..end).unwrap().matches(needle).count() as i32) \n } \n - Ok(counter) \n } \n /// [string.endswith](", "msg": "Optimise count\nSummary: Previously we did a lot of things that looked slightly surprising. Now use the matches function with some fast paths."}
{"diff": "a / starlark/src/eval/fragment/stmt.rs \n  b / starlark/src/eval/fragment/stmt.rs \n@@ -270,7 +270,7 @@ fn add_assign<'v>(lhs: Value<'v>, rhs: Value<'v>, heap: &'v Heap) -> anyhow::Res \n // mutably borrowed when we iterate over `rhs`, as they might alias. \n let lhs_aref = lhs.get_ref(); \n - let lhs_ty = lhs_aref.as_dyn_any().static_type_of(); \n + let lhs_ty = lhs_aref.static_type_of(); \n if List::is_list_type(lhs_ty) { \n // If the value is None, that must mean its a FrozenList, thus turn it into an immutable error", "msg": "No need to convert vtable before static_type_of\nSummary: Since static_type_of is a super-trait method, you can just invoke it directly."}
{"diff": "a / modules/src/ics23_commitment/mod.rs \n  b / modules/src/ics23_commitment/mod.rs \nuse serde_derive::{Deserialize, Serialize}; \n use crate::path::Path; \n -use tendermint::abci; \n #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \n pub struct CommitmentRoot; \n @@ -18,7 +17,7 @@ impl CommitmentPath { \n } \n } \n -pub type CommitmentProof = abci::Proof; \n +pub type CommitmentProof = tendermint::merkle::proof::Proof; \n /* \n impl CommitmentProof { \n pub fn from_bytes(_bytes: &[u8]) -> Self {", "msg": "switch to merke proof after tendermint change"}
{"diff": "a / relayer/src/worker/wallet.rs \n  b / relayer/src/worker/wallet.rs \nuse std::time::Duration; \n -use tracing::{debug, error_span}; \n +use tracing::{error_span, trace}; \n use crate::{ \n chain::handle::ChainHandle, \n @@ -28,7 +28,7 @@ pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n )) \n })?; \n - debug!(%amount, denom = %balance.denom, account = %key.account, \"wallet balance\"); \n + trace!(%amount, denom = %balance.denom, account = %key.account, \"wallet balance\"); \n telemetry!( \n wallet_balance,", "msg": "Change wallet balance debug to trace in wallet worker\nChange debug to trace for wallet balance"}
{"diff": "a / relayer/src/worker/wallet.rs \n  b / relayer/src/worker/wallet.rs \n@@ -4,12 +4,11 @@ use tracing::{error_span, trace}; \n use crate::{ \n chain::handle::ChainHandle, \n - util::task::{spawn_background_task, Next, TaskHandle}, \n + telemetry, \n + util::task::{spawn_background_task, Next, TaskError, TaskHandle}, \n }; \n pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n - use crate::{telemetry, util::task::TaskError}; \n - \n let span = error_span!(\"wallet\", chain = %chain.id()); \n spawn_background_task(span, Some(Duration::from_secs(5)), move || { \n @@ -21,7 +20,7 @@ pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n TaskError::Ignore(format!(\"failed to query balance for the account: {e}\")) \n })?; \n - let amount = balance.amount.parse().map_err(|_| { \n + let amount: u64 = balance.amount.parse().map_err(|_| { \n TaskError::Ignore(format!( \n \"failed to parse amount into u64: {}\", \n balance.amount", "msg": "Add type ascription to fix build when `telemetry` feature is disabled"}
{"diff": "a / src/api/raw.rs \n  b / src/api/raw.rs \n@@ -576,7 +576,9 @@ impl RawApi { \n let mut req = http::Request::put(urlstr); \n Ok(req.body(data).context(ErrorKind::RequestBuild)?) \n } \n +} \n +impl RawApi { \n /// Get a pod logs \n pub fn log(&self, name: &str, lp: &LogParams) -> Result<http::Request<Vec<u8>>> { \n let base_url = self.make_url() + \"/\" + name + \"/\" + \"log\"; \n @@ -618,7 +620,6 @@ impl RawApi { \n let mut req = http::Request::get(urlstr); \n Ok(req.body(vec![]).context(ErrorKind::RequestBuild)?) \n } \n - \n } \n #[test]", "msg": "move log function to a new impl RawApi block"}
{"diff": "a / starlark/src/values/layout/typed/mod.rs \n  b / starlark/src/values/layout/typed/mod.rs \n@@ -39,10 +39,10 @@ use crate::{ \n }; \n /// [`Value`] wrapper which asserts contained value is of type `<T>`. \n -#[derive(Copy_, Clone_, Dupe_)] \n +#[derive(Copy_, Clone_, Dupe_, AnyLifetime)] \n pub struct ValueTyped<'v, T: StarlarkValue<'v>>(Value<'v>, marker::PhantomData<&'v T>); \n /// [`FrozenValue`] wrapper which asserts contained value is of type `<T>`. \n -#[derive(Copy_, Clone_, Dupe_)] \n +#[derive(Copy_, Clone_, Dupe_, AnyLifetime)] \n pub struct FrozenValueTyped<'v, T: StarlarkValue<'v>>(FrozenValue, marker::PhantomData<&'v T>); \n unsafe impl<'v, T: StarlarkValue<'v>> Trace<'v> for FrozenValueTyped<'v, T> {", "msg": "derive(AnyLifetime) for ValueTyped\nSummary: For the following diffs."}
{"diff": "a / starlark/src/values/types/string/repr.rs \n  b / starlark/src/values/types/string/repr.rs \n@@ -228,12 +228,8 @@ pub(crate) fn string_repr(str: &str, buffer: &mut String) { \n #[cfg(test)] \n mod tests { \n - use std::mem; \n - use crate::{ \n - assert, \n - values::types::string::repr::{chunk_non_ascii_or_need_escape, string_repr}, \n - }; \n + use crate::{assert, values::types::string::repr::string_repr}; \n #[test] \n fn test_to_repr() { \n @@ -326,9 +322,11 @@ mod tests { \n #[cfg(target_feature = \"sse2\")] \n #[test] \n fn test_chunk_non_ascii_or_need_escape() { \n - use std::arch::x86_64::*; \n + use std::{arch::x86_64::*, mem}; \n - use crate::values::types::string::simd::Vector; \n + use crate::values::{ \n + string::repr::chunk_non_ascii_or_need_escape, types::string::simd::Vector, \n + }; \n unsafe fn load(s: &str) -> __m128i { \n assert_eq!(s.len(), mem::size_of::<__m128i>());", "msg": "Move conditionally needed imports to the conditional block\nSummary: these complain on local .test.py"}
{"diff": "a / starlark/src/values/typing.rs \n  b / starlark/src/values/typing.rs \n@@ -204,7 +204,7 @@ impl TypeCompiled { \n } \n } \n - pub(crate) fn new<'h>(ty: Value<'h>, heap: &'h Heap) -> anyhow::Result<Self> { \n + pub(crate) fn new<'v>(ty: Value<'v>, heap: &'v Heap) -> anyhow::Result<Self> { \n if let Some(s) = ty.unpack_str() { \n Ok(TypeCompiled::from_str(s)) \n } else if ty.is_none() { \n @@ -221,7 +221,7 @@ impl TypeCompiled { \n } \n } \n -fn invalid_type_annotation<'h>(ty: Value<'h>, heap: &'h Heap) -> TypingError { \n +fn invalid_type_annotation<'v>(ty: Value<'v>, heap: &'v Heap) -> TypingError { \n if let Some(name) = ty \n .get_attr(\"type\", heap) \n .ok()", "msg": "Use 'v consistently\nSummary: Not sure why we were using 'h for the lifetime here... But we were, so switch to 'v as that is standard."}
{"diff": "a / starlark/src/syntax/ast.rs \n  b / starlark/src/syntax/ast.rs \n@@ -23,7 +23,7 @@ use std::{ \n }; \n use derivative::Derivative; \n -use gazebo::prelude::*; \n +use gazebo::{prelude::*, variants::VariantName}; \n use static_assertions::assert_eq_size; \n use crate::{ \n @@ -206,7 +206,7 @@ pub(crate) enum ClauseP<P: AstPayload> { \n If(AstExprP<P>), \n } \n -#[derive(Debug, Clone, Copy, Dupe, Eq, PartialEq)] \n +#[derive(Debug, Clone, Copy, Dupe, Eq, PartialEq, VariantName)] \n pub(crate) enum BinOp { \n Or, \n And, \n @@ -231,7 +231,7 @@ pub(crate) enum BinOp { \n RightShift, \n } \n -#[derive(Debug, Clone, Copy, Dupe, PartialEq, Eq)] \n +#[derive(Debug, Clone, Copy, Dupe, PartialEq, Eq, VariantName)] \n pub(crate) enum AssignOp { \n Add, // += \n Subtract, // -=", "msg": "Add VariantName to operators\nSummary: Used in a subsequent diff, not harmful. Useful to be able to automatically generate names for such things."}
{"diff": "a / starlark/src/stdlib/string.rs \n  b / starlark/src/stdlib/string.rs \n@@ -788,16 +788,19 @@ pub(crate) fn string_methods(builder: &mut MethodsBuilder) { \n /// \"#, \"argument was negative\"); \n /// ``` \n #[starlark(speculative_exec_safe)] \n - fn replace( \n - this: &str, \n + fn replace<'v>( \n + this: ValueOf<'v, &'v str>, \n #[starlark(require = pos)] old: &str, \n #[starlark(require = pos)] new: &str, \n #[starlark(require = pos)] count: Option<i32>, \n - ) -> anyhow::Result<String> { \n + heap: &'v Heap, \n + ) -> anyhow::Result<Value<'v>> { \n match count { \n - Some(count) if count >= 0 => Ok(this.replacen(old, new, count as usize)), \n + Some(count) if count >= 0 => Ok(heap \n + .alloc_str(&this.typed.replacen(old, new, count as usize)) \n + .to_value()), \n Some(count) => Err(anyhow!(\"Replace final argument was negative '{}'\", count)), \n - None => Ok(this.replace(old, new)), \n + None => Ok(heap.alloc_str(&this.typed.replace(old, new)).to_value()), \n } \n }", "msg": "Use ValueOf/Value for replace\nSummary: Change the interface to prepare for allowing string reuse optimisation."}
{"diff": "a / starlark/src/eval/runtime/evaluator.rs \n  b / starlark/src/eval/runtime/evaluator.rs \n@@ -299,13 +299,13 @@ impl<'v, 'a> Evaluator<'v, 'a> { \n /// Write a profile to a file. \n /// Only valid if corresponding profiler was enabled. \n - pub fn write_profile<P: AsRef<Path>>(&self, filename: P) -> anyhow::Result<()> { \n + pub fn write_profile<P: AsRef<Path>>(&mut self, filename: P) -> anyhow::Result<()> { \n self.gen_profile()?.write(filename.as_ref()) \n } \n /// Generate profile for a given mode. \n /// Only valid if corresponding profiler was enabled. \n - pub fn gen_profile(&self) -> anyhow::Result<ProfileData> { \n + pub fn gen_profile(&mut self) -> anyhow::Result<ProfileData> { \n let mode = match &self.profile_or_instrumentation_mode { \n ProfileOrInstrumentationMode::None => { \n return Err(EvaluatorError::ProfilingNotEnabled.into());", "msg": "&mut self in Evaluator::gen_profile\nSummary: Following diff modifies `Evaluator` state in `gen_profile`."}
{"diff": "a / starlark/src/values/layout/heap/heap_type.rs \n  b / starlark/src/values/layout/heap/heap_type.rs \n@@ -692,6 +692,15 @@ impl Heap { \n x.alloc_value(self) \n } \n + /// Allocate a value and return [`ValueTyped`] of it. \n + /// Can fail if the [`AllocValue`] trait generates a different type on the heap. \n + pub fn alloc_typed<'v, T: AllocValue<'v> + StarlarkValue<'v>>( \n + &'v self, \n + x: T, \n + ) -> ValueTyped<'v, T> { \n + ValueTyped::new(self.alloc(x)).expect(\"just allocated value must have the right type\") \n + } \n + \n /// Allocate a value and return [`ValueOf`] of it. \n pub fn alloc_value_of<'v, T>(&'v self, x: T) -> ValueOf<'v, &'v T> \n where", "msg": "Add alloc_typed\nSummary: Helper to allocate something and immediately assert it has the type you expected."}
{"diff": "a / starlark/src/stdlib/funcs.rs \n  b / starlark/src/stdlib/funcs.rs \n@@ -39,6 +39,7 @@ use crate::values::none::NoneType; \n use crate::values::num::Num; \n use crate::values::range::Range; \n use crate::values::string::STRING_TYPE; \n +use crate::values::tuple::TupleRef; \n use crate::values::types::tuple::value::Tuple; \n use crate::values::AllocValue; \n use crate::values::FrozenStringValue; \n @@ -1090,6 +1091,10 @@ pub(crate) fn global_functions(builder: &mut GlobalsBuilder) { \n ) -> anyhow::Result<Value<'v>> { \n let mut l = Vec::new(); \n if let Some(a) = a { \n + if TupleRef::from_value(a).is_some() { \n + return Ok(a); \n + } \n + \n a.with_iterator(heap, |it| { \n l.extend(it); \n })?; \n @@ -1217,4 +1222,9 @@ hash(foo) \n assert::fail(\"int('2147483648')\", \"overflow\"); \n assert::fail(\"int('-2147483649')\", \"overflow\"); \n } \n + \n + #[test] \n + fn test_tuple() { \n + assert::eq(\"(1, 2)\", \"tuple((1, 2))\"); \n + } \n }", "msg": "Do not allocate new tuple when tuple function argument is already a tuple"}
{"diff": "a / rage/src/bin/rage-mount/main.rs \n  b / rage/src/bin/rage-mount/main.rs \n@@ -126,8 +126,22 @@ where \n } \n fn main() -> Result<(), Error> { \n + use std::env::args; \n + \n env_logger::builder().format_timestamp(None).init(); \n + let args = args().collect::<Vec<_>>(); \n + \n + if console::user_attended() && args.len() == 1 { \n + // If gumdrop ever merges that PR, that can be used here \n + // instead. \n + println!(\"Usage: {} [OPTIONS]\", args[0]); \n + println!(); \n + println!(\"{}\", AgeMountOptions::usage()); \n + \n + return Ok(()); \n + } \n + \n let opts = AgeMountOptions::parse_args_default_or_exit(); \n if opts.filename.is_empty() {", "msg": "Print the default rage-mount help to stdout with no args or flags"}
{"diff": "a / age/benches/throughput.rs \n  b / age/benches/throughput.rs \n@@ -6,7 +6,7 @@ use std::io::{self, Write}; \n const KB: usize = 1024; \n fn bench(c: &mut Criterion<CyclesPerByte>) { \n - let encryptor = Encryptor::Keys(vec![SecretKey::generate().to_public()]); \n + let recipients = vec![SecretKey::generate().to_public()]; \n let mut group = c.benchmark_group(\"stream\"); \n for size in &[KB, 2 * KB, 4 * KB, 8 * KB, 16 * KB, 128 * KB] { \n @@ -15,7 +15,9 @@ fn bench(c: &mut Criterion<CyclesPerByte>) { \n group.throughput(Throughput::Bytes(*size as u64)); \n group.bench_function(BenchmarkId::new(\"encrypt\", size), |b| { \n - let mut output = encryptor.wrap_output(io::sink(), Format::Binary).unwrap(); \n + let mut output = Encryptor::with_recipients(recipients.clone()) \n + .wrap_output(io::sink(), Format::Binary) \n + .unwrap(); \n b.iter(|| output.write_all(&buf)) \n });", "msg": "Update benchmark code for Encryptor API change"}
{"diff": "a / relayer/src/worker/wallet.rs \n  b / relayer/src/worker/wallet.rs \nuse std::time::Duration; \n -use tracing::{debug, error_span}; \n +use tracing::{error_span, trace}; \n use crate::{ \n chain::handle::ChainHandle, \n @@ -28,7 +28,7 @@ pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n )) \n })?; \n - debug!(%amount, denom = %balance.denom, account = %key.account, \"wallet balance\"); \n + trace!(%amount, denom = %balance.denom, account = %key.account, \"wallet balance\"); \n telemetry!( \n wallet_balance,", "msg": "Change wallet balance debug to trace in wallet worker\nChange debug to trace for wallet balance"}
{"diff": "a / relayer/src/worker/wallet.rs \n  b / relayer/src/worker/wallet.rs \n@@ -4,12 +4,11 @@ use tracing::{error_span, trace}; \n use crate::{ \n chain::handle::ChainHandle, \n - util::task::{spawn_background_task, Next, TaskHandle}, \n + telemetry, \n + util::task::{spawn_background_task, Next, TaskError, TaskHandle}, \n }; \n pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n - use crate::{telemetry, util::task::TaskError}; \n - \n let span = error_span!(\"wallet\", chain = %chain.id()); \n spawn_background_task(span, Some(Duration::from_secs(5)), move || { \n @@ -21,7 +20,7 @@ pub fn spawn_wallet_worker<Chain: ChainHandle>(chain: Chain) -> TaskHandle { \n TaskError::Ignore(format!(\"failed to query balance for the account: {e}\")) \n })?; \n - let amount = balance.amount.parse().map_err(|_| { \n + let amount: u64 = balance.amount.parse().map_err(|_| { \n TaskError::Ignore(format!( \n \"failed to parse amount into u64: {}\", \n balance.amount", "msg": "Add type ascription to fix build when `telemetry` feature is disabled"}
{"diff": "a / src/api/raw.rs \n  b / src/api/raw.rs \n@@ -576,7 +576,9 @@ impl RawApi { \n let mut req = http::Request::put(urlstr); \n Ok(req.body(data).context(ErrorKind::RequestBuild)?) \n } \n +} \n +impl RawApi { \n /// Get a pod logs \n pub fn log(&self, name: &str, lp: &LogParams) -> Result<http::Request<Vec<u8>>> { \n let base_url = self.make_url() + \"/\" + name + \"/\" + \"log\"; \n @@ -618,7 +620,6 @@ impl RawApi { \n let mut req = http::Request::get(urlstr); \n Ok(req.body(vec![]).context(ErrorKind::RequestBuild)?) \n } \n - \n } \n #[test]", "msg": "move log function to a new impl RawApi block"}
{"diff": "a / kube/src/config/mod.rs \n  b / kube/src/config/mod.rs \n@@ -354,5 +354,5 @@ fn hacky_cert_lifetime_for_macos(_: &Der) -> bool { \n // Expose raw config structs \n pub use file_config::{ \n AuthInfo, AuthProviderConfig, Cluster, Context, ExecConfig, Kubeconfig, NamedCluster, NamedContext, \n - NamedExtension, Preferences, \n + NamedExtension, Preferences, NamedAuthInfo, \n };", "msg": "Expose NamedAuthInfo\nExpose `NamedAuthInfo` to allow programmatic construction of\nKubeconfigs."}
{"diff": "a / kube/src/api/admission.rs \n  b / kube/src/api/admission.rs \n//! For more information on admission controllers, see: \n //! https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ \n //! https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/ \n +//! https://github.com/kubernetes/api/blob/master/admission/v1/types.go \n use crate::{ \n api::{DynamicObject, GroupVersionKind, GroupVersionResource, Resource, TypeMeta},", "msg": "add a link to the admission types"}
{"diff": "a / src/sparse/csmat.rs \n  b / src/sparse/csmat.rs \n@@ -1082,14 +1082,21 @@ where \n } \n } \n - pub fn map<F>(&self, f: F) -> CsMatI<N, I> \n + /// Return a new sparse matrix with the same sparsity pattern, with all non-zero values mapped by the function `f`. \n + pub fn map<F, N2>(&self, f: F) -> CsMatI<N2, I> \n where \n - F: FnMut(&N) -> N, \n - N: Clone, \n + F: FnMut(&N) -> N2, \n { \n - let mut res = self.to_owned(); \n - res.map_inplace(f); \n - res \n + let data: Vec<N2> = self.data.iter().map(f).collect(); \n + \n + CsMatI { \n + storage: self.storage, \n + nrows: self.nrows, \n + ncols: self.ncols, \n + indptr: self.indptr.to_vec(), \n + indices: self.indices.to_vec(), \n + data, \n + } \n } \n /// Access an element given its outer_ind and inner_ind.", "msg": "allow CsMat::map to return a new type"}

{"diff": "a / IPython / lib / irunner . py \n  b / IPython / lib / irunner . py \n \n import os \n import sys \n \n - # Third - party modules . \n - import pexpect \n + # Third - party modules : we carry a copy of pexpect to reduce the need for \n + # external dependencies , but our import checks for a system version first . \n + from IPython . external import pexpect \n \n # Global usage strings , to avoid indentation issues when typing it below . \n USAGE = \" \" \"", "msg": "Use IPython . external for pexpect import .\n"}
{"diff": "a / Lib / test / test_resource . py \n  b / Lib / test / test_resource . py \n \n limit_set = 0 \n f = open ( TESTFN , \" wb \" ) \n f . write ( \" X \" * 1024 ) \n + f . flush ( ) \n try : \n f . write ( \" Y \" ) \n f . flush ( ) \n \n for i in range ( 5 ) : \n time . sleep ( . 1 ) \n f . flush ( ) \n + f . close ( ) \n except IOError : \n if not limit_set : \n raise", "msg": "Try harder to provoke the exception since the ia64 buildbot still\n"}
{"diff": "a / sklearn / model_selection / tests / test_search . py \n  b / sklearn / model_selection / tests / test_search . py \n def test_grid_search_incorrect_param_grid ( ) : \n { ' C ' : 1 } ) \n \n \n - def test_grid_search_incorrect_param_grid ( ) : \n + def test_grid_search_param_grid_includes_sequence_of_a_zero_length ( ) : \n assert_raise_message ( \n ValueError , \n \" Parameter values for parameter ( C ) need to be a non - empty sequence . \" ,", "msg": "Test method test_grid_search_incorrect_param_grid has been renamed to test_grid_search_param_grid_includes_sequence_of_a_zero_length .\n"}
{"diff": "a / zephyr / lib / cache_helpers . py \n  b / zephyr / lib / cache_helpers . py \n def cache_save_message ( message ) : \n \n @ cache_with_key ( message_cache_key ) \n def cache_get_message ( message_id ) : \n - return Message . objects . select_related ( \" sending_client \" , \" sender \" ) . get ( id = message_id ) \n + return Message . objects . select_related ( ) . get ( id = message_id ) \n \n # Called on Tornado startup to ensure our message cache isn ' t empty \n def populate_message_cache ( ) : \n items_for_memcached = { } \n - for m in Message . objects . select_related ( \" sending_client \" , \" sender \" ) . all ( ) . order_by ( \n - \" - id \" ) [ 0 : MESSAGE_CACHE_SIZE ] : \n + for m in Message . objects . select_related ( ) . all ( ) . order_by ( \" - id \" ) [ 0 : MESSAGE_CACHE_SIZE ] : \n items_for_memcached [ message_cache_key ( m . id ) ] = ( m , ) \n \n djcache . set_many ( items_for_memcached , timeout = 3600 * 24 )", "msg": "message_cache : Query related models to avoid more queries later .\n"}
{"diff": "a / IPython / core / release . py \n  b / IPython / core / release . py \n \n _version_major = 0 \n _version_minor = 12 \n _version_micro = ' ' # use ' ' for first of series , number for 1 and above \n - _version_extra = ' dev ' \n + _version_extra = ' beta ' \n # _version_extra = ' ' # Uncomment this for full releases \n \n # Construct full version string from these .", "msg": "Change version number to beta to start the release cycle .\n"}
{"diff": "a / IPython / external / qt . py \n  b / IPython / external / qt . py \n \n from PySide import QtCore , QtGui , QtSvg \n \n else : \n - raise RuntimeError ( ' Invalid Qt API \" % s \" ' % QT_API ) \n + raise RuntimeError ( ' Invalid Qt API % r , valid values are : % r or % r ' % \n + ( QT_API , QT_API_PYQT , QT_API_PYSIDE ) )", "msg": "Improved error message for Qt API switcher .\n"}
{"diff": "a / src / you_get / extractors / universal . py \n  b / src / you_get / extractors / universal . py \n def universal_download ( url , output_dir = ' . ' , merge = True , info_only = False , * * kwarg \n \n else : \n # direct download \n - filename = parse . unquote ( url . split ( ' / ' ) [ - 1 ] ) or parse . unquote ( url . split ( ' / ' ) [ - 2 ] ) \n + url_trunk = url . split ( ' ? ' ) [ 0 ] # strip query string \n + filename = parse . unquote ( url_trunk . split ( ' / ' ) [ - 1 ] ) or parse . unquote ( url_trunk . split ( ' / ' ) [ - 2 ] ) \n title = ' . ' . join ( filename . split ( ' . ' ) [ : - 1 ] ) or filename \n _ , ext , size = url_info ( url , faker = True ) \n print_info ( site_info , title , ext , size )", "msg": "[ universal ] strip query string for direct download\n"}
{"diff": "a / IPython / core / shellapp . py \n  b / IPython / core / shellapp . py \n def init_code ( self ) : \n self . _run_exec_lines ( ) \n self . _run_exec_files ( ) \n self . _run_cmd_line_code ( ) \n + \n + # Hide variables defined here from % who etc . \n + self . shell . user_ns_hidden . update ( self . shell . user_ns ) \n \n def _run_exec_lines ( self ) : \n \" \" \" Run lines of code in IPythonApp . exec_lines in the user ' s namespace . \" \" \"", "msg": "Do not expose variables defined at startup to % who etc .\n"}
{"diff": "a / lib / ansible / modules / packaging / language / pip . py \n  b / lib / ansible / modules / packaging / language / pip . py \n def main ( ) : \n this_dir = os . path . join ( this_dir , chdir ) \n \n if module . check_mode : \n - if env or extra_args or requirements or state = = ' latest ' or not name : \n + if extra_args or requirements or state = = ' latest ' or not name : \n module . exit_json ( changed = True ) \n elif name . startswith ( ' svn + ' ) or name . startswith ( ' git + ' ) or \\ \n name . startswith ( ' hg + ' ) or name . startswith ( ' bzr + ' ) :", "msg": "Correct check mode for pip in virtualenv .\n"}
{"diff": "a / lib / logger . py \n  b / lib / logger . py \n \n from tqdm import tqdm \n \n from lib . queue_manager import queue_manager \n - from lib . sysinfo import sysinfo \n \n LOG_QUEUE = queue_manager . _log_queue # pylint : disable = protected - access \n \n def get_loglevel ( loglevel ) : \n \n def crash_log ( ) : \n \" \" \" Write debug_buffer to a crash log on crash \" \" \" \n + from lib . sysinfo import sysinfo \n path = os . getcwd ( ) \n filename = os . path . join ( path , datetime . now ( ) . strftime ( \" crash_report . % Y . % m . % d . % H % M % S % f . log \" ) )", "msg": "Move crash logging imports to crash_log function\n"}
{"diff": "a / zerver / lib / url_preview / parsers / open_graph . py \n  b / zerver / lib / url_preview / parsers / open_graph . py \n def extract_data ( self ) - > Dict [ str , str ] : \n meta = self . _soup . findAll ( ' meta ' ) \n content = { } \n for tag in meta : \n - if tag . has_attr ( ' property ' ) and ' og : ' in tag [ ' property ' ] : \n + if tag . has_attr ( ' property ' ) and ' og : ' in tag [ ' property ' ] and tag . has_attr ( ' content ' ) : \n content [ re . sub ( ' og : ' , ' ' , tag [ ' property ' ] ) ] = tag [ ' content ' ] \n return content", "msg": "url preview : Ignore open graph tags without a content attribute .\n"}
{"diff": "a / lib / ansible / modules / extras / cloud / misc / proxmox . py \n  b / lib / ansible / modules / extras / cloud / misc / proxmox . py \n \n default : present \n notes : \n - Requires proxmoxer and requests modules on host . This modules can be installed with pip . \n - requirements : [ \" proxmoxer \" , \" requests \" ] \n + requirements : [ \" proxmoxer \" , \" python > = 2 . 7 \" , \" requests \" ] \n author : \" Sergei Antipov @ UnderGreen \" \n ' ' '", "msg": "Add python - 2 . 6 requirement to the proxmox module\n"}
{"diff": "a / celery / beat . py \n  b / celery / beat . py \n def setup_schedule ( self ) : \n else : \n if ' __version__ ' not in self . _store : \n self . _store . clear ( ) # remove schedule at 2 . 2 . 2 upgrade . \n + if ' utc ' not in self . _store : \n + self . _store . clear ( ) # remove schedule at 3 . 0 . 1 upgrade . \n entries = self . _store . setdefault ( ' entries ' , { } ) \n self . merge_inplace ( self . app . conf . CELERYBEAT_SCHEDULE ) \n self . install_default_entries ( self . schedule ) \n - self . _store [ ' __version__ ' ] = __version__ \n + self . _store . update ( __version__ = __version__ , utc = True ) \n self . sync ( ) \n debug ( ' Current schedule : \\ n ' + ' \\ n ' . join ( repr ( entry ) \n for entry in entries . itervalues ( ) ) )", "msg": "Reset beat schedule to upgrade to UTC . Closes\n"}
{"diff": "a / pipenv / cli . py \n  b / pipenv / cli . py \n def pip_install ( package_name = None , r = None , allow_global = False ) : \n \n \n def pip_download ( package_name ) : \n - c = delegator . run ( ' { 0 } download \" { 1 } \" - d { 2 } ' . format ( which_pip ( ) , package_name , project . download_location ) ) \n + for source in project . sources : \n + c = delegator . run ( \n + ' { 0 } download \" { 1 } \" - i { 2 } - d { 3 } ' . format ( \n + which_pip ( ) , \n + package_name , \n + source [ ' url ' ] , \n + project . download_location \n + ) \n + ) \n + if c . return_code = = 0 : \n + break \n + \n return c", "msg": "making pip_download use all available sources by passing the ` - i ` arg to pip for each source\n"}
{"diff": "a / celery / backends / redis . py \n  b / celery / backends / redis . py \n def _get ( key ) : \n ' port ' : _get ( ' PORT ' ) or 6379 , \n ' db ' : _get ( ' DB ' ) or 0 , \n ' password ' : _get ( ' PASSWORD ' ) , \n + ' socket_timeout ' : _get ( ' SOCKET_TIMEOUT ' ) , \n ' max_connections ' : self . max_connections , \n } \n if url : \n def _params_from_url ( self , url , defaults ) : \n \n # Query parameters override other parameters \n connparams . update ( query ) \n - connparams . update ( socket_timeout = 5 ) \n return connparams \n \n def get ( self , key ) :", "msg": "Better way for setting it through CELERY_REDIS_SOCKET_TIMEOUT\n"}
{"diff": "a / lib / ansible / modules / packaging / language / bower . py \n  b / lib / ansible / modules / packaging / language / bower . py \n def main ( ) : \n name = module . params [ ' name ' ] \n offline = module . params [ ' offline ' ] \n production = module . params [ ' production ' ] \n - path = os . path . expanduser ( module . params [ ' path ' ] ) \n + path = module . params [ ' path ' ] \n relative_execpath = module . params [ ' relative_execpath ' ] \n state = module . params [ ' state ' ] \n version = module . params [ ' version ' ]", "msg": "Removes usage of expanduser because of type path\n"}
{"diff": "a / zerver / views / message_edit . py \n  b / zerver / views / message_edit . py \n def get_message_edit_history ( request : HttpRequest , user_profile : UserProfile , \n \n # Fill in all the extra data that will make it usable \n fill_edit_history_entries ( message_edit_history , message ) \n - return json_success ( { \" message_history \" : reversed ( message_edit_history ) } ) \n + return json_success ( { \" message_history \" : list ( reversed ( message_edit_history ) ) } ) \n \n PROPAGATE_MODE_VALUES = [ \" change_later \" , \" change_one \" , \" change_all \" ] \n @ has_request_variables", "msg": "message_edit : Output a list , not a reversed iterator .\n"}
{"diff": "a / sanic / response . py \n  b / sanic / response . py \n def cookies ( self ) : \n return self . _cookies \n \n \n - def json ( body , status = 200 , headers = None ) : \n + def json ( body , status = 200 , headers = None , * * kwargs ) : \n \" \" \" \n Returns response object with body in json format . \n : param body : Response data to be serialized . \n : param status : Response code . \n : param headers : Custom Headers . \n + : param \\ * * kwargs : Remaining arguments that are passed to the json encoder . \n \" \" \" \n - return HTTPResponse ( json_dumps ( body ) , headers = headers , status = status , \n + return HTTPResponse ( json_dumps ( body , * * kwargs ) , headers = headers , status = status , \n content_type = \" application / json \" )", "msg": "allowed passing arguments to json response encoder\n"}
{"diff": "a / zerver / tornado / socket . py \n  b / zerver / tornado / socket . py \n def on_close ( self ) : \n \n def fake_message_sender ( event ) : \n # type : ( Dict [ str , Any ] ) - > None \n + \" \" \" This function is used only for Casper and backend tests , where \n + rabbitmq is disabled \" \" \" \n log_data = dict ( ) # type : Dict [ str , Any ] \n record_request_start_data ( log_data ) \n \n req = event [ ' request ' ] \n try : \n sender = get_user_profile_by_id ( event [ ' server_meta ' ] [ ' user_id ' ] ) \n - client = get_client ( req [ ' client ' ] ) \n + client = get_client ( \" website \" ) \n \n msg_id = check_send_message ( sender , client , req [ ' type ' ] , \n extract_recipients ( req [ ' to ' ] ) ,", "msg": "socket : Hardcode website message sender for fake messages .\n"}
{"diff": "a / Lib / tokenize . py \n  b / Lib / tokenize . py \n class TokenError ( Exception ) : pass \n \n class StopTokenizing ( Exception ) : pass \n \n - def printtoken ( type , token , ( srow , scol ) , ( erow , ecol ) , line ) : # for testing \n + def printtoken ( type , token , srow_scol , erow_ecol , line ) : # for testing \n + srow , scol = srow_scol \n + erow , ecol = erow_ecol \n print \" % d , % d - % d , % d : \\ t % s \\ t % s \" % \\ \n ( srow , scol , erow , ecol , tok_name [ type ] , repr ( token ) )", "msg": "Remove a tuple unpacking in a parameter list to remove a SyntaxWarning raised\n"}
{"diff": "a / pipenv / utils . py \n  b / pipenv / utils . py \n def python_version ( path_to_python ) : \n if not path_to_python : \n return None \n \n + # Quote the path to Python , for Windows . \n + path_to_python = shellquote ( path_to_python ) \n + \n try : \n TEMPLATE = ' Python { } . { } . { } ' \n print ( ' { 0 } - - version ' . format ( path_to_python ) )", "msg": "quote the path to python , for windows\n"}
{"diff": "a / lib / ansible / plugins / filter / ipaddr . py \n  b / lib / ansible / plugins / filter / ipaddr . py \n def _need_netaddr ( f_name , * args , * * kwargs ) : \n ' installed on the ansible controller ' % f_name ) \n \n \n - def ip4_hex ( arg ) : \n + def ip4_hex ( arg , delimiter = ' ' ) : \n ' ' ' Convert an IPv4 address to Hexadecimal notation ' ' ' \n numbers = list ( map ( int , arg . split ( ' . ' ) ) ) \n - return ' { : 02x } { : 02x } { : 02x } { : 02x } ' . format ( * numbers ) \n + return ' { 0 : 02x } { sep } { 1 : 02x } { sep } { 2 : 02x } { sep } { 3 : 02x } ' . format ( * numbers , sep = delimiter ) \n \n \n #  - Ansible filters  -", "msg": "filter ipaddr : add custom delimiter option to ip4_hex ( ) ; fix format ( )\n"}
{"diff": "a / zerver / management / commands / create_user . py \n  b / zerver / management / commands / create_user . py \n def handle ( self , * args : Any , * * options : Any ) - > None : \n user_initial_password = initial_password ( email ) \n if user_initial_password is None : \n raise CommandError ( \" Password is unusable . \" ) \n - pw = user_initial_password . encode ( ) \n + pw = user_initial_password \n do_create_user ( email , pw , realm , full_name , email_to_username ( email ) ) \n except IntegrityError : \n raise CommandError ( \" User already exists . \" )", "msg": "check_user : Get rid of incorrect encode call for initial password .\n"}
{"diff": "a / libpathod / pathod . py \n  b / libpathod / pathod . py \n def handle ( self ) : \n self . server . ssloptions [ \" keyfile \" ] , \n ) \n \n - while 1 : \n + while not self . finished : \n line = self . rfile . readline ( ) \n if line = = \" \\ r \\ n \" or line = = \" \\ n \" : # Possible leftover from previous message \n line = self . rfile . readline ( )", "msg": "Handle client close more gracefully .\n"}
{"diff": "a / bokeh / server / protocol / __init__ . py \n  b / bokeh / server / protocol / __init__ . py \n \n ' ' ' \n from __future__ import absolute_import \n \n + import logging \n + log = logging . getLogger ( __name__ ) \n + \n from tornado . escape import json_decode \n \n from . . exceptions import ProtocolError \n def assemble ( self , header_json , metadata_json , content_json ) : \n \n ' ' ' \n header = json_decode ( header_json ) \n + if ' msgtype ' not in header : \n + log . error ( \" Bad header with no msgtype was : % r \" , header ) \n + raise ProtocolError ( \" No ' msgtype ' in header \" ) \n return self . _messages [ header [ ' msgtype ' ] ] . assemble ( \n header_json , metadata_json , content_json \n )", "msg": "raise ProtocolError if we get a header with no msgtype\n"}
{"diff": "a / Demo / scripts / mboxconvert . py \n  b / Demo / scripts / mboxconvert . py \n def mmdf ( f ) : \n ' Bad line in MMFD mailbox : % s \\ n ' % ` line ` ) \n return sts \n \n + counter = 0 # for generating unique Message - ID headers \n + \n def message ( f , delimiter = ' ' ) : \n sts = 0 \n # Parse RFC822 header \n def message ( f , delimiter = ' ' ) : \n # Copy RFC822 header \n for line in m . headers : \n print line , \n + # Invent Message - ID header if none is present \n + if not m . has_key ( ' message - id ' ) : \n + global counter \n + counter = counter + 1 \n + msgid = \" < % s . % d > \" % ( hex ( t ) , counter ) \n + sys . stderr . write ( \" Adding Message - ID % s ( From % s ) \\ n \" % \n + ( msgid , email ) ) \n + print \" Message - ID : \" , msgid \n print \n # Copy body \n while 1 :", "msg": "Invent Message - ID header if none is present\n"}
{"diff": "a / zerver / tests / test_alert_words . py \n  b / zerver / tests / test_alert_words . py \n def test_remove_word ( self ) - > None : \n \" \" \" \n user = self . example_user ( ' cordelia ' ) \n \n + expected_remaining_alerts = set ( self . interesting_alert_word_list ) \n add_user_alert_words ( user , self . interesting_alert_word_list ) \n \n - theoretical_remaining_alerts = self . interesting_alert_word_list [ : ] \n - \n for alert_word in self . interesting_alert_word_list : \n remove_user_alert_words ( user , alert_word ) \n - theoretical_remaining_alerts . remove ( alert_word ) \n + expected_remaining_alerts . remove ( alert_word ) \n actual_remaining_alerts = user_alert_words ( user ) \n self . assertEqual ( set ( actual_remaining_alerts ) , \n - set ( theoretical_remaining_alerts ) ) \n + expected_remaining_alerts ) \n \n def test_realm_words ( self ) - > None : \n \" \" \"", "msg": "test_alert_words : Use better variable names .\n"}
{"diff": "a / mobject / image_mobject . py \n  b / mobject / image_mobject . py \n class ImageMobject ( Mobject ) : \n Automatically filters out black pixels \n \" \" \" \n DEFAULT_CONFIG = { \n - \" filter_color \" : \" black \" , \n - \" invert \" : True , \n - \" use_cache \" : True , \n + \" filter_color \" : \" black \" , \n + \" invert \" : True , \n + \" use_cache \" : True , \n \" point_thickness \" : 1 , \n - \" scale_value \" : 1 . 0 , \n - \" should_center \" : True \n + \" scale_value \" : 1 . 0 , \n + \" should_center \" : True , \n } \n def __init__ ( self , image_file , * * kwargs ) : \n digest_locals ( self ) \n def __init__ ( self , image_array , * * kwargs ) : \n \n \n class MobjectFromRegion ( MobjectFromPixelArray ) : \n - def __init__ ( self , region , * * kwargs ) : \n + def __init__ ( self , region , color = None , * * kwargs ) : \n MobjectFromPixelArray . __init__ ( \n self , \n - disp . paint_region ( region ) , \n + disp . paint_region ( region , color = color ) , \n * * kwargs \n )", "msg": "Why did MobjectFromRegion not specify color ?\n"}
{"diff": "a / IPython / core / completer . py \n  b / IPython / core / completer . py \n \n \n try : \n import jedi \n + jedi . settings . case_insensitive_completion = False \n import jedi . api . helpers \n import jedi . api . classes \n JEDI_INSTALLED = True", "msg": "Change defaults to be consistent with original defaults and REPL performance .\n"}
{"diff": "a / lib / matplotlib / axes / _axes . py \n  b / lib / matplotlib / axes / _axes . py \n def fill_between ( self , x , y1 , y2 = 0 , where = None , interpolate = False , \n step : { ' pre ' , ' post ' , ' mid ' } , optional \n If not None , fill with step logic . \n \n + Returns \n + mmmmmm - \n + ` PolyCollection ` \n + Plotted polygon collection \n \n Notes \n  - - \n def fill_betweenx ( self , y , x1 , x2 = 0 , where = None , \n end points of the filled region will only occur on explicit \n values in the * x * array . \n \n + \n + Returns \n + mmmmmm - \n + ` PolyCollection ` \n + Plotted polygon collection \n + \n Notes \n  - -", "msg": "Add returns documentation to fill_between methods ( )\n"}
{"diff": "similarity index 95 % \n rename from IPython / extensions / pspersistence . py \n rename to IPython / extensions / storemagic . py \n  a / IPython / extensions / pspersistence . py \n  b / IPython / extensions / storemagic . py \n \n % store magic for lightweight persistence . \n \n Stores variables , aliases etc . in PickleShare database . \n + \n + To enable this functionality , run : : \n + \n + % load_ext storemagic \n + \n + in your IPython session . If you always want it enabled , you can list it in \n + your default profile ` ipython_config . py ` file : : \n + \n + c . InteractiveShellApp . extensions = [ ' storemagic ' ] \n \" \" \" \n \n from IPython . core . error import TryNext , UsageError", "msg": "Add docstring explaining how to enable the ` storemagic ` extension .\n"}
{"diff": "a / zephyr / management / commands / populate_db . py \n  b / zephyr / management / commands / populate_db . py \n def handle ( self , * * options ) : \n ] \n create_users ( realms , internal_humbug_users_nosubs ) \n \n + # Mark all messages as read \n + with transaction . commit_on_success ( ) : \n + UserMessage . objects . all ( ) . update ( flags = 0 ) \n + \n self . stdout . write ( \" Successfully populated test database . \\ n \" ) \n if options [ \" replay_old_messages \" ] : \n restore_saved_messages ( )", "msg": "Mark messages as read when creating them in populate_db\n"}
{"diff": "a / Lib / distutils / config . py \n  b / Lib / distutils / config . py \n def _read_pypirc ( self ) : \n \" \" \" Reads the . pypirc file . \" \" \" \n rc = self . _get_rc_file ( ) \n if os . path . exists ( rc ) : \n - print ' Using PyPI login from % s ' % rc \n + self . announce ( ' Using PyPI login from % s ' % rc ) \n repository = self . repository or self . DEFAULT_REPOSITORY \n realm = self . realm or self . DEFAULT_REALM", "msg": "Use announce instead of print , to suppress output in\n"}
{"diff": "a / IPython / core / magics / script . py \n  b / IPython / core / magics / script . py \n def _script_magics_default ( self ) : \n ' python2 ' , \n ' python3 ' , \n ' pypy ' , \n - ' julia ' , \n ] \n if os . name = = ' nt ' : \n defaults . extend ( [", "msg": "Revert \" Add Julia to default script magics . \"\n"}
{"diff": "a / zerver / management / commands / runtornado . py \n  b / zerver / management / commands / runtornado . py \n def inner_run ( ) - > None : \n # We pass display_num_errors = False , since Django will \n # likely display similar output anyway . \n self . check ( display_num_errors = False ) \n - print ( f \" Tornado server is running at http : / / { addr } : { port } / \" ) \n + print ( f \" Tornado server ( re ) started on port { port } \" ) \n \n if settings . USING_RABBITMQ : \n queue_client = get_queue_client ( )", "msg": "runtornado : Avoid providing a URL for Tornado on startup .\n"}
{"diff": "a / lib / ansible / module_utils / basic . py \n  b / lib / ansible / module_utils / basic . py \n def run_command ( self , args , check_rc = False , close_fds = False , executable = None , da \n \n shell = False \n if isinstance ( args , list ) : \n - pass \n + if use_unsafe_shell : \n + args = \" \" . join ( [ pipes . quote ( x ) for x in args ] ) \n + shell = True \n elif isinstance ( args , basestring ) and use_unsafe_shell : \n shell = True \n elif isinstance ( args , basestring ) :", "msg": "Rejoin args list into a string for run_command when using an unsafe shell\n"}
{"diff": "a / Lib / BaseHTTPServer . py \n  b / Lib / BaseHTTPServer . py \n def send_error ( self , code , message = None ) : \n explain = long \n self . log_error ( \" code % d , message % s \" , code , message ) \n self . send_response ( code , message ) \n + self . send_header ( \" Content - Type \" , \" text / html \" ) \n self . end_headers ( ) \n self . wfile . write ( self . error_message_format % \n { ' code ' : code ,", "msg": "add Content - Type header to error responses\n"}
{"diff": "a / Lib / mimetypes . py \n  b / Lib / mimetypes . py \n def read ( self , filename , strict = True ) : \n types . \n \" \" \" \n fp = open ( filename ) \n - self . readfp ( fp ) \n + self . readfp ( fp , strict ) \n fp . close ( ) \n \n def readfp ( self , fp , strict = True ) :", "msg": "Pass the strict argument from read ( ) on to readfp ( ) , so the\n"}
{"diff": "a / IPython / frontend / html / notebook / handlers . py \n  b / IPython / frontend / html / notebook / handlers . py \n def get ( self , path , include_body = True ) : \n raise HTTPError ( 403 , \" % s is not a file \" , path ) \n \n stat_result = os . stat ( abspath ) \n - modified = datetime . datetime . fromtimestamp ( stat_result [ stat . ST_MTIME ] ) \n + modified = datetime . datetime . utcfromtimestamp ( stat_result [ stat . ST_MTIME ] ) \n \n self . set_header ( \" Last - Modified \" , modified ) \n \n def get ( self , path , include_body = True ) : \n ims_value = self . request . headers . get ( \" If - Modified - Since \" ) \n if ims_value is not None : \n date_tuple = email . utils . parsedate ( ims_value ) \n - if_since = datetime . datetime . fromtimestamp ( time . mktime ( date_tuple ) ) \n + if_since = datetime . datetime ( * date_tuple [ : 6 ] ) \n if if_since > = modified : \n self . set_status ( 304 ) \n return", "msg": "backport If - Modified - Since fix from tornado\n"}
{"diff": "similarity index 96 % \n rename from conversions / bin_to_octal . py \n rename to conversions / binary_to_octal . py \n  a / conversions / bin_to_octal . py \n  b / conversions / binary_to_octal . py \n def bin_to_octal ( bin_string : str ) - > str : \n while len ( bin_string ) % 3 ! = 0 : \n bin_string = \" 0 \" + bin_string \n bin_string_in_3_list = [ \n - bin_string [ index : index + 3 ] \n + bin_string [ index : index + 3 ] \n for index , value in enumerate ( bin_string ) \n if index % 3 = = 0 \n ]", "msg": "Update and rename bin_to_octal . py to binary_to_octal . py ( )\n"}
{"diff": "a / lib / ansible / plugins / loader . py \n  b / lib / ansible / plugins / loader . py \n def all ( self , * args , * * kwargs ) : \n if not found_in_cache : \n self . _load_config_defs ( basename , path ) \n \n - self . _update_object ( obj , name , path ) \n + self . _update_object ( obj , basename , path ) \n yield obj", "msg": "We need to save the basename into an attribute for calling code\n"}
{"diff": "a / numpy / __init__ . py \n  b / numpy / __init__ . py \n \n \n def test ( level = 1 , verbosity = 1 ) : \n return NumpyTest ( ) . test ( level , verbosity ) \n + test . __doc__ = NumpyTest . test . __doc__ \n \n import add_newdocs", "msg": "Don ' t want to assign test to NumpyTest ( ) . test ( which instantiates NumpyTest ) ,\n"}
{"diff": "a / zerver / lib / actions . py \n  b / zerver / lib / actions . py \n def do_update_message ( user_profile , message_id , subject , propagate_mode , content \n event [ ' message_ids ' ] = [ ] \n for changed_message in changed_messages : \n event [ ' message_ids ' ] . append ( changed_message . id ) \n - cache_save_message ( changed_message ) \n + items_for_memcached [ message_cache_key ( changed_message . id ) ] = ( changed_message , ) \n items_for_memcached [ to_dict_cache_key ( changed_message , True ) ] = \\ \n ( stringify_message_dict ( changed_message . to_dict_uncached ( apply_markdown = True ) ) , ) \n items_for_memcached [ to_dict_cache_key ( changed_message , False ) ] = \\", "msg": "Bulk update messages in deprecated cache when updating message\n"}
{"diff": "a / lib / matplotlib / axis . py \n  b / lib / matplotlib / axis . py \n def get_minor_ticks ( self , numticks = None ) : \n \n def grid ( self , b = None , which = ' major ' , * * kwargs ) : \n \" \" \" \n - Set the axis grid on or off ; b is a boolean use * which * = \n - ' major ' | ' minor ' to set the grid for major or minor ticks \n + Set the axis grid on or off ; b is a boolean . Use * which * = \n + ' major ' | ' minor ' to set the grid for major or minor ticks . \n \n - if * b * is * None * and len ( kwargs ) = = 0 , toggle the grid state . If \n + If * b * is * None * and len ( kwargs ) = = 0 , toggle the grid state . If \n * kwargs * are supplied , it is assumed you want the grid on and * b * \n - will be set to True \n + will be set to True . \n \n * kwargs * are used to set the line properties of the grids , eg ,", "msg": "convert Axis . grid ( ) docstrings to use complete sentences\n"}
{"diff": "a / Lib / test / test_sys . py \n  b / Lib / test / test_sys . py \n def test_compact_freelists ( self ) : \n del floats \n # should free more than 200 blocks each \n r = sys . _compact_freelists ( ) \n + self . assert_ ( r [ 0 ] [ 1 ] > 200 , r [ 0 ] [ 1 ] ) \n + self . assert_ ( r [ 1 ] [ 2 ] > 200 , r [ 1 ] [ 1 ] ) \n + \n self . assert_ ( r [ 0 ] [ 2 ] > 200 , r [ 0 ] [ 2 ] ) \n self . assert_ ( r [ 1 ] [ 2 ] > 200 , r [ 1 ] [ 2 ] )", "msg": "Increase debugging to investige failing tests on some build bots\n"}
{"diff": "a / telegram / ext / jobqueue . py \n  b / telegram / ext / jobqueue . py \n class JobQueue ( object ) : \n def __init__ ( self , bot ) : \n self . queue = PriorityQueue ( ) \n self . bot = bot \n - self . logger = logging . getLogger ( __name__ ) \n + self . logger = logging . getLogger ( self . __class__ . __name__ ) \n self . __lock = Lock ( ) \n self . __tick = Event ( ) \n self . _next_peek = None", "msg": "JobQueue : use class name for the logger name\n"}
{"diff": "a / zipline / pipeline / factors / factor . py \n  b / zipline / pipeline / factors / factor . py \n def if_not_float64_tell_caller_to_use_isnull ( f ) : \n directing the user to ` isnull ` or ` notnull ` instead . \n \" \" \" \n @ wraps ( f ) \n - def wrapped_method ( self , * args , * * kwargs ) : \n + def wrapped_method ( self ) : \n if self . dtype ! = float64_dtype : \n raise TypeError ( \n \" { meth } ( ) was called on a factor of dtype { dtype } . \\ n \" \n def wrapped_method ( self , * args , * * kwargs ) : \n dtype = self . dtype , \n ) , \n ) \n - return f ( self , * args , * * kwargs ) \n + return f ( self ) \n return wrapped_method", "msg": "MAINT : Use more specific signature in decorator .\n"}
{"diff": "a / Lib / mailbox . py \n  b / Lib / mailbox . py \n class Maildir : \n def __init__ ( self , dirname ) : \n import string \n self . dirname = dirname \n - self . boxes = [ ] \n \n # check for new mail \n newdir = os . path . join ( self . dirname , ' new ' ) \n - for file in os . listdir ( newdir ) : \n - if len ( string . split ( file , ' . ' ) ) > 2 : \n - self . boxes . append ( os . path . join ( newdir , file ) ) \n + boxes = [ os . path . join ( newdir , f ) \n + for f in os . listdir ( newdir ) if f [ 0 ] ! = ' . ' ] \n \n # Now check for current mail in this maildir \n curdir = os . path . join ( self . dirname , ' cur ' ) \n - for file in os . listdir ( curdir ) : \n - if len ( string . split ( file , ' . ' ) ) > 2 : \n - self . boxes . append ( os . path . join ( curdir , file ) ) \n + boxes + = [ os . path . join ( curdir , f ) \n + for f in os . listdir ( curdir ) if f [ 0 ] ! = ' . ' ] \n \n def next ( self ) : \n if not self . boxes :", "msg": "Maildir . __init__ ( ) : Use the correct filter for filenames , so that this\n"}
{"diff": "a / mmdet / models / dense_heads / gfl_head . py \n  b / mmdet / models / dense_heads / gfl_head . py \n def forward ( self , x ) : \n offsets from the box center in four directions , shape ( N , 4 ) . \n \" \" \" \n x = F . softmax ( x . reshape ( - 1 , self . reg_max + 1 ) , dim = 1 ) \n - x = F . linear ( x , self . project ) . reshape ( - 1 , 4 ) \n + x = F . linear ( x , self . project . type_as ( x ) ) . reshape ( - 1 , 4 ) \n return x", "msg": "Update to support fp16 training of GFL ( )\n"}
{"diff": "a / numpy / oldnumeric / typeconv . py \n  b / numpy / oldnumeric / typeconv . py \n \n \n - __all__ = [ ' oldtype2dtype ' , ' convtypecode ' , ' convtypecode2 ' ] \n + __all__ = [ ' oldtype2dtype ' , ' convtypecode ' , ' convtypecode2 ' , ' oldtypecodes ' ] \n \n import numpy as N \n \n def convtypecode2 ( typecode , dtype = None ) : \n return N . dtype ( typecode ) \n else : \n return dtype \n + \n + _changedtypes = { ' B ' : ' b ' , \n + ' b ' : ' 1 ' , \n + ' h ' : ' s ' , \n + ' H ' : ' w ' , \n + ' I ' : ' u ' } \n + \n + class _oldtypecodes ( dict ) : \n + def __getitem__ ( self , obj ) : \n + char = N . dtype ( obj ) . char \n + try : \n + return _changedtypes [ char ] \n + except KeyError : \n + return char \n + \n + \n + oldtypecodes = _oldtypecodes ( )", "msg": "Add oldtypecodes to oldnumeric . typeconv so that old Numeric character codes can be identified .\n"}
{"diff": "a / IPython / frontend / html / notebook / notebookapp . py \n  b / IPython / frontend / html / notebook / notebookapp . py \n class NotebookApp ( BaseIPythonApplication ) : \n config = True , \n help = \" Set the log level by value or name . \" ) \n \n + # create requested profiles by default , if they don ' t exist : \n + auto_create = Bool ( True ) \n + \n # Network related information . \n \n ip = Unicode ( LOCALHOST , config = True ,", "msg": "set auto_create flag for notebook apps\n"}
{"diff": "a / sklearn / decomposition / sparse_pca . py \n  b / sklearn / decomposition / sparse_pca . py \n def transform ( self , X , ridge_alpha = None ) : \n \" \" \" \n ridge_alpha = self . ridge_alpha if ridge_alpha is None else ridge_alpha \n U = ridge_regression ( self . components_ . T , X . T , ridge_alpha , \n - solver = ' dense_cholesky ' ) \n + solver = ' cholesky ' ) \n s = np . sqrt ( ( U * * 2 ) . sum ( axis = 0 ) ) \n s [ s = = 0 ] = 1 \n U / = s", "msg": "Changed solver from ' dense_cholesky ' to ' cholesky ' to eliminate deprecation warning .\n"}
{"diff": "a / homeassistant / components / switch / edimax . py \n  b / homeassistant / components / switch / edimax . py \n def update ( self ) : \n \" \" \" Update edimax switch . \" \" \" \n try : \n self . _now_power = float ( self . smartplug . now_power ) \n - except ValueError : \n + except ( TypeError , ValueError ) : \n self . _now_power = None \n \n try : \n self . _now_energy_day = float ( self . smartplug . now_energy_day ) \n - except ValueError : \n + except ( TypeError , ValueError ) : \n self . _now_energy_day = None \n \n self . _state = self . smartplug . state = = ' ON '", "msg": "Restore typeerror check for units sans energy tracking ( )\n"}
{"diff": "a / redash / authentication / __init__ . py \n  b / redash / authentication / __init__ . py \n def sign ( key , path , expires ) : \n @ login_manager . user_loader \n def load_user ( user_id_with_identity ) : \n org = current_org . _get_current_object ( ) \n - user_id , _ = user_id_with_identity . split ( \" - \" ) \n + is_legacy_session_identifier = user_id_with_identity . find ( ' - ' ) < 0 \n + \n + if is_legacy_session_identifier : \n + user_id = user_id_with_identity \n + else : \n + user_id , _ = user_id_with_identity . split ( \" - \" ) \n \n try : \n user = models . User . get_by_id_and_org ( user_id , org ) \n - if user . is_disabled or user . get_id ( ) ! = user_id_with_identity : \n + if user . is_disabled : \n + return None \n + \n + if not is_legacy_session_identifier and user . get_id ( ) ! = user_id_with_identity : \n return None \n \n return user", "msg": "backward compatibility so users who have the old session identifier don ' t get logged out\n"}
{"diff": "a / tests / integration / project_test . py \n  b / tests / integration / project_test . py \n def test_up_with_network_link_local_ips ( self ) : \n name = ' composetest ' , \n config_data = config_data \n ) \n - project . up ( ) \n + project . up ( detached = True ) \n \n - service_container = project . get_service ( ' web ' ) . containers ( ) [ 0 ] \n + service_container = project . get_service ( ' web ' ) . containers ( stopped = True ) [ 0 ] \n ipam_config = service_container . inspect ( ) . get ( \n ' NetworkSettings ' , { } \n ) . get ( \n def test_up_with_isolation ( self ) : \n name = ' composetest ' , \n config_data = config_data \n ) \n - project . up ( ) \n - service_container = project . get_service ( ' web ' ) . containers ( ) [ 0 ] \n + project . up ( detached = True ) \n + service_container = project . get_service ( ' web ' ) . containers ( stopped = True ) [ 0 ] \n assert service_container . inspect ( ) [ ' HostConfig ' ] [ ' Isolation ' ] = = ' default ' \n \n @ v2_1_only ( )", "msg": "Improve robustness of a couple integration tests with occasional failures\n"}
{"diff": "a / rest_framework / fields . py \n  b / rest_framework / fields . py \n def __init__ ( self , * args , * * kwargs ) : \n self . view_name = kwargs . pop ( ' view_name ' ) \n except : \n raise ValueError ( \" Hyperlinked field requires ' view_name ' kwarg \" ) \n + self . slug_url_kwarg = kwargs . pop ( ' slug_url_kwargs ' , self . slug_url_kwarg ) \n + self . slug_field = kwargs . pop ( ' slug_field ' , self . slug_field ) \n self . format = kwargs . pop ( ' format ' , None ) \n super ( HyperlinkedRelatedField , self ) . __init__ ( * args , * * kwargs )", "msg": "added support for custom slug field and kwargs\n"}
{"diff": "a / spacy / pipeline / entity_linker . py \n  b / spacy / pipeline / entity_linker . py \n def make_entity_linker ( \n incl_prior : bool , \n incl_context : bool , \n ) : \n + \" \" \" Construct an EntityLinker component . \n + \n + model ( Model [ List [ Doc ] , Floats2d ] ) : A model that learns document vector \n + representations . Given a batch of Doc objects , it should return a single \n + array , with one row per item in the batch . \n + kb ( KnowledgeBase ) : The knowledge - base to link entities to . \n + labels_discard ( Iterable [ str ] ) : NER labels that will automatically get a \" NIL \" prediction . \n + incl_prior ( bool ) : Whether or not to include prior probabilities from the KB in the model . \n + incl_context ( bool ) : Whether or not to include the local context in the model . \n + \" \" \" \n return EntityLinker ( \n nlp . vocab , \n model ,", "msg": "Add docstring for entity linker factory\n"}
{"diff": "a / lib / ansible / release . py \n  b / lib / ansible / release . py \n \n from __future__ import ( absolute_import , division , print_function ) \n __metaclass__ = type \n \n - __version__ = ' 2 . 6 . 0a1 ' \n + __version__ = ' 2 . 6 . 0dev0 ' \n __author__ = ' Ansible , Inc . ' \n __codename__ = ' Heartbreaker '", "msg": "Update Ansible release version to 2 . 6 . 0dev0 .\n"}
{"diff": "a / scrapy / trunk / scrapy / utils / markup . py \n  b / scrapy / trunk / scrapy / utils / markup . py \n \n import re \n import htmlentitydefs \n \n - ent_re = re . compile ( r ' & ( # ? ) ( [ ^ & ; ] + ) ; ' ) \n + _ent_re = re . compile ( r ' & ( # ? ) ( [ ^ & ; ] + ) ; ' ) \n _tag_re = re . compile ( r ' < [ a - zA - Z \\ / ! ] . * ? > ' , re . DOTALL ) \n \n def remove_entities ( text , keep = ( ) , remove_illegal = True ) : \n def convert_entity ( m ) : \n else : \n return u ' & % s ; ' % m . group ( 2 ) \n \n - return ent_re . sub ( convert_entity , text . decode ( ' utf - 8 ' ) ) \n + return _ent_re . sub ( convert_entity , text . decode ( ' utf - 8 ' ) ) \n \n + def has_entities ( text ) : \n + return bool ( _ent_re . search ( text ) ) \n \n def replace_tags ( text , token = ' ' ) : \n \" \" \" Replace all markup tags found in the given text by the given token . By", "msg": "rolled back public ent_re to private and added a function has_entities instead\n"}
{"diff": "a / Lib / test / test_logging . py \n  b / Lib / test / test_logging . py \n def setup_via_listener ( self , text ) : \n logging . config . stopListening ( ) \n t . join ( 2 . 0 ) \n \n - @ unittest . skip ( \" See issue # 7857 \" ) \n + # @ unittest . skip ( \" See issue # 7857 \" ) \n def test_listen_config_10_ok ( self ) : \n with captured_stdout ( ) as output : \n self . setup_via_listener ( json . dumps ( self . config10 ) )", "msg": "Issue : Tentatively re - enabling one test to see effect on buildbots .\n"}
{"diff": "a / distribute_setup . py \n  b / distribute_setup . py \n def quote ( arg ) : \n args = [ quote ( arg ) for arg in args ] \n return os . spawnl ( os . P_WAIT , sys . executable , * args ) = = 0 \n \n + MINIMUM_VERSION = \" 0 . 6 . 28 \" \n DEFAULT_VERSION = \" 0 . 6 . 45 \" \n DEFAULT_URL = \" http : / / pypi . python . org / packages / source / d / distribute / \" \n SETUPTOOLS_FAKED_VERSION = \" 0 . 6c11 \" \n def _do_download ( version , download_base , to_dir , download_delay ) : \n setuptools . bootstrap_install_from = egg \n \n \n - def use_setuptools ( version = DEFAULT_VERSION , download_base = DEFAULT_URL , \n + def use_setuptools ( version = MINIMUM_VERSION , download_base = DEFAULT_URL , \n to_dir = os . curdir , download_delay = 15 , no_fake = True ) : \n # making sure we use the absolute path \n to_dir = os . path . abspath ( to_dir )", "msg": "Allow for a minimum distribute version that is older than the downloaded version\n"}
{"diff": "a / lib / ansible / plugins / connections / docker . py \n  b / lib / ansible / plugins / connections / docker . py \n def normalize ( v ) : \n \n def _connect ( self , port = None ) : \n \" \" \" Connect to the container . Nothing to do \" \" \" \n + if not self . _connected : \n + self . _display . vvv ( \" ESTABLISH LOCAL CONNECTION FOR USER : { 0 } \" . format ( \n + self . _play_context . remote_user , host = self . _play_context . remote_addr ) \n + ) \n + self . _connected = True \n + \n return self \n \n def exec_command ( self , cmd , tmp_path , sudo_user = None , sudoable = False , \n def fetch_file ( self , in_path , out_path ) : \n \n def close ( self ) : \n \" \" \" Terminate the connection . Nothing to do for Docker \" \" \" \n - pass \n + self . _connected = False", "msg": "fake being connected for logging purposes\n"}
{"diff": "a / keras / layers / recurrent . py \n  b / keras / layers / recurrent . py \n def get_output ( self , train = False ) : \n if not self . input_shape [ 1 ] : \n raise Exception ( ' When using TensorFlow , you should define ' + \n ' explicitly the number of timesteps of ' + \n - ' your sequences . Make sure the first layer ' + \n - ' has a \" batch_input_shape \" argument ' + \n - ' including the samples axis . ' ) \n + ' your sequences . \\ n ' + \n + ' If your first layer is an Embedding , ' + \n + ' make sure to pass it an \" input_length \" ' + \n + ' argument . Otherwise , make sure ' + \n + ' the first layer has ' + \n + ' an \" input_shape \" or \" batch_input_shape \" ' + \n + ' argument , including the time axis . ' ) \n if self . stateful : \n initial_states = self . states \n else :", "msg": "Improve error message in recurrent . py\n"}
{"diff": "a / lib / ansible / module_utils / junos . py \n  b / lib / ansible / module_utils / junos . py \n def get_config ( self , config_format = \" text \" ) : \n return ele \n \n def load_config ( self , config , commit = False , replace = False , confirm = None , \n - comment = None , config_format = ' text ' ) : \n + comment = None , config_format = ' text ' , overwrite = False ) : \n + \n + if all ( [ replace , overwrite ] ) : \n + self . raise_exc ( ' setting both replace and overwrite to True is invalid ' ) \n \n if replace : \n merge = False \n - overwrite = True \n + overwrite = False \n + elif overwrite : \n + merge = True \n + overwrite = False \n else : \n merge = True \n overwrite = False", "msg": "adds overwrite kwarg to load_config in junos ( )\n"}
{"diff": "a / redash / models . py \n  b / redash / models . py \n def should_schedule_next ( previous_iteration , now , schedule ) : \n \n class Query ( ChangeTrackingMixin , TimestampMixin , BelongsToOrgMixin , db . Model ) : \n id = Column ( db . Integer , primary_key = True ) \n - version = Column ( db . Integer ) \n + version = Column ( db . Integer , default = 1 ) \n org_id = Column ( db . Integer , db . ForeignKey ( ' organizations . id ' ) ) \n org = db . relationship ( Organization , backref = \" queries \" ) \n data_source_id = Column ( db . Integer , db . ForeignKey ( \" data_sources . id \" ) , nullable = True )", "msg": "Set default value for query version\n"}
{"diff": "a / plugins / inventory / linode . py \n  b / plugins / inventory / linode . py \n def is_cache_valid ( self ) : \n if os . path . isfile ( self . cache_path_index ) : \n return True \n return False \n - \n + \n def read_settings ( self ) : \n \" \" \" Reads the settings from the . ini file . \" \" \" \n config = ConfigParser . SafeConfigParser ( ) \n def add_node ( self , node ) : \n dest = node . label \n \n # Add to index \n - self . index [ dest ] = [ node . api_id ] \n + self . index [ dest ] = node . api_id \n \n # Inventory : Group by node ID ( always a group of 1 ) \n self . inventory [ node . api_id ] = [ dest ]", "msg": "Save api id to index as an int instead of a list\n"}
{"diff": "a / Lib / distutils / fancy_getopt . py \n  b / Lib / distutils / fancy_getopt . py \n def __init__ ( self , option_table = None ) : \n \n \n def _build_index ( self ) : \n + self . option_index . clear ( ) \n for option in self . option_table : \n self . option_index [ option [ 0 ] ] = option \n \n def _grok_option_table ( self ) : \n the option table . Called by ' getopt ( ) ' before it can do \n anything worthwhile . \" \" \" \n \n + self . long_opts = [ ] \n + self . short_opts = [ ] \n + self . short2long . clear ( ) \n + \n for option in self . option_table : \n try : \n ( long , short , help ) = option", "msg": "Fix so we clear or reinitialize various data structures before populating\n"}
{"diff": "a / lib / ansible / module_utils / ec2 . py \n  b / lib / ansible / module_utils / ec2 . py \n def page ( * args , * * kwargs ) : \n return page \n return wrapper \n \n + \n + def camel_dict_to_snake_dict ( camel_dict ) : \n + \n + def camel_to_snake ( name ) : \n + \n + import re \n + \n + first_cap_re = re . compile ( ' ( . ) ( [ A - Z ] [ a - z ] + ) ' ) \n + all_cap_re = re . compile ( ' ( [ a - z0 - 9 ] ) ( [ A - Z ] ) ' ) \n + s1 = first_cap_re . sub ( r ' \\ 1_ \\ 2 ' , name ) \n + \n + return all_cap_re . sub ( r ' \\ 1_ \\ 2 ' , s1 ) . lower ( ) \n + \n + \n + snake_dict = { } \n + for k , v in camel_dict . iteritems ( ) : \n + if isinstance ( v , dict ) : \n + v = camel_dict_to_snake_dict ( v ) \n + snake_dict [ camel_to_snake ( k ) ] = v \n + \n + return snake_dict", "msg": "Add function to convert CamelCased key names to snake_names\n"}
{"diff": "a / doc / make . py \n  b / doc / make . py \n def texinfo ( ) : \n def clean ( ) : \n \" \" \" Remove generated files . \" \" \" \n shutil . rmtree ( \" build \" , ignore_errors = True ) \n - shutil . rmtree ( \" examples \" , ignore_errors = True ) \n + shutil . rmtree ( \" tutorials \" , ignore_errors = True ) \n shutil . rmtree ( \" api / _as_gen \" , ignore_errors = True ) \n for pattern in [ ' _static / matplotlibrc ' , \n ' _templates / gallery . html ' ,", "msg": "Update make . py clean for tutorials\n"}
{"diff": "a / lib / mpl_toolkits / mplot3d / axes3d . py \n  b / lib / mpl_toolkits / mplot3d / axes3d . py \n def can_pan ( self ) : \n def cla ( self ) : \n \" \" \" Clear axes and disable mouse button callbacks . \n \" \" \" \n - self . disable_mouse_rotation ( ) \n + # Disabling mouse interaction might have been needed a long \n + # time ago , but I can ' t find a reason for it now - BVR ( 2012 - 03 ) \n + # self . disable_mouse_rotation ( ) \n self . zaxis . cla ( ) \n \n # TODO : Support sharez", "msg": "Don ' t disable mouse interaction upon call to cla ( ) in mplot3d\n"}
{"diff": "a / Lib / logging / config . py \n  b / Lib / logging / config . py \n def resolve ( self , s ) : \n \" \" \" \n name = s . split ( ' . ' ) \n used = name . pop ( 0 ) \n - found = self . importer ( used ) \n - for frag in name : \n - used + = ' . ' + frag \n - try : \n - found = getattr ( found , frag ) \n - except AttributeError : \n - self . importer ( used ) \n - found = getattr ( found , frag ) \n - return found \n + try : \n + found = self . importer ( used ) \n + for frag in name : \n + used + = ' . ' + frag \n + try : \n + found = getattr ( found , frag ) \n + except AttributeError : \n + self . importer ( used ) \n + found = getattr ( found , frag ) \n + return found \n + except ImportError : \n + e , tb = sys . exc_info ( ) [ 1 : ] \n + v = ValueError ( ' Cannot resolve % r : % s ' % ( s , e ) ) \n + v . __cause__ , v . __traceback__ = e , tb \n + raise v \n \n def ext_convert ( self , value ) : \n \" \" \" Default converter for the ext : / / protocol . \" \" \"", "msg": "Logging : improved error reporting for BaseConfigurator . resolve ( ) .\n"}
{"diff": "a / Lib / collections . py \n  b / Lib / collections . py \n def move_to_end ( self , key , last = True ) : \n \n def __repr__ ( self ) : \n ' od . __repr__ ( ) < = = > repr ( od ) ' \n - if self . __in_repr : \n - return ' . . . ' \n if not self : \n return ' % s ( ) ' % ( self . __class__ . __name__ , ) \n + if self . __in_repr : \n + return ' . . . ' \n self . __in_repr = True \n try : \n result = ' % s ( % r ) ' % ( self . __class__ . __name__ , list ( self . items ( ) ) )", "msg": "Put tests in more logical order .\n"}
{"diff": "a / homeassistant / components / trend / binary_sensor . py \n  b / homeassistant / components / trend / binary_sensor . py \n \n import logging \n import math \n \n + import numpy as np \n import voluptuous as vol \n \n from homeassistant . components . binary_sensor import ( \n \n CONF_DEVICE_CLASS , \n CONF_ENTITY_ID , \n CONF_FRIENDLY_NAME , \n - STATE_UNKNOWN , \n - STATE_UNAVAILABLE , \n CONF_SENSORS , \n + STATE_UNAVAILABLE , \n + STATE_UNKNOWN , \n ) \n from homeassistant . core import callback \n import homeassistant . helpers . config_validation as cv \n def _calculate_gradient ( self ) : \n \n This need run inside executor . \n \" \" \" \n - import numpy as np \n - \n timestamps = np . array ( [ t for t , _ in self . samples ] ) \n values = np . array ( [ s for _ , s in self . samples ] ) \n coeffs = np . polyfit ( timestamps , values , 1 )", "msg": "Move trend imports to top level ( )\n"}
{"diff": "a / zephyr / views . py \n  b / zephyr / views . py \n def update_pointer_backend ( request , user_profile , \n if pointer < = user_profile . pointer : \n return json_success ( ) \n \n + prev_pointer = user_profile . pointer \n user_profile . pointer = pointer \n user_profile . save ( update_fields = [ \" pointer \" ] ) \n \n def update_pointer_backend ( request , user_profile , \n # this is a shim that will mark as read any messages up until the \n # pointer move \n UserMessage . objects . filter ( user_profile = user_profile , \n + message__id__gt = prev_pointer , \n message__id__lte = pointer , \n flags = ~ UserMessage . flags . read ) \\ \n . update ( flags = F ( ' flags ' ) . bitor ( UserMessage . flags . read ) )", "msg": "Add lower message id bound when marking messages as read for the mobile unread count hack\n"}
{"diff": "a / sklearn / linear_model / ransac . py \n  b / sklearn / linear_model / ransac . py \n class RANSAC ( BaseEstimator ) : \n from a subset of inliers from the complete data set . More information can \n be found in the general documentation of linear models . \n \n + A detailed description of the algorithm can be found in the documentation \n + of the ` ` linear_model ` ` sub - package . \n + \n Parameters \n mmmmmmmmm - \n base_estimator : object , optional", "msg": "Add reference to narrative documentation for detailed description of RANSAC\n"}
{"diff": "a / lib / matplotlib / axes / _axes . py \n  b / lib / matplotlib / axes / _axes . py \n def boxplot ( self , x , notch = False , sym = ' b + ' , vert = True , whis = 1 . 5 , \n Examples \n mmmmmm - - \n \n - . . plot : : examples / statistics / boxplot_demo . py \n + . . plot : : ~ / examples / statistics / boxplot_demo . py \n \" \" \" \n bxpstats = cbook . boxplot_stats ( x , whis = whis , bootstrap = bootstrap , \n labels = labels ) \n def bxp ( self , bxpstats , positions = None , widths = None , vert = True , \n Examples \n mmmmmm - - \n \n - . . plot : : examples / statistics / bxp_demo . py \n + . . plot : : ~ / examples / statistics / bxp_demo . py \n \" \" \" \n # lists of artists to be output \n whiskers = [ ]", "msg": "fixed references to demos in boxplot and bxp docstrings\n"}
{"diff": "a / setup . py \n  b / setup . py \n def _decimal_ext ( self ) : \n \n # Increase warning level for gcc : \n if ' gcc ' in cc : \n - cmd = ( \" echo ' ' | gcc - Wextra - Wno - missing - field - initializers - E - \" \n - \" > / dev / null 2 > & 1 \" ) \n + cmd = ( \" echo ' ' | % s - Wextra - Wno - missing - field - initializers - E - \" \n + \" > / dev / null 2 > & 1 \" % cc ) \n ret = os . system ( cmd ) \n if ret > > 8 = = 0 : \n extra_compile_args . extend ( [ ' - Wextra ' ,", "msg": "Issue : Use cc from sysconfig for testing flags .\n"}
{"diff": "a / libmproxy / utils . py \n  b / libmproxy / utils . py \n def __repr__ ( self ) : \n Returns a string containing a formatted header string . \n \" \" \" \n headerElements = [ ] \n - for key in self . keys ( ) : \n + for key in sorted ( self . keys ( ) ) : \n for val in self [ key ] : \n headerElements . append ( key + \" : \" + val ) \n headerElements . append ( \" \" )", "msg": "Sort header names for a predictable result\n"}
{"diff": "a / api / zulip / __init__ . py \n  b / api / zulip / __init__ . py \n def _register ( cls , name , url = None , make_request = None , \n if url is None : \n url = name \n if make_request is None : \n - make_request = lambda request = None : { } if request is None else request \n + def make_request ( request = None ) : \n + if request is None : \n + request = { } \n + return request \n def call ( self , * args , * * kwargs ) : \n request = make_request ( * args , * * kwargs ) \n if computed_url is not None :", "msg": "Changed make_request lambda to more readable function .\n"}
{"diff": "a / mmdet / apis / test . py \n  b / mmdet / apis / test . py \n def single_gpu_test ( model , \n result = bbox_results , encoded_mask_results \n results . append ( result ) \n \n - batch_size = data [ ' img ' ] [ 0 ] . size ( 0 ) \n + batch_size = len ( data [ ' img_metas ' ] [ 0 ] . data ) \n for _ in range ( batch_size ) : \n prog_bar . update ( ) \n return results \n def multi_gpu_test ( model , data_loader , tmpdir = None , gpu_collect = False ) : \n results . append ( result ) \n \n if rank = = 0 : \n - batch_size = ( \n - len ( data [ ' img_meta ' ] . _data ) \n - if ' img_meta ' in data else data [ ' img ' ] [ 0 ] . size ( 0 ) ) \n + batch_size = len ( data [ ' img_metas ' ] [ 0 ] . data ) \n for _ in range ( batch_size * world_size ) : \n prog_bar . update ( )", "msg": "Use img_metas to indicate batch ( )\n"}
{"diff": "a / test / runner / lib / ansible_util . py \n  b / test / runner / lib / ansible_util . py \n def ansible_environment ( args , color = True ) : \n ANSIBLE_FORCE_COLOR = ' % s ' % ' true ' if args . color and color else ' false ' , \n ANSIBLE_DEPRECATION_WARNINGS = ' false ' , \n ANSIBLE_HOST_KEY_CHECKING = ' false ' , \n + ANSIBLE_RETRY_FILES_ENABLED = ' false ' , \n ANSIBLE_CONFIG = os . path . abspath ( ansible_config ) , \n ANSIBLE_LIBRARY = ' / dev / null ' , \n PYTHONPATH = os . path . abspath ( ' lib ' ) ,", "msg": "Disable retry files for integration tests .\n"}
{"diff": "a / lib / matplotlib / widgets . py \n  b / lib / matplotlib / widgets . py \n def __init__ ( self , ax , labels , active = 0 , activecolor = ' blue ' ) : \n def _clicked ( self , event ) : \n if event . button ! = 1 : return \n if event . inaxes ! = self . ax : return \n - xy = self . ax . transAxes . inverse_xy_tup ( ( event . x , event . y ) ) \n + xy = self . ax . transAxes . inverted ( ) . transform_point ( ( event . x , event . y ) ) \n pclicked = np . array ( [ xy [ 0 ] , xy [ 1 ] ] ) \n def inside ( p ) : \n pcirc = np . array ( [ p . center [ 0 ] , p . center [ 1 ] ] )", "msg": "Update radio buttons to new transformation framework ( thanks Matthias Michler )\n"}
{"diff": "a / IPython / zmq / completer . py \n  b / IPython / zmq / completer . py \n \n from __future__ import print_function \n \n import itertools \n - import readline \n + try : \n + import readline \n + except ImportError : \n + readline = None \n import rlcompleter \n import time \n \n class ClientCompleter ( object ) : \n and then return them for each value of state . \" \" \" \n \n def __init__ ( self , client , session , socket ) : \n - # ugly , but we get called asynchronously and need access to some \n - # client state , like backgrounded code \n + # ugly , but we get called asynchronously and need access to some \n + # client state , like backgrounded code \n + assert readline is not None , \" ClientCompleter depends on readline \" \n self . client = client \n self . session = session \n self . socket = socket", "msg": "allow IPython . zmq . completer to be imported without readline\n"}
{"diff": "a / youtube_dl / extractor / common . py \n  b / youtube_dl / extractor / common . py \n def _twitter_search_player ( self , html ) : \n return self . _html_search_meta ( ' twitter : player ' , html , \n ' twitter card player ' ) \n \n - def _search_json_ld ( self , html , video_id , fatal = True ) : \n + def _search_json_ld ( self , html , video_id , * * kwargs ) : \n json_ld = self . _search_regex ( \n r ' ( ? s ) < script [ ^ > ] + type = ( [ \" \\ ' ] ) application / ld \\ + json \\ 1 [ ^ > ] * > ( ? P < json_ld > . + ? ) < / script > ' , \n - html , ' JSON - LD ' , fatal = fatal , group = ' json_ld ' ) \n + html , ' JSON - LD ' , group = ' json_ld ' , * * kwargs ) \n if not json_ld : \n return { } \n - return self . _json_ld ( json_ld , video_id , fatal = fatal ) \n + return self . _json_ld ( json_ld , video_id , fatal = kwargs . get ( ' fatal ' , True ) ) \n \n def _json_ld ( self , json_ld , video_id , fatal = True ) : \n if isinstance ( json_ld , compat_str ) :", "msg": "[ extractor / common ] Allow passing more parameters to _search_json_ld\n"}
{"diff": "a / homeassistant / __init__ . py \n  b / homeassistant / __init__ . py \n \n DOMAIN = \" homeassistant \" \n \n # How often time_changed event should fire \n - TIMER_INTERVAL = 10 # seconds \n + TIMER_INTERVAL = 3 # seconds \n \n # How long we wait for the result of a service call \n SERVICE_CALL_LIMIT = 10 # seconds", "msg": "Reduce the timer interval to make sensors useful\n"}
{"diff": "new file mode 100644 \n index 000000000000 . . 0729be43585b \n  / dev / null \n  b / zephyr / management / commands / reset_colors . py \n \n + from django . core . management . base import BaseCommand \n + from zephyr . models import StreamColor , UserProfile , Subscription , Recipient \n + \n + class Command ( BaseCommand ) : \n + help = \" \" \" Reset all colors for a person to the default grey \" \" \" \n + \n + def handle ( self , * args , * * options ) : \n + if not args : \n + self . print_help ( \" python manage . py \" , \" reset_colors \" ) \n + exit ( 1 ) \n + \n + for email in args : \n + user_profile = UserProfile . objects . get ( user__email__iexact = email ) \n + subs = Subscription . objects . filter ( user_profile = user_profile , \n + active = True , \n + recipient__type = Recipient . STREAM ) \n + \n + for sub in subs : \n + stream_color , _ = StreamColor . objects . get_or_create ( subscription = sub ) \n + stream_color . color = StreamColor . DEFAULT_STREAM_COLOR \n + stream_color . save ( )", "msg": "Add a management command to reset your stream colors to the default .\n"}
{"diff": "a / celery / execute / __init__ . py \n  b / celery / execute / __init__ . py \n def apply_async ( task , args = None , kwargs = None , countdown = None , eta = None , \n \n \" \" \" \n if conf . ALWAYS_EAGER : \n - return apply ( task , args , kwargs ) \n + return apply ( task , args , kwargs , task_id = task_id ) \n return _apply_async ( task , args = args , kwargs = kwargs , countdown = countdown , \n eta = eta , task_id = task_id , publisher = publisher , \n connection = connection , \n def apply ( task , args , kwargs , * * options ) : \n \" \" \" \n args = args or [ ] \n kwargs = kwargs or { } \n - task_id = gen_unique_id ( ) \n + task_id = options . get ( \" task_id \" , gen_unique_id ( ) ) \n retries = options . get ( \" retries \" , 0 ) \n \n task = tasks [ task . name ] # Make sure we get the instance , not class .", "msg": "Added task_id to apply ( ) call from apply_async . Made apply ( ) respect task_id argument if it exists .\n"}
{"diff": "a / scrapy / conf / __init__ . py \n  b / scrapy / conf / __init__ . py \n \n import os \n import cPickle as pickle \n \n + from scrapy . conf import default_settings \n + \n import_ = lambda x : __import__ ( x , { } , { } , [ ' ' ] ) \n \n class Settings ( object ) : \n \n - default_settings_module = ' scrapy . conf . default_settings ' \n - \n def __init__ ( self ) : \n self . defaults = { } \n - self . global_defaults = import_ ( self . default_settings_module ) \n + self . global_defaults = default_settings \n self . disabled = os . environ . get ( ' SCRAPY_SETTINGS_DISABLED ' , False ) \n settings_module_path = os . environ . get ( ' SCRAPY_SETTINGS_MODULE ' , \\ \n os . environ . get ( ' SCRAPYSETTINGS_MODULE ' , ' scrapy_settings ' ) )", "msg": "minor simplification to how default settings are loaded\n"}
{"diff": "a / numpy / lib / npyio . py \n  b / numpy / lib / npyio . py \n def save ( file , arr , allow_pickle = True , fix_imports = True ) : \n then the filename is unchanged . If file is a string or Path , a ` ` . npy ` ` \n extension will be appended to the file name if it does not already \n have one . \n + arr : array_like \n + Array data to be saved . \n allow_pickle : bool , optional \n Allow saving object arrays using Python pickles . Reasons for disallowing \n pickles include security ( loading pickled data can execute arbitrary \n def save ( file , arr , allow_pickle = True , fix_imports = True ) : \n pickled in a Python 2 compatible way . If ` fix_imports ` is True , pickle \n will try to map the new Python 3 names to the old module names used in \n Python 2 , so that the pickle data stream is readable with Python 2 . \n - arr : array_like \n - Array data to be saved . \n \n See Also \n mmmmmm - -", "msg": "Updates order of parameters in save docstring to match function parameter order\n"}
{"diff": "a / zerver / lib / push_notifications . py \n  b / zerver / lib / push_notifications . py \n def handle_push_notification ( user_profile_id : int , missed_message : Dict [ str , Any \n user_profile = get_user_profile_by_id ( user_profile_id ) \n ( message , user_message ) = access_message ( user_profile , missed_message [ ' message_id ' ] ) \n if user_message is not None : \n - # If ther user has read the message already , don ' t push - notify . \n + # If the user has read the message already , don ' t push - notify . \n # \n # TODO : It feels like this is already handled when things are \n # put in the queue ; maybe we should centralize this logic with", "msg": "push notifications : Fix a comment typo .\n"}
{"diff": "a / Lib / urlparse . py \n  b / Lib / urlparse . py \n \n # Characters valid in scheme names \n scheme_chars = string . letters + string . digits + ' + - . ' \n \n + MAX_CACHE_SIZE = 2000 \n _parse_cache = { } \n \n def clear_cache ( ) : \n + \" \" \" Clear the parse cache . \" \" \" \n global _parse_cache \n _parse_cache = { } \n \n def urlparse ( url , scheme = ' ' , allow_framents = 1 ) : \n key = url , scheme , allow_framents \n if _parse_cache . has_key ( key ) : \n return _parse_cache [ key ] \n + if len ( _parse_cache ) > = MAX_CACHE_SIZE : # avoid runaway growth \n + clear_cache ( ) \n netloc = path = params = query = fragment = ' ' \n i = string . find ( url , ' : ' ) \n if i > 0 :", "msg": "Crude but effective hack to clear the parser cache every so often .\n"}
{"diff": "a / rest_framework / filters . py \n  b / rest_framework / filters . py \n def filter_queryset ( self , request , queryset , view ) : \n \n def get_template_context ( self , request , queryset , view ) : \n current = self . get_ordering ( request , queryset , view ) \n - current = None if current is None else current [ 0 ] \n + current = None if not current else current [ 0 ] \n options = [ ] \n context = { \n ' request ' : request ,", "msg": "Enable OrderingFilter to handle an empty tuple ( or list ) for the ' ordering ' field . ( )\n"}
{"diff": "a / zerver / lib / import_realm . py \n  b / zerver / lib / import_realm . py \n def import_uploads_local ( import_dir : Path , records : List [ Dict [ str , Any ] ] , \n random_name ( 18 ) , \n sanitize_name ( os . path . basename ( record [ ' path ' ] ) ) \n ] ) \n - path_maps [ ' attachment_path ' ] [ record [ ' path ' ] ] = relative_path \n + path_maps [ ' attachment_path ' ] [ record [ ' s3_path ' ] ] = relative_path \n \n if processing_avatars or processing_emojis : \n file_path = os . path . join ( settings . LOCAL_UPLOADS_DIR , \" avatars \" , relative_path )", "msg": "import : Use the s3_path attribute for path_maps unconditionally .\n"}
{"diff": "a / Lib / test / test_support . py \n  b / Lib / test / test_support . py \n \n - # Python test set - - supporting definitions . \n + \" \" \" Supporting definitions for the Python regression test . \" \" \" \n + \n + \n + class Error ( Exception ) : \n + \" \" \" Base class for regression test exceptions . \" \" \" \n + \n + class TestFailed ( Error ) : \n + \" \" \" Test failed . \" \" \" \n + \n + class TestSkipped ( Error ) : \n + \" \" \" Test skipped . \n + \n + This can be raised to indicate that a test was deliberatly \n + skipped , but not because a feature wasn ' t available . For \n + example , if some resource can ' t be used , such as the network \n + appears to be unavailable , this should be raised instead of \n + TestFailed . \n + \n + \" \" \" \n \n - class TestFailed ( Exception ) : \n - pass \n \n verbose = 1 # Flag set to 0 by regrtest . py \n + use_large_resources = 1 # Flag set to 0 by regrtest . py \n \n def unload ( name ) : \n import sys", "msg": "Restore TestSkipped exception ; appears to have disappeared in last checkin .\n"}
{"diff": "a / homeassistant / components / logger . py \n  b / homeassistant / components / logger . py \n def setup ( hass , config = None ) : \n \n # Set log filter for all log handler \n for handler in logging . root . handlers : \n + handler . setLevel ( logging . NOTSET ) \n handler . addFilter ( HomeAssistantLogFilter ( logfilter ) ) \n \n return True", "msg": "Reset log handlers to lowest level .\n"}
{"diff": "a / locust / web . py \n  b / locust / web . py \n def request_stats ( ) : \n try : \n report [ \" fail_ratio \" ] = float ( stats [ len ( stats ) - 1 ] [ \" num_failures \" ] ) / stats [ len ( stats ) - 1 ] [ \" num_reqs \" ] \n except ZeroDivisionError : \n - report [ \" fail_ratio \" ] = 0 \n + if stats [ len ( stats ) - 1 ] [ \" num_failures \" ] > 0 : \n + report [ \" fail_ratio \" ] = 100 \n + else : \n + report [ \" fail_ratio \" ] = 0 \n _request_stats_context_cache = { \" time \" : time ( ) , \" report \" : report } \n else : \n report = _request_stats_context_cache [ \" report \" ]", "msg": "Small error ratio UI fix if all requests has failed .\n"}
{"diff": "a / celery / worker . py \n  b / celery / worker . py \n def fetch_next_task ( self ) : \n except Exception , error : \n self . logger . critical ( \" Worker got exception % s : % s \\ n % s \" % ( \n error . __class__ , error , traceback . format_exc ( ) ) ) \n - else : \n - message . ack ( ) \n + return \n \n + message . ack ( ) \n return result , task_name , task_id \n \n def run_periodic_tasks ( self ) : \n def run ( self ) : \n self . run_periodic_tasks ( ) \n try : \n result , task_name , task_id = self . fetch_next_task ( ) \n + except ValueError : \n + # fetch_next_task didn ' t return a r / name / id tuple , \n + # probably because it got an exception . \n + continue \n except EmptyQueue : \n if not last_empty_emit or \\ \n time . time ( ) > last_empty_emit + EMPTY_MSG_EMIT_EVERY :", "msg": "Handle the case where fetch_next_task ( ) returns None ( because of an exception )\n"}
{"diff": "a / bokeh / command / subcommand . py \n  b / bokeh / command / subcommand . py \n \n line application . \n \n ' ' ' \n + from abc import ABCMeta , abstractmethod \n \n + # TODO ( bev ) change this after bokeh . util . future is merged \n + from six import add_metaclass \n + \n + @ add_metaclass ( ABCMeta ) \n class Subcommand ( object ) : \n ' ' ' Abstract base class for subcommands ' ' ' \n \n def __init__ ( self , parser ) : \n ' ' ' \n self . parser = parser \n \n - def func ( self , args ) : \n + @ abstractmethod \n + def invoke ( self , args ) : \n ' ' ' Takes over main program flow to perform the subcommand . \n \n Args : \n args ( seq ) : command line arguments for the subcommand to parse \n \n ' ' ' \n - raise NotImplementedError ( \" Implement func ( args ) \" ) \n \\ No newline at end of file \n + pass \n \\ No newline at end of file", "msg": "use abc module to make Subcommand a real abstract base class\n"}
{"diff": "a / scrapy / contrib / spiderstate . py \n  b / scrapy / contrib / spiderstate . py \n \n from six . moves import cPickle as pickle \n \n from scrapy import signals \n + from scrapy . utils . job import job_dir \n \n class SpiderState ( object ) : \n \" \" \" Store and load spider state during a scraping job \" \" \" \n def __init__ ( self , jobdir = None ) : \n \n @ classmethod \n def from_crawler ( cls , crawler ) : \n - obj = cls ( crawler . settings . get ( ' JOBDIR ' ) ) \n + obj = cls ( job_dir ( crawler . settings ) ) \n crawler . signals . connect ( obj . spider_closed , signal = signals . spider_closed ) \n crawler . signals . connect ( obj . spider_opened , signal = signals . spider_opened ) \n return obj", "msg": "For consistency , use ` job_dir ` helper in ` SpiderState ` extension .\n"}
{"diff": "a / lib / ansible / executor / task_queue_manager . py \n  b / lib / ansible / executor / task_queue_manager . py \n def _load_callbacks ( self , stdout_callback ) : \n if callback_name ! = stdout_callback or stdout_callback_loaded : \n continue \n stdout_callback_loaded = True \n - elif C . DEFAULT_CALLBACK_WHITELIST is not None and callback_name not in C . DEFAULT_CALLBACK_WHITELIST : \n + elif C . DEFAULT_CALLBACK_WHITELIST is None or callback_name not in C . DEFAULT_CALLBACK_WHITELIST : \n continue \n \n loaded_plugins . append ( callback_plugin ( self . _display ) )", "msg": "fixed condition for loading whitelisted callbacks\n"}
{"diff": "a / lib / ansible / plugins / filter / core . py \n  b / lib / ansible / plugins / filter / core . py \n def version_compare ( value , version , operator = ' eq ' , strict = False ) : \n except Exception , e : \n raise errors . AnsibleFilterError ( ' Version comparison : % s ' % e ) \n \n - def re_escape ( string ) : \n + def regex_escape ( string ) : \n ' ' ' Escape all regular expressions special characters from STRING . ' ' ' \n return re . escape ( string ) \n \n def filters ( self ) : \n ' search ' : search , \n ' regex ' : regex , \n ' regex_replace ' : regex_replace , \n - ' re_escape ' : re_escape , \n + ' regex_escape ' : regex_escape , \n \n # ? : ; \n ' ternary ' : ternary ,", "msg": "Change name from re_escape to regex_escape to fit existing function names .\n"}
{"diff": "a / src / transformers / data / processors / glue . py \n  b / src / transformers / data / processors / glue . py \n def glue_convert_examples_to_features ( \n features = [ ] \n for ( ex_index , example ) in enumerate ( examples ) : \n if ex_index % 10000 = = 0 : \n - logger . info ( \" Writing example % d \" % ( ex_index ) ) \n + logger . info ( \" Writing example % d / % d \" % ( ex_index , len ( examples ) ) ) \n if is_tf_dataset : \n example = processor . get_example_from_tensor_dict ( example ) \n example = processor . tfds_map ( example )", "msg": "Improve logging message in glue feature conversion\n"}
{"diff": "a / zerver / lib / actions . py \n  b / zerver / lib / actions . py \n def bulk_get_subscriber_emails ( streams , user_profile ) : \n recipient__type = Recipient . STREAM , \n recipient__type_id__in = [ stream . id for stream in target_streams ] , \n user_profile__is_active = True , \n - active = True ) . only ( \" user_profile__email \" , \" recipient__type_id \" ) \n + active = True ) . values ( \" user_profile__email \" , \" recipient__type_id \" ) \n \n result = dict ( ( stream . id , [ ] ) for stream in streams ) \n for sub in subscriptions : \n - result [ sub . recipient . type_id ] . append ( sub . user_profile . email ) \n + result [ sub [ \" recipient__type_id \" ] ] . append ( sub [ \" user_profile__email \" ] ) \n \n return result", "msg": "bulk_get_subscriber_emails : Use . values ( ) to substantially improve performance .\n"}
{"diff": "a / scrapy / commands / crawl . py \n  b / scrapy / commands / crawl . py \n def process_options ( self , args , opts ) : \n def run ( self , args , opts ) : \n if len ( args ) < 1 : \n raise UsageError ( ) \n + elif len ( args ) > 1 : \n + raise UsageError ( \" running ' scrapy crawl ' with more than one spider is no longer supported \" ) \n for spname in args : \n spider = self . crawler . spiders . create ( spname , * * opts . spargs ) \n self . crawler . crawl ( spider )", "msg": "removed support for passing more than a single spider on ' scrapy crawl ' command\n"}
{"diff": "a / lib / ansible / modules / cloud / amazon / rds_instance . py \n  b / lib / ansible / modules / cloud / amazon / rds_instance . py \n def main ( ) : \n new_db_instance_identifier = dict ( aliases = [ ' new_instance_id ' , ' new_id ' ] ) , \n option_group_name = dict ( ) , \n performance_insights_kms_key_id = dict ( ) , \n - performance_insights_retention_period = dict ( ) , \n + performance_insights_retention_period = dict ( type = ' int ' ) , \n port = dict ( type = ' int ' ) , \n preferred_backup_window = dict ( aliases = [ ' backup_window ' ] ) , \n preferred_maintenance_window = dict ( aliases = [ ' maintenance_window ' ] ) ,", "msg": "Add variable type for performance_insights_retention_period ( )\n"}
{"diff": "a / spacy / _ml . py \n  b / spacy / _ml . py \n def _zero_init_impl ( self , X , y ) : \n def _preprocess_doc ( docs , drop = 0 . ) : \n keys = [ doc . to_array ( [ LOWER ] ) for doc in docs ] \n ops = Model . ops \n - lengths = ops . asarray ( [ arr . shape [ 0 ] for arr in keys ] ) \n + lengths = ops . asarray ( [ arr . shape [ 0 ] for arr in keys ] , dtype = ' int32 ' ) \n keys = ops . xp . concatenate ( keys ) \n vals = ops . allocate ( keys . shape [ 0 ] ) + 1 \n return ( keys , vals , lengths ) , None", "msg": "Make a dtype more specific , to fix a windows build\n"}
{"diff": "a / lib / matplotlib / backends / backend_agg . py \n  b / lib / matplotlib / backends / backend_agg . py \n def print_png ( self , filename , * args , * * kwargs ) : \n renderer = self . get_renderer ( ) \n original_dpi = renderer . dpi \n renderer . dpi = self . figure . dpi \n + filename = str ( filename ) # until we figure out unicode handling \n renderer . _renderer . write_png ( filename , self . figure . dpi ) \n renderer . dpi = original_dpi", "msg": "forced nonunicode fname for save in agg\n"}
{"diff": "a / scrapy / core / engine . py \n  b / scrapy / core / engine . py \n def close_spider ( self , spider , reason = ' cancelled ' ) : \n \n dfd = slot . close ( ) \n \n - dfd . addBoth ( lambda _ : self . scheduler . close_spider ( spider ) ) \n - dfd . addErrback ( log . err , spider = spider ) \n - \n dfd . addBoth ( lambda _ : self . downloader . close_spider ( spider ) ) \n dfd . addErrback ( log . err , spider = spider ) \n \n dfd . addBoth ( lambda _ : self . scraper . close_spider ( spider ) ) \n dfd . addErrback ( log . err , spider = spider ) \n \n + dfd . addBoth ( lambda _ : self . scheduler . close_spider ( spider ) ) \n + dfd . addErrback ( log . err , spider = spider ) \n + \n dfd . addBoth ( lambda _ : self . _cancel_next_call ( spider ) ) \n dfd . addErrback ( log . err , spider = spider )", "msg": "Close the scheduler after closing the scraper and downloader . This shouldn ' t have any real effect in practice , but it feels more appropiate to close the components in this order\n"}
{"diff": "a / Tools / msi / merge . py \n  b / Tools / msi / merge . py \n def merge ( msi , feature , rootdir , modules ) : \n msilib . add_stream ( db , stream , cabname ) \n os . unlink ( cabname ) \n maxmedia + = count \n + # The merge module sets ALLUSERS to 1 in the property table . \n + # This is undesired ; delete that \n + v = db . OpenView ( \" DELETE FROM Property WHERE Property = ' ALLUSERS ' \" ) \n + v . Execute ( None ) \n + v . Close ( ) \n db . Commit ( ) \n \n merge ( msi , \" SharedCRT \" , \" TARGETDIR \" , modules )", "msg": "Delete ALLUSERS property merged from CRT merge module , so that per - user installations become possible again .\n"}
{"diff": "a / homeassistant / components / deconz / device_trigger . py \n  b / homeassistant / components / deconz / device_trigger . py \n \n ( CONF_SHAKE , \" \" ) : 1007 , \n } \n \n + AQARA_SQUARE_SWITCH_WXKG11LM_2016_MODEL = \" lumi . sensor_switch . aq2 \" \n + AQARA_SQUARE_SWITCH_WXKG11LM_2016 = { \n + ( CONF_SHORT_PRESS , CONF_TURN_ON ) : 1002 , \n + ( CONF_DOUBLE_PRESS , CONF_TURN_ON ) : 1004 , \n + ( CONF_TRIPLE_PRESS , CONF_TURN_ON ) : 1005 , \n + ( CONF_QUADRUPLE_PRESS , CONF_TURN_ON ) : 1006 , \n + } \n + \n REMOTES = { \n HUE_DIMMER_REMOTE_MODEL_GEN1 : HUE_DIMMER_REMOTE , \n HUE_DIMMER_REMOTE_MODEL_GEN2 : HUE_DIMMER_REMOTE , \n \n AQARA_MINI_SWITCH_MODEL : AQARA_MINI_SWITCH , \n AQARA_ROUND_SWITCH_MODEL : AQARA_ROUND_SWITCH , \n AQARA_SQUARE_SWITCH_MODEL : AQARA_SQUARE_SWITCH , \n + AQARA_SQUARE_SWITCH_WXKG11LM_2016_MODEL : AQARA_SQUARE_SWITCH_WXKG11LM_2016 , \n } \n \n TRIGGER_SCHEMA = TRIGGER_BASE_SCHEMA . extend (", "msg": "Add device trigger support for Aqara WXKG11LM 2016 switch to Deconz ( )\n"}
{"diff": "a / lib / ansible / modules / network / junos / junos_user . py \n  b / lib / ansible / modules / network / junos / junos_user . py \n def main ( ) : \n name = dict ( ) , \n full_name = dict ( ) , \n role = dict ( choices = ROLES ) , \n - encrypted_password = dict ( ) , \n + encrypted_password = dict ( no_log = True ) , \n sshkey = dict ( ) , \n state = dict ( choices = [ ' present ' , ' absent ' ] , default = ' present ' ) , \n active = dict ( type = ' bool ' , default = True )", "msg": "Set no_log to True for junos_user encrypted_password ( )\n"}
{"diff": "a / lib / ansible / runner / __init__ . py \n  b / lib / ansible / runner / __init__ . py \n def _executor_internal_inner ( self , host , module_name , module_args , inject , port , \n actual_port = delegate_info . get ( ' ansible_ssh_port ' , port ) \n actual_user = delegate_info . get ( ' ansible_ssh_user ' , actual_user ) \n actual_pass = delegate_info . get ( ' ansible_ssh_pass ' , actual_pass ) \n + actual_transport = delegate_info . get ( ' ansible_connection ' , self . transport ) \n for i in delegate_info : \n if i . startswith ( \" ansible_ \" ) and i . endswith ( \" _interpreter \" ) : \n inject [ i ] = delegate_info [ i ]", "msg": "Handle delegate_to case for local connections in hosts file\n"}
{"diff": "a / scripts / Identify_Old_Issue_And_PR . py \n  b / scripts / Identify_Old_Issue_And_PR . py \n def is_old_pull_request ( issue ) : \n print ( \" [ + ] State : \" ) \n return_code = len ( old_issues [ \" PR \" ] ) + len ( old_issues [ \" ISSUE \" ] ) \n if len ( old_issues [ \" PR \" ] ) > 0 : \n - print ( \" \\ tOld pull request identified : % s \" % old_issues [ \" PR \" ] ) \n - else : \n - print ( \" \\ tAll PR are OK . \" ) \n + print ( \" Old pull request identified : % s \" % old_issues [ \" PR \" ] ) \n if len ( old_issues [ \" ISSUE \" ] ) > 0 : \n - print ( \" \\ tOld issue identified : % s \" % old_issues [ \" ISSUE \" ] ) \n - else : \n - print ( \" \\ tAll issues are OK . \" ) \n + print ( \" Old issue identified : % s \" % old_issues [ \" ISSUE \" ] ) \n + if return_code = = 0 : \n + print ( \" Nothing identified . \" ) \n sys . exit ( return_code ) \n \\ No newline at end of file", "msg": "Add a script to identify any old Issue or PR .\n"}
{"diff": "a / youtube_dl / YoutubeDL . py \n  b / youtube_dl / YoutubeDL . py \n def process_video_result ( self , info_dict , download = True ) : \n if thumbnails is None : \n thumbnail = info_dict . get ( ' thumbnail ' ) \n if thumbnail : \n - thumbnails = [ { ' url ' : thumbnail } ] \n + info_dict [ ' thumbnails ' ] = thumbnails = [ { ' url ' : thumbnail } ] \n if thumbnails : \n thumbnails . sort ( key = lambda t : ( \n t . get ( ' preference ' ) , t . get ( ' width ' ) , t . get ( ' height ' ) ,", "msg": "[ YoutubeDL ] set the ' thumbnails ' field if the info_dict has the ' thumbnails ' field\n"}
{"diff": "a / sklearn / cluster / dbscan_ . py \n  b / sklearn / cluster / dbscan_ . py \n \n # \n # License : BSD \n \n + import warnings \n import numpy as np \n \n from . . base import BaseEstimator \n def fit ( self , X , * * params ) : \n params : dict \n Overwrite keywords from __init__ . \n \" \" \" \n - \n - self . set_params ( * * params ) \n + if params : \n + warnings . warn ( ' Passing parameters to fit methods is ' \n + ' depreciated ' , stacklevel = 2 ) \n + self . set_params ( * * params ) \n self . core_sample_indices_ , self . labels_ = dbscan ( X , \n * * self . get_params ( ) ) \n self . components_ = X [ self . core_sample_indices_ ] . copy ( )", "msg": "Warn : Passing params to fit is depreciated\n"}
{"diff": "a / lib / ansible / plugins / connection / chroot . py \n  b / lib / ansible / plugins / connection / chroot . py \n def __init__ ( self , play_context , new_stdin , * args , * * kwargs ) : \n raise AnsibleError ( \" % s is not a directory \" % self . chroot ) \n \n chrootsh = os . path . join ( self . chroot , ' bin / sh ' ) \n - if not is_executable ( chrootsh ) : \n + # Want to check for a usable bourne shell inside the chroot . \n + # is_executable ( ) = = True is sufficient . For symlinks it \n + # gets really complicated really fast . So we punt on finding that \n + # out . As long as it ' s a symlink we assume that it will work \n + if not ( is_executable ( chrootsh ) or ( os . path . lexists ( chrootsh ) and os . path . islink ( chrootsh ) ) ) : \n raise AnsibleError ( \" % s does not look like a chrootable dir ( / bin / sh missing ) \" % self . chroot ) \n \n self . chroot_cmd = distutils . spawn . find_executable ( ' chroot ' )", "msg": "Be more lenient of symlinked / bin / sh inside the chroot ( )\n"}
{"diff": "a / scene / scene . py \n  b / scene / scene . py \n def setup ( self ) : \n \" \" \" \n pass \n \n + def setup_bases ( self ) : \n + for base in self . __class__ . __bases__ : \n + base . setup ( self ) \n + \n def construct ( self ) : \n pass # To be implemented in subclasses", "msg": "setup_bases for scenes that subclass multiple other scenes\n"}
{"diff": "a / scrapy / contrib / ibl / extraction / pageparsing . py \n  b / scrapy / contrib / ibl / extraction / pageparsing . py \n def _close_unpaired_tag ( self ) : \n def _handle_unpaired_tag ( self , html_tag ) : \n if self . _read_bool_template_attribute ( html_tag , \" ignore \" ) and html_tag . tag = = \" img \" : \n self . ignored_regions . append ( ( self . next_tag_index , self . next_tag_index + 1 ) ) \n - elif self . _read_bool_template_attribute ( html_tag , \" ignore - beneath \" ) and html_tag . tag = = \" img \" : \n + elif self . _read_bool_template_attribute ( html_tag , \" ignore - beneath \" ) : \n self . ignored_regions . append ( ( self . next_tag_index , None ) ) \n jannotation = self . _read_template_annotation ( html_tag ) \n if jannotation :", "msg": "Remove restriction of marking ignore - beneath only for img unpaired tags\n"}
{"diff": "a / sklearn / linear_model / logistic . py \n  b / sklearn / linear_model / logistic . py \n class LogisticRegression ( BaseEstimator , LinearClassifierMixin , \n \" \" \" Logistic Regression ( aka logit , MaxEnt ) classifier . \n \n In the multiclass case , the training algorithm uses the one - vs - rest ( OvR ) \n - scheme if the ' multi_class ' option is set to ' ovr ' and uses the cross - \n - entropy loss , if the ' multi_class ' option is set to ' multinomial ' . \n + scheme if the ' multi_class ' option is set to ' ovr ' , and uses the cross - \n + entropy loss if the ' multi_class ' option is set to ' multinomial ' . \n ( Currently the ' multinomial ' option is supported only by the ' lbfgs ' , \n ' sag ' and ' newton - cg ' solvers . )", "msg": "[ DOC ] Moves comma to make sentence more clear\n"}
{"diff": "a / setup . py \n  b / setup . py \n \n if os . path . exists ( ' MANIFEST ' ) : os . remove ( ' MANIFEST ' ) \n \n try : \n - from setuptools . command import bdist_egg \n - # from setuptools import setup # use setuptools if possible \n + # check if we have a reasonably recent copy of setuptools \n + from setuptools . command import bdist_egg \n has_setuptools = True \n except ImportError : \n from distutils . core import setup \n has_setuptools = False \n + \n + if has_setuptools : \n + from setuptools import setup \n \n import sys , os \n import glob", "msg": "fixed ' python setup . py foo ' if setuptools installed\n"}
{"diff": "a / lib / ansible / runner / lookup_plugins / first_found . py \n  b / lib / ansible / runner / lookup_plugins / first_found . py \n def run ( self , terms , inject = None , * * kwargs ) : \n else : \n total_search = terms \n \n - result = None \n for fn in total_search : \n + if inject and ' _original_file ' in inject : \n + # check the templates and vars directories too , \n + # if they exist \n + for roledir in ( ' templates ' , ' vars ' ) : \n + path = utils . path_dwim ( os . path . join ( self . basedir , ' . . ' , roledir ) , fn ) \n + if os . path . exists ( path ) : \n + return [ path ] \n + # if none of the above were found , just check the \n + # current filename against the basedir ( this will already \n + # have . . / files from runner , if it ' s a role task \n path = utils . path_dwim ( self . basedir , fn ) \n if os . path . exists ( path ) : \n return [ path ] \n - \n - \n - if not result : \n + else : \n if skip : \n return [ ] \n else :", "msg": "Allow with_first_files to search relative to templates and vars in roles\n"}
{"diff": "a / Lib / doctest . py \n  b / Lib / doctest . py \n def _from_module ( self , module , object ) : \n elif inspect . isfunction ( object ) : \n return module . __dict__ is object . __globals__ \n elif inspect . ismethoddescriptor ( object ) : \n - return module . __name__ = = object . __objclass__ . __module__ \n + if hasattr ( object , ' __objclass__ ' ) : \n + obj_mod = object . __objclass__ . __module__ \n + elif hasattr ( object , ' __module__ ' ) : \n + obj_mod = object . __module__ \n + else : \n + return True # [ XX ] no easy way to tell otherwise \n + return module . __name__ = = obj_mod \n elif inspect . isclass ( object ) : \n return module . __name__ = = object . __module__ \n elif hasattr ( object , ' __module__ ' ) :", "msg": "Issue : Provide a couple of fallbacks for in case a method_descriptor\n"}
{"diff": "a / celery / contrib / rdb . py \n  b / celery / contrib / rdb . py \n def add ( x , y ) : \n return result \n \n \n + * * Environment Variables * * \n + \n + . . envvar : : CELERY_RDB_HOST \n + \n + Hostname to bind to . Default is ' 127 . 0 . 01 ' , which means the socket \n + will only be accessible from the local host . \n + \n + . . envvar : : CELERY_RDB_PORt \n + \n + Base port to bind to . Default is 6899 . \n + The debugger will try to find an available port starting from the \n + base port . The selected port will be logged by celeryd . \n \n \" \" \" \n import bdb \n def add ( x , y ) : \n \n default_port = 6899 \n \n - CELERY_RDB_HOST = os . environ . get ( \" CELERY_RDB_HOST \" ) or socket . gethostname ( ) \n + CELERY_RDB_HOST = os . environ . get ( \" CELERY_RDB_HOST \" ) or \" 127 . 0 . 0 . 1 \" \n CELERY_RDB_PORT = int ( os . environ . get ( \" CELERY_RDB_PORT \" ) or default_port ) \n \n # : Holds the currently active debugger .", "msg": "Remote Debugger : Only bind to localhost by default ( not accessible by other hosts )\n"}
{"diff": "a / zproject / backends . py \n  b / zproject / backends . py \n def has_module_perms ( self , user , app_label ) : \n return False \n \n def get_all_permissions ( self , user , obj = None ) : \n - # type : ( Optional [ UserProfile ] , Any ) - > Set \n + # type : ( Optional [ UserProfile ] , Any ) - > Set [ Any ] \n # Using Any type is safe because we are not doing anything with \n - # the arguments . \n + # the arguments and always return empty set . \n return set ( ) \n \n def get_group_permissions ( self , user , obj = None ) : \n - # type : ( Optional [ UserProfile ] , Any ) - > Set \n + # type : ( Optional [ UserProfile ] , Any ) - > Set [ Any ] \n # Using Any type is safe because we are not doing anything with \n - # the arguments . \n + # the arguments and always return empty set . \n return set ( ) \n \n def django_to_ldap_username ( self , username ) :", "msg": "mypy : Explicitly return Set [ Any ] for empty set in backends . py .\n"}
{"diff": "a / lib / ansible / runner / __init__ . py \n  b / lib / ansible / runner / __init__ . py \n def __init__ ( self , \n \n euid = pwd . getpwuid ( os . geteuid ( ) ) [ 0 ] \n if self . transport = = ' local ' and self . remote_user ! = euid : \n - raise Exception ( \" User mismatch : expected % s , but is % s \" % ( self . remote_user , euid ) ) \n + raise errors . AnsibleError ( \" User mismatch : expected % s , but is % s \" % ( self . remote_user , euid ) ) \n if type ( self . module_args ) not in [ str , unicode , dict ] : \n - raise Exception ( \" module_args must be a string or dict : % s \" % self . module_args ) \n + raise errors . AnsibleError ( \" module_args must be a string or dict : % s \" % self . module_args ) \n + if self . transport = = ' ssh ' and self . remote_pass : \n + raise errors . AnsibleError ( \" SSH transport does not support remote passwords , only keys or agents \" ) \n \n self . _tmp_paths = { } \n random . seed ( )", "msg": "make Runner options conflict errors raise AnsibleErrors not traceback in general\n"}
{"diff": "a / Lib / test / test_venv . py \n  b / Lib / test / test_venv . py \n def setUp ( self ) : \n self . ps3name = ' pysetup3 - script . py ' \n self . lib = ( ' Lib ' , ) \n self . include = ' Include ' \n - self . exe = ' python . exe ' \n else : \n self . bindir = ' bin ' \n self . ps3name = ' pysetup3 ' \n self . lib = ( ' lib ' , ' python % s ' % sys . version [ : 3 ] ) \n self . include = ' include ' \n - self . exe = ' python ' \n + self . exe = os . path . split ( sys . executable ) [ - 1 ] \n \n def tearDown ( self ) : \n shutil . rmtree ( self . env_dir )", "msg": "Changed executable name computation in test_venv to allow for debug executables .\n"}
{"diff": "a / lib / ansible / executor / play_iterator . py \n  b / lib / ansible / executor / play_iterator . py \n def __init__ ( self , inventory , play , play_context , variable_manager , all_vars , st \n self . _play . handlers . extend ( play . compile_roles_handlers ( ) ) \n \n def get_host_state ( self , host ) : \n - try : \n - return self . _host_states [ host . name ] . copy ( ) \n - except KeyError : \n - raise AnsibleError ( \" invalid host ( % s ) specified for playbook iteration \" % host ) \n + # Since we ' re using the PlayIterator to carry forward failed hosts , \n + # in the event that a previous host was not in the current inventory \n + # we create a stub state for it now \n + if host . name not in self . _host_states : \n + self . _host_states [ host . name ] = HostState ( blocks = [ ] ) \n + \n + return self . _host_states [ host . name ] . copy ( ) \n \n def get_next_task_for_host ( self , host , peek = False ) :", "msg": "Create state in PlayIterator for unknown hosts rather than raise errors\n"}
{"diff": "a / Lib / re . py \n  b / Lib / re . py \n \n import sys \n import sre_compile \n import sre_parse \n - import _locale \n + try : \n + import _locale \n + except ImportError : \n + _locale = None \n \n # public symbols \n __all__ = [ \n def _compile ( pattern , flags ) : \n if len ( _cache ) > = _MAXCACHE : \n _cache . clear ( ) \n if p . flags & LOCALE : \n + if not _locale : \n + return p \n loc = _locale . setlocale ( _locale . LC_CTYPE ) \n else : \n loc = None", "msg": "Fixed compile error in issue . The _locale module is optional .\n"}
{"diff": "a / IPython / parallel / client / client . py \n  b / IPython / parallel / client / client . py \n def __init__ ( self , url_or_file = None , profile = None , profile_dir = None , ipython_dir \n if location is not None : \n proto , addr , port = util . split_url ( url ) \n if addr = = ' 127 . 0 . 0 . 1 ' and location not in LOCAL_IPS and not sshserver : \n - sshserver = location \n - warnings . warn ( \n - \" Controller appears to be listening on localhost , but is not local . \" \n - \" IPython will try to use SSH tunnels to % s \" % location , \n - RuntimeWarning ) \n + warnings . warn ( \" \" \" \n + Controller appears to be listening on localhost , but not on this machine . \n + If this is true , you should specify Client ( . . . , sshserver = ' you @ % s ' ) \n + or instruct your controller to listen on an external IP . \" \" \" % location , \n + RuntimeWarning ) \n \n self . _config = cfg", "msg": "better warning on non - local controller without ssh\n"}
{"diff": "a / numpy / core / tests / test_regression . py \n  b / numpy / core / tests / test_regression . py \n def test_to_ctypes ( self ) : \n assert arr . size * arr . itemsize > 2 * * 31 \n c_arr = np . ctypeslib . as_ctypes ( arr ) \n assert_equal ( c_arr . _length_ , arr . size ) \n + \n + def test_complex_conversion_error ( self ) : \n + # gh - 17068 \n + with pytest . raises ( TypeError , match = r \" Unable to convert dtype . * \" ) : \n + complex ( np . array ( \" now \" , np . datetime64 ) ) \n + \n + def test__array_interface__descr ( self ) : \n + # gh - 17068 \n + dt = np . dtype ( dict ( names = [ ' a ' , ' b ' ] , \n + offsets = [ 0 , 0 ] , \n + formats = [ np . int64 , np . int64 ] ) ) \n + descr = np . array ( ( 1 , 1 ) , dtype = dt ) . __array_interface__ [ ' descr ' ] \n + assert descr = = [ ( ' ' , ' | V8 ' ) ] # instead of [ ( b ' ' , ' | V8 ' ) ]", "msg": "TST : Add tests for bugs fixed in gh - 17068 .\n"}
{"diff": "a / Lib / distutils / command / bdist_wininst . py \n  b / Lib / distutils / command / bdist_wininst . py \n \n __revision__ = \" $ Id $ \" \n \n import sys , os , string \n + import base64 \n from distutils . core import Command \n from distutils . util import get_platform \n from distutils . dir_util import create_tree , remove_tree \n def create_exe ( self , arcname , fullname , bitmap = None ) : \n # create_exe ( ) \n \n def get_exe_bytes ( self ) : \n - import base64 \n return base64 . decodestring ( EXEDATA ) \n # class bdist_wininst \n \n def get_exe_bytes ( self ) : \n # - Built wininst . exe from the MSVC project file distutils / misc / wininst . dsw \n # - Execute this file ( distutils / distutils / command / bdist_wininst . py ) \n \n - import re , base64 \n + import re \n moddata = open ( \" bdist_wininst . py \" , \" r \" ) . read ( ) \n exedata = open ( \" . . / . . / misc / wininst . exe \" , \" rb \" ) . read ( ) \n print \" wininst . exe length is % d bytes \" % len ( exedata )", "msg": "import base64 at the top to avoid two different imports at other times\n"}
{"diff": "a / lib / ansible / utils / __init__ . py \n  b / lib / ansible / utils / __init__ . py \n def _load_vars_from_path ( path , results , vault_password = None ) : \n # regular file \n elif stat . S_ISREG ( pathstat . st_mode ) : \n data = parse_yaml_from_file ( path , vault_password = vault_password ) \n - if type ( data ) ! = dict : \n + if data and type ( data ) ! = dict : \n raise errors . AnsibleError ( \n \" % s must be stored as a dictionary / hash \" % path ) \n + elif data is None : \n + data = { } \n \n # combine vars overrides by default but can be configured to do a \n # hash merge in settings", "msg": "Re - fixing ability to have empty json files after inventory refactoring\n"}
{"diff": "a / setup . py \n  b / setup . py \n \n package_dir = { ' ansible ' : ' lib / ansible ' } , \n packages = find_packages ( ' lib ' ) , \n package_data = { \n - ' ' : [ ' module_utils / * . ps1 ' , ' modules / core / windows / * . ps1 ' ] , \n + ' ' : [ ' module_utils / * . ps1 ' , ' modules / core / windows / * . ps1 ' , ' modules / extras / windows / * . ps1 ' ] , \n } , \n scripts = [ \n ' bin / ansible ' ,", "msg": "Need to include extras in setup to accomodate future windows extras modules\n"}
{"diff": "a / lib / ansible / modules / source_control / hg . py \n  b / lib / ansible / modules / source_control / hg . py \n \n aliases : [ version ] \n force : \n description : \n - - Discards uncommitted changes . Runs C ( hg update - C ) . \n + - Discards uncommitted changes . Runs C ( hg update - C ) . Prior to \n + 1 . 9 , the default was ` yes ` . \n required : false \n - default : \" yes \" \n + default : \" no \" \n choices : [ \" yes \" , \" no \" ] \n purge : \n description : \n def main ( ) : \n repo = dict ( required = True , aliases = [ ' name ' ] ) , \n dest = dict ( required = True ) , \n revision = dict ( default = None , aliases = [ ' version ' ] ) , \n - force = dict ( default = ' yes ' , type = ' bool ' ) , \n + force = dict ( default = ' no ' , type = ' bool ' ) , \n purge = dict ( default = ' no ' , type = ' bool ' ) , \n executable = dict ( default = None ) , \n ) ,", "msg": "Reverse the force parameter for the hg module\n"}
{"diff": "a / Lib / bsddb / test / test_join . py \n  b / Lib / bsddb / test / test_join . py \n def test01_join ( self ) : \n try : \n # lets look up all of the red Products \n sCursor = secDB . cursor ( ) \n - assert sCursor . set ( ' red ' ) \n + # Don ' t do the . set ( ) in an assert , or you can get a bogus failure \n + # when running python - O \n + tmp = sCursor . set ( ' red ' ) \n + assert tmp \n \n # FIXME : jCursor doesn ' t properly hold a reference to its \n # cursors , if they are closed before jcursor is used it", "msg": "test01_join ( ) : Fix a test failure when run with \" python - O \" . The\n"}
{"diff": "a / lib / ansible / plugins / connection / ssh . py \n  b / lib / ansible / plugins / connection / ssh . py \n def _run ( self , cmd , in_data , sudoable = True ) : \n state + = 1 \n \n while True : \n - rfd , wfd , efd = select . select ( rpipes , [ ] , rpipes , timeout ) \n + rfd , wfd , efd = select . select ( rpipes , [ ] , rpipes , 0 . 1 ) \n \n # We pay attention to timeouts only while negotiating a prompt .", "msg": "Don ' t use the connection timeout for the select poll timeout\n"}
{"diff": "a / zerver / management / commands / export_single_user . py \n  b / zerver / management / commands / export_single_user . py \n class Command ( BaseCommand ) : \n \" \" \" \n \n def add_arguments ( self , parser ) : \n - parser . add_argument ( ' email ' , metavar = ' < realm > ' , type = str , \n + parser . add_argument ( ' email ' , metavar = ' < email > ' , type = str , \n help = \" email of user to export \" ) \n parser . add_argument ( ' - - output ' , \n dest = ' output_dir ' ,", "msg": "export : Fix usage in export_single_user command .\n"}
{"diff": "a / Lib / ConfigParser . py \n  b / Lib / ConfigParser . py \n def remove_option ( self , section , option ) : \n sectdict = self . __sections [ section ] \n except KeyError : \n raise NoSectionError ( section ) \n - existed = sectdict . has_key ( key ) \n + existed = sectdict . has_key ( option ) \n if existed : \n - del sectdict [ key ] \n + del sectdict [ option ] \n return existed \n \n def remove_section ( self , section ) :", "msg": "remove_option ( ) : Use the right variable name for the option name !\n"}
{"diff": "a / manimlib / extract_scene . py \n  b / manimlib / extract_scene . py \n def get_scene_classes ( scene_names_to_classes , config ) : \n if config [ \" scene_name \" ] in scene_names_to_classes : \n return [ scene_names_to_classes [ config [ \" scene_name \" ] ] ] \n if config [ \" scene_name \" ] ! = \" \" : \n - print ( manimlib . constants . SCENE_NOT_FOUND_MESSAGE ) \n - return [ ] \n + print ( manimlib . constants . SCENE_NOT_FOUND_MESSAGE , file = sys . stderr ) \n + sys . exit ( 2 ) \n if config [ \" write_all \" ] : \n return list ( scene_names_to_classes . values ( ) ) \n return prompt_user_for_choice ( scene_names_to_classes )", "msg": "print to stderr for scene not found\n"}
{"diff": "a / lib / ansible / runner / __init__ . py \n  b / lib / ansible / runner / __init__ . py \n def run ( self ) : \n # We aren ' t iterating over all the hosts in this \n # group . So , just pick the first host in our group to \n # construct the conn object with . \n - result_data = self . _executor ( hosts [ 0 ] [ 1 ] ) . result \n + result_data = self . _executor ( hosts [ 0 ] ) . result \n # Create a ResultData item for each host in this group \n # using the returned result . If we didn ' t do this we would \n # get false reports of dark hosts .", "msg": "Send a host to runner executor instead of a letter .\n"}
{"diff": "a / analytics / management / commands / update_analytics_counts . py \n  b / analytics / management / commands / update_analytics_counts . py \n def add_arguments ( self , parser ) : \n help = ' Update stat tables from current state to - - time . Defaults to the current time . ' , \n default = timezone_now ( ) . isoformat ( ) ) \n parser . add_argument ( ' - - utc ' , \n - type = bool , \n + action = ' store_true ' , \n help = \" Interpret - - time in UTC . \" , \n default = False ) \n parser . add_argument ( ' - - stat ' , ' - s ' ,", "msg": "analytics : Fix - - utc argument in update_analytics_counts . py .\n"}
{"diff": "a / compose / config / validation . py \n  b / compose / config / validation . py \n def _parse_valid_types_from_validator ( validator ) : \n if len ( validator ) > = 2 : \n first_type = anglicize_validator ( validator [ 0 ] ) \n last_type = anglicize_validator ( validator [ - 1 ] ) \n - types_from_validator = \" { } { } \" . format ( first_type , \" , \" . join ( validator [ 1 : - 1 ] ) ) \n + types_from_validator = \" { } \" . format ( \" , \" . join ( [ first_type ] + validator [ 1 : - 1 ] ) ) \n \n msg = \" { } or { } \" . format ( \n types_from_validator , \n def _parse_oneof_validator ( error ) : \n Inspecting the context value of a ValidationError gives us information about \n which sub schema failed and which kind of error it is . \n \" \" \" \n - \n required = [ context for context in error . context if context . validator = = ' required ' ] \n if required : \n return required [ 0 ] . message", "msg": "Improve error message for type constraints\n"}
{"diff": "a / lib / ansible / playbook / base . py \n  b / lib / ansible / playbook / base . py \n def post_validate ( self , templar ) : \n if isinstance ( value , string_types ) and ' % ' in value : \n value = value . replace ( ' % ' , ' ' ) \n value = float ( value ) \n - elif attribute . isa = = ' list ' : \n + elif attribute . isa in ( ' list ' , ' barelist ' ) : \n if value is None : \n value = [ ] \n elif not isinstance ( value , list ) : \n - if isinstance ( value , string_types ) : \n + if isinstance ( value , string_types ) and attribute . isa = = ' barelist ' : \n + display . deprecated ( \n + \" Using comma separated values for a list has been deprecated . \" \\ \n + \" You should instead use the correct YAML syntax for lists . \" \\ \n + ) \n value = value . split ( ' , ' ) \n else : \n value = [ value ]", "msg": "Create a special class of list FieldAttribute for splitting on commas\n"}
{"diff": "a / Lib / test / test_fstring . py \n  b / Lib / test / test_fstring . py \n def test_no_backslashes_in_expression_part ( self ) : \n ] ) \n \n def test_no_escapes_for_braces ( self ) : \n - # \\ x7b is ' { ' . Make sure it doesn ' t start an expression . \n - self . assertEqual ( f ' \\ x7b2 } } ' , ' { 2 } ' ) \n - self . assertEqual ( f ' \\ x7b2 ' , ' { 2 ' ) \n - self . assertEqual ( f ' \\ u007b2 ' , ' { 2 ' ) \n - self . assertEqual ( f ' \\ N { LEFT CURLY BRACKET } 2 \\ N { RIGHT CURLY BRACKET } ' , ' { 2 } ' ) \n + \" \" \" \n + Only literal curly braces begin an expression . \n + \" \" \" \n + # \\ x7b is ' { ' . \n + self . assertEqual ( f ' \\ x7b1 + 1 } } ' , ' { 1 + 1 } ' ) \n + self . assertEqual ( f ' \\ x7b1 + 1 ' , ' { 1 + 1 ' ) \n + self . assertEqual ( f ' \\ u007b1 + 1 ' , ' { 1 + 1 ' ) \n + self . assertEqual ( f ' \\ N { LEFT CURLY BRACKET } 1 + 1 \\ N { RIGHT CURLY BRACKET } ' , ' { 1 + 1 } ' ) \n \n def test_newlines_in_expressions ( self ) : \n self . assertEqual ( f ' { 0 } ' , ' 0 ' )", "msg": "Update test_no_escapes_for_braces to clarify behavior with a docstring and expressions that clearly are not evaluated .\n"}
{"diff": "a / transformers / configuration_utils . py \n  b / transformers / configuration_utils . py \n def __init__ ( self , * * kwargs ) : \n self . use_bfloat16 = kwargs . pop ( ' use_bfloat16 ' , False ) \n self . pruned_heads = kwargs . pop ( ' pruned_heads ' , { } ) \n self . is_decoder = kwargs . pop ( ' is_decoder ' , False ) \n - self . idx2label = kwargs . pop ( ' idx2label ' , { i : ' LABEL_ { } ' . format ( i ) for i in range ( self . num_labels ) } ) \n - self . label2idx = kwargs . pop ( ' label2idx ' , dict ( zip ( self . idx2label . values ( ) , self . idx2label . keys ( ) ) ) ) \n + self . id2label = kwargs . pop ( ' id2label ' , { i : ' LABEL_ { } ' . format ( i ) for i in range ( self . num_labels ) } ) \n + self . label2id = kwargs . pop ( ' label2id ' , dict ( zip ( self . id2label . values ( ) , self . id2label . keys ( ) ) ) ) \n \n def save_pretrained ( self , save_directory ) : \n \" \" \" Save a configuration object to the directory ` save_directory ` , so that it", "msg": "Adding labels mapping for classification models in their respective config .\n"}
{"diff": "a / rest_framework / pagination . py \n  b / rest_framework / pagination . py \n def __init__ ( self , * args , * * kwargs ) : \n super ( BasePaginationSerializer , self ) . __init__ ( * args , * * kwargs ) \n results_field = self . results_field \n object_serializer = self . opts . object_serializer_class \n - self . fields [ results_field ] = object_serializer ( source = ' object_list ' ) \n + self . serialize_fields [ results_field ] = object_serializer ( source = ' object_list ' ) \n \n def to_native ( self , obj ) : \n \" \" \"", "msg": "Change pagination to update Serializer . serialize_fields\n"}
{"diff": "a / Lib / test / test_math . py \n  b / Lib / test / test_math . py \n class TestNoTrunc ( object ) : \n \n self . assertRaises ( TypeError , math . trunc ) \n self . assertRaises ( TypeError , math . trunc , 1 , 2 ) \n - # XXX : This is not ideal , but see the comment in math_trunc ( ) . \n - self . assertRaises ( AttributeError , math . trunc , TestNoTrunc ( ) ) \n + self . assertRaises ( ( AttributeError , TypeError ) , math . trunc , \n + TestNoTrunc ( ) ) \n \n t = TestNoTrunc ( ) \n t . __trunc__ = lambda * args : args", "msg": "be more generous to implementations that have implemented correctly\n"}
{"diff": "a / homeassistant / observers . py \n  b / homeassistant / observers . py \n def track_sun ( eventbus , statemachine , latitude , longitude ) : \n logger = logging . getLogger ( __name__ ) \n \n def update_sun_state ( now ) : \n + \" \" \" Method to update the current state of the sun and time the next update . \" \" \" \n observer = ephem . Observer ( ) \n observer . lat = latitude \n observer . long = longitude", "msg": "Missing doc string for one method .\n"}
{"diff": "a / lib / ansible / runner / action_plugins / debug . py \n  b / lib / ansible / runner / action_plugins / debug . py \n def run ( self , conn , tmp , module_name , module_args , inject , complex_args = None , * * \n result = dict ( msg = args [ ' msg ' ] ) \n elif ' var ' in args and not utils . LOOKUP_REGEX . search ( args [ ' var ' ] ) : \n results = template . template ( self . basedir , args [ ' var ' ] , inject , convert_bare = True ) \n - result [ args [ ' var ' ] ] = results \n + result [ ' var ' ] = { args [ ' var ' ] : results } \n \n # force flag to make debug output module always verbose \n result [ ' verbose_always ' ] = True", "msg": "Do not use the variable name as a key for the result of the module\n"}
{"diff": "a / locust / clients . py \n  b / locust / clients . py \n class HttpBrowser ( object ) : \n def __init__ ( self , base_url , gzip = False ) : \n self . base_url = base_url \n self . gzip = gzip \n + self . new_session ( ) \n + \n + def new_session ( self ) : \n + \" \" \" \n + Get a new HTTP session for this HttpBrowser instance \n + \" \" \" \n handlers = [ urllib2 . HTTPCookieProcessor ( ) ] \n \n # Check for basic authentication", "msg": "Added a new_session ( ) method to HttpBrowser that can be used to give the client a new session\n"}
{"diff": "a / Lib / mimetypes . py \n  b / Lib / mimetypes . py \n def guess_extension ( type ) : \n \n Return value is a string giving a filename extension , including the \n leading dot ( ' . ' ) . The extension is not guaranteed to have been \n - associated with any particular data stream , but has been known to be \n - used for streams of the MIME type given by ` type ' . If ` type ' is not \n - known , None is returned . \n + associated with any particular data stream , but would be mapped to the \n + MIME type ` type ' by guess_type ( ) . If no extension can be guessed for \n + ` type ' , None is returned . \n \" \" \" \n + global inited \n + if not inited : \n + init ( ) \n type = string . lower ( type ) \n for ext , stype in types_map . items ( ) : \n if type = = stype :", "msg": "guess_extension ( ) : Revise documentation string to be more clear . If not\n"}
{"diff": "a / scrapy / linkextractors / __init__ . py \n  b / scrapy / linkextractors / __init__ . py \n def __new__ ( cls , * args , * * kwargs ) : \n warn ( ' scrapy . linkextractors . FilteringLinkExtractor is deprecated , ' \n ' please use scrapy . linkextractors . LinkExtractor instead ' , \n ScrapyDeprecationWarning , stacklevel = 2 ) \n - return super ( FilteringLinkExtractor , cls ) . __new__ ( cls ) \n + return super ( ) . __new__ ( cls , * args , * * kwargs ) \n \n def __init__ ( self , link_extractor , allow , deny , allow_domains , deny_domains , \n restrict_xpaths , canonicalize , deny_extensions , restrict_css , restrict_text ) :", "msg": "Improve FilteringLinkExtractor . __new__\n"}
{"diff": "a / zephyr / models . py \n  b / zephyr / models . py \n def bulk_create_huddles ( users , huddle_user_list ) : \n huddles = { } \n huddles_by_id = { } \n huddle_set = set ( ) \n - existing_huddles = { } \n + existing_huddles = set ( ) \n for huddle in Huddle . objects . all ( ) : \n - existing_huddles [ huddle . huddle_hash ] = True \n + existing_huddles . add ( huddle . huddle_hash ) \n for huddle_users in huddle_user_list : \n user_ids = [ users [ email ] . id for email in huddle_users ] \n huddle_hash = get_huddle_hash ( user_ids )", "msg": "bulk_create_huddles : Use a set for existing_huddles\n"}
{"diff": "a / lib / matplotlib / backends / _backend_tk . py \n  b / lib / matplotlib / backends / _backend_tk . py \n def showtip ( self , text ) : \n except tk . TclError : \n pass \n label = tk . Label ( tw , text = self . text , justify = tk . LEFT , \n - background = \" # ffffe0 \" , relief = tk . SOLID , borderwidth = 1 ) \n + relief = tk . SOLID , borderwidth = 1 ) \n label . pack ( ipadx = 1 ) \n \n def hidetip ( self ) :", "msg": "Backport PR : Fix tk tooltips for dark themes .\n"}
{"diff": "a / zerver / tests / test_decorators . py \n  b / zerver / tests / test_decorators . py \n \n # - * - coding : utf - 8 - * - \n import mock \n \n - from typing import Any , Iterable , Optional , Text \n + from typing import Any , Iterable , List , Optional , Text , Tuple \n from django . test import TestCase \n from django . utils . translation import ugettext as _ \n from django . http import HttpResponse , HttpRequest \n \n return_success_on_head_request \n ) \n from zerver . lib . validator import ( \n - check_string , check_dict , check_bool , check_int , check_list \n + check_string , check_dict , check_bool , check_int , check_list , Validator \n ) \n from zerver . models import \\ \n get_realm , get_user_profile_by_email , UserProfile , Client \n def test_check_dict ( self ) : \n keys = [ \n ( ' names ' , check_list ( check_string ) ) , \n ( ' city ' , check_string ) , \n - ] \n + ] # type : List [ Tuple [ str , Validator ] ] \n \n x = { \n ' names ' : [ ' alice ' , ' bob ' ] ,", "msg": "Type annotate a variable to prevent future errors .\n"}
{"diff": "a / scrapy / commands / deploy . py \n  b / scrapy / commands / deploy . py \n def add_options ( self , parser ) : \n help = \" list available targets \" ) \n parser . add_option ( \" - l \" , \" - - list - projects \" , metavar = \" TARGET \" , \\ \n help = \" list available projects on TARGET \" ) \n + parser . add_option ( \" - - egg \" , metavar = \" FILE \" , \n + help = \" use the given egg , instead of building it \" ) \n \n def run ( self , args , opts ) : \n try : \n def run ( self , args , opts ) : \n return \n target , project = _get_target_project ( args ) \n version = _get_version ( opts ) \n - egg = _build_egg ( ) \n + if opts . egg : \n + egg = open ( opts . egg , ' rb ' ) \n + else : \n + _log ( \" Bulding egg of % s - % s \" % ( project , version ) ) \n + egg = _build_egg ( ) \n _upload_egg ( target , egg , project , version ) \n \n def _log ( message ) :", "msg": "added - - egg argument to scrapy deploy command , and log message when building the egg\n"}
{"diff": "a / Tools / bgen / bgen / bgenObjectDefinition . py \n  b / Tools / bgen / bgen / bgenObjectDefinition . py \n class ObjectDefinition ( GeneratorGroup ) : \n basechain = \" NULL \" \n tp_flags = \" Py_TPFLAGS_DEFAULT \" \n basetype = None \n + argref = \" \" # set to \" * \" if arg to < type > _New should be pointer \n \n def __init__ ( self , name , prefix , itselftype ) : \n \" \" \" ObjectDefinition constructor . May be extended , but do not override . \n def __init__ ( self , name , prefix , itselftype ) : \n self . itselftype = itselftype \n self . objecttype = name + ' Object ' \n self . typename = name + ' _Type ' \n - self . argref = \" \" # set to \" * \" if arg to < type > _New should be pointer \n self . static = \" static \" # set to \" \" to make < type > _New and < type > _Convert public \n self . modulename = None \n if hasattr ( self , \" assertions \" ) :", "msg": "Handle argref so it can be overridden more easily in a subclass .\n"}
{"diff": "a / zipline / utils / simfactory . py \n  b / zipline / utils / simfactory . py \n def create_test_zipline ( * * config ) : \n test_algo . sim_params , \n concurrent = concurrent_trades \n ) \n - \n - test_algo . set_sources ( [ trade_source ] ) \n + if trade_source : \n + test_algo . set_sources ( [ trade_source ] ) \n \n # mmmmmmmmmmmmmmmmmm - \n # Transforms", "msg": "allows config to force no datasources by passing a None .\n"}
{"diff": "a / lib / ansible / modules / extras / system / locale_gen . py \n  b / lib / ansible / modules / extras / system / locale_gen . py \n def is_present ( name ) : \n def fix_case ( name ) : \n \" \" \" locale - a might return the encoding in either lower or upper case . \n Passing through this function makes them uniform for comparisons . \" \" \" \n - for s , r in LOCALE_NORMALIZATION . iteritems ( ) : \n + for s , r in LOCALE_NORMALIZATION . items ( ) : \n name = name . replace ( s , r ) \n return name", "msg": "replace iteritems with items to ensure python3 compatibility\n"}
{"diff": "a / tests / test_parsers . py \n  b / tests / test_parsers . py \n \n - # - * - coding : utf - 8 - * - \n + # - * - coding : utf - 8 - * - \n \n from __future__ import unicode_literals \n from rest_framework . compat import StringIO \n def test_get_encoded_filename ( self ) : \n \n def __replace_content_disposition ( self , disposition ) : \n self . parser_context [ ' request ' ] . META [ ' HTTP_CONTENT_DISPOSITION ' ] = disposition \n - \n -", "msg": "Move parser tests to correct directory\n"}
{"diff": "a / sklearn / cluster / k_means_ . py \n  b / sklearn / cluster / k_means_ . py \n def predict ( self , X ) : \n \n Returns \n mmmmmm - \n - Y : array , shape [ n_samples , ] \n - Index of the closest center each sample belongs to . \n + labels : array , shape [ n_samples , ] \n + Index of the cluster each sample belongs to . \n \" \" \" \n self . _check_fitted ( ) \n X = self . _check_test_data ( X )", "msg": "Better docstring for KMeans . predict .\n"}
{"diff": "a / bokeh / charts / _charts . py \n  b / bokeh / charts / _charts . py \n def end_plot ( self ) : \n def make_axis ( self , dimension , scale , label ) : \n \" Create linear , date or categorical axis depending on the scale and dimension . \" \n if scale = = \" linear \" : \n + dim_loc_map = { 0 : \" bottom \" , 1 : \" left \" } \n axis = LinearAxis ( plot = self . plot , \n dimension = dimension , \n - location = \" min \" , \n + location = dim_loc_map [ dimension ] , \n axis_label = label ) \n elif scale = = \" date \" : \n axis = DatetimeAxis ( plot = self . plot , \n dimension = dimension , \n - location = \" min \" , \n + location = dim_loc_map [ dimension ] , \n axis_label = label ) \n elif scale = = \" categorical \" : \n axis = CategoricalAxis ( plot = self . plot ,", "msg": "make_axis ( ) needs to use new ' location ' values after layout PR merge\n"}
{"diff": "a / lib / mpl_toolkits / axisartist / grid_finder . py \n  b / lib / mpl_toolkits / axisartist / grid_finder . py \n def __init__ ( self , \n \n \n class MaxNLocator ( mticker . MaxNLocator ) : \n - def __init__ ( self , nbins = 10 , steps = None , \n - trim = True , \n + def __init__ ( self , nbins = 10 , steps = None , \n + trim = True , \n integer = False , \n symmetric = False , \n prune = None ) : \n - \n + # trim argument has no effect . It has been left for API compatibility \n mticker . MaxNLocator . __init__ ( self , nbins , steps = steps , \n - trim = trim , integer = integer , \n + integer = integer , \n symmetric = symmetric , prune = prune ) \n self . create_dummy_axis ( ) \n self . _factor = None", "msg": "Dont forward trim argument to ticker where it has no effect\n"}
{"diff": "a / zerver / views / email_log . py \n  b / zerver / views / email_log . py \n \n from zerver . models import get_realm , get_user_by_delivery_email \n from zerver . lib . notifications import enqueue_welcome_emails \n from zerver . lib . response import json_success \n + from zerver . lib . actions import do_change_user_delivery_email \n from zproject . email_backends import ( \n get_forward_address , \n set_forward_address , \n def generate_all_emails ( request : HttpRequest ) - > HttpResponse : \n assert result . status_code = = 200 \n \n # Reset the email value so we can run this again \n - user_profile . email = registered_email \n - user_profile . save ( update_fields = [ ' email ' ] ) \n + do_change_user_delivery_email ( user_profile , registered_email ) \n \n # Follow up day1 day2 emails for normal user \n enqueue_welcome_emails ( user_profile )", "msg": "emails : Fix broken email revert process in email_log .\n"}
{"diff": "a / zerver / tests / test_narrow . py \n  b / zerver / tests / test_narrow . py \n def _do_add_term_test ( self , term : Dict [ str , Any ] , where_clause : Text , \n if params is not None : \n actual_params = query . compile ( ) . params \n self . assertEqual ( actual_params , params ) \n - self . assertTrue ( where_clause in str ( query ) ) \n + self . assertIn ( where_clause , str ( query ) ) \n \n def _build_query ( self , term : Dict [ str , Any ] ) - > Query : \n return self . builder . add_term ( self . raw_query , term )", "msg": "test_narrow : Use a better assert for easier debugging .\n"}
{"diff": "a / zerver / data_import / mattermost . py \n  b / zerver / data_import / mattermost . py \n def get_invite_only_value_from_channel_type ( channel_type : str ) - > bool : \n for username in channel_members_map [ stream_name ] : \n channel_users . add ( user_id_mapper . get ( username ) ) \n \n - if channel_users : \n - subscriber_handler . set_info ( \n - users = channel_users , \n - stream_id = stream_id , \n - ) \n + subscriber_handler . set_info ( \n + users = channel_users , \n + stream_id = stream_id , \n + ) \n streams . append ( stream ) \n return streams", "msg": "mattermost import : Fix handling of channels with no subscribers .\n"}
{"diff": "a / Lib / test / test_smtpnet . py \n  b / Lib / test / test_smtpnet . py \n def check_ssl_verifiy ( host , port ) : \n \n class SmtpTest ( unittest . TestCase ) : \n testServer = ' smtp . gmail . com ' \n - remotePort = 25 \n + remotePort = 587 \n \n def test_connect_starttls ( self ) : \n support . get_attribute ( smtplib , ' SMTP_SSL ' )", "msg": "Issue : Update Gmail port number for STARTTLS to 587 .\n"}
{"diff": "a / scrapy / extensions / telnet . py \n  b / scrapy / extensions / telnet . py \n def from_crawler ( cls , crawler ) : \n def start_listening ( self ) : \n self . port = listen_tcp ( self . portrange , self . host , self ) \n h = self . port . getHost ( ) \n - logger . debug ( \" Telnet console listening on % ( host ) s : % ( port ) d \" , \n - { ' host ' : h . host , ' port ' : h . port } , \n - extra = { ' crawler ' : self . crawler } ) \n + logger . info ( \" Telnet console listening on % ( host ) s : % ( port ) d \" , \n + { ' host ' : h . host , ' port ' : h . port } , \n + extra = { ' crawler ' : self . crawler } ) \n \n def stop_listening ( self ) : \n self . port . stopListening ( )", "msg": "use INFO log level to show telnet host / port\n"}
{"diff": "a / lib / ansible / modules / cloud / amazon / ec2_lc . py \n  b / lib / ansible / modules / cloud / amazon / ec2_lc . py \n \n required : false \n security_groups : \n description : \n - - A list of security groups into which instances should be found \n + - A list of security groups to apply to the instances . For VPC instances , specify security group IDs . For EC2 - Classic , specify either security group names or IDs . \n required : false \n region : \n description :", "msg": "Improve documentation on security_groups - option\n"}
{"diff": "a / keras / initializers . py \n  b / keras / initializers . py \n def get_config ( self ) : \n \n @ classmethod \n def from_config ( cls , config ) : \n + if ' dtype ' in config : \n + # Initializers saved from ` tf . keras ` \n + # may contain an unused ` dtype ` argument . \n + config . pop ( ' dtype ' ) \n return cls ( * * config )", "msg": "Add handling for ` dtype ` arg in initializer config .\n"}
{"diff": "a / glances / __init__ . py \n  b / glances / __init__ . py \n \n __license__ = ' LGPL ' \n \n # Import system lib \n + import locale \n import platform \n import signal \n import sys \n - import locale \n \n # Import psutil \n try : \n \n from glances . core . glances_logging import logger \n from glances . core . glances_main import GlancesMain \n \n - # Setup translations \n try : \n locale . setlocale ( locale . LC_ALL , ' ' ) \n except locale . Error : \n - # Issue # 517 \n - # Setting LC_ALL to ' ' should not generate an error unless LC_ALL is not \n - # defined in the user environment , which can be the case when used via SSH . \n - # So simply skip this error , as python will use the C locale by default . \n - logger . warning ( \" No locale LC_ALL variable found . Use the default C locale . \" ) \n - pass \n + print ( \" Warning : Unable to set locale . Expect encoding problems . \" ) \n \n # Check Python version \n if sys . version_info < ( 2 , 6 ) or ( 3 , 0 ) < = sys . version_info < ( 3 , 3 ) :", "msg": "Be more explicit about when the modification of the locale fails\n"}
{"diff": "a / zerver / data_import / hipchat . py \n  b / zerver / data_import / hipchat . py \n def get_raw_message ( d : Dict [ str , Any ] ) - > Optional [ ZerverFieldsT ] : \n return None \n \n if is_pm_data : \n - if int ( sender_id ) ! = int ( fn_id ) : \n + # We need to compare with str ( ) on both sides here . \n + # In Stride , user IDs are strings , but in HipChat , \n + # they are integers , and fn_id is always a string . \n + if str ( sender_id ) ! = str ( fn_id ) : \n # PMs are in multiple places in the Hipchat export , \n # and we only use the copy from the sender \n return None", "msg": "hipchat : Fix handling of user IDs in Stride import .\n"}
{"diff": "a / scipy_test / testing . py \n  b / scipy_test / testing . py \n def _get_module_tests ( self , module , level ) : \n return self . _get_suite_list ( test_module , level , module . __name__ ) \n \n def _get_suite_list ( self , test_module , level , module_name = ' __main__ ' ) : \n + mstr = self . _module_str \n if hasattr ( test_module , ' test_suite ' ) : \n # Using old styled test suite \n try :", "msg": "added missing definition of ' mstr ' for use in failure reporting\n"}
{"diff": "a / youtube_dl / YoutubeDL . py \n  b / youtube_dl / YoutubeDL . py \n def save_console_title ( self ) : \n if not self . params . get ( ' consoletitle ' , False ) : \n return \n if ' TERM ' in os . environ : \n - write_string ( u ' \\ 033 [ 22t ' , self . _screen_file ) \n + # Save the title on stack \n + write_string ( u ' \\ 033 [ 22 ; 0t ' , self . _screen_file ) \n \n def restore_console_title ( self ) : \n if not self . params . get ( ' consoletitle ' , False ) : \n return \n if ' TERM ' in os . environ : \n - write_string ( u ' \\ 033 [ 23t ' , self . _screen_file ) \n + # Restore the title from stack \n + write_string ( u ' \\ 033 [ 23 ; 0t ' , self . _screen_file ) \n \n def __enter__ ( self ) : \n self . save_console_title ( )", "msg": "Correctly write and restore the console title on the stack ( fixes )\n"}
{"diff": "a / setup . py \n  b / setup . py \n def detect_modules ( self ) : \n # strip out double - dashes first so that we don ' t end up with \n # substituting \" - - Long \" to \" - Long \" and thus lead to \" ong \" being \n # used for a library directory . \n - env_val = re . sub ( r ' ( ^ | \\ s + ) - ( - | ( ? ! % s ) ) ' % arg_name [ 1 ] , ' ' , env_val ) \n + env_val = re . sub ( r ' ( ^ | \\ s + ) - ( - | ( ? ! % s ) ) ' % arg_name [ 1 ] , \n + ' ' , env_val ) \n parser = optparse . OptionParser ( ) \n # Make sure that allowing args interspersed with options is \n # allowed", "msg": "Bug : fix stripping of unwanted LDFLAGS .\n"}
{"diff": "a / updateHostsFile . py \n  b / updateHostsFile . py \n \n \n try : \n import requests \n - except ModuleNotFoundError : # noqa : F821 \n - raise ModuleNotFoundError ( \" This project ' s dependencies have changed . The Requests library ( \" # noqa : F821 \n - \" https : / / requests . readthedocs . io / en / master / ) is now required . \" ) \n + except ImportError : \n + raise ImportError ( \" This project ' s dependencies have changed . The Requests library ( \" \n + \" https : / / requests . readthedocs . io / en / master / ) is now required . \" ) \n \n \n # Syntactic sugar for \" sudo \" command in UNIX / Linux", "msg": "Changed dependency - related exception to be compatible with Python versions < 3 . 6\n"}
{"diff": "a / numpy / core / tests / test_print . py \n  b / numpy / core / tests / test_print . py \n def _test_locale_independance ( tp ) : \n else : \n locale . setlocale ( locale . LC_NUMERIC , ' FRENCH ' ) \n \n - assert_equal ( locale . format ( \" % f \" , tp ( 1 . 2 ) ) , locale . format ( \" % f \" , float ( 1 . 2 ) ) , \n + assert_equal ( str ( tp ( 1 . 2 ) ) , str ( float ( 1 . 2 ) ) , \n err_msg = ' Failed locale test for type % s ' % tp ) \n finally : \n locale . setlocale ( locale . LC_NUMERIC , locale = curloc )", "msg": "Revert buggy test fix for locale independecce .\n"}
{"diff": "a / Lib / json / __init__ . py \n  b / Lib / json / __init__ . py \n \n Compact encoding : : \n \n > > > import json \n - > > > json . dumps ( [ 1 , 2 , 3 , { ' 4 ' : 5 , ' 6 ' : 7 } ] , separators = ( ' , ' , ' : ' ) ) \n + > > > json . dumps ( [ 1 , 2 , 3 , { ' 4 ' : 5 , ' 6 ' : 7 } ] , sort_keys = True , separators = ( ' , ' , ' : ' ) ) \n ' [ 1 , 2 , 3 , { \" 4 \" : 5 , \" 6 \" : 7 } ] ' \n \n Pretty printing ( using repr ( ) because of extraneous whitespace in the output ) : :", "msg": "Let ' s sort the keys so that this test passes even with random hashes .\n"}
{"diff": "a / sklearn / feature_extraction / text . py \n  b / sklearn / feature_extraction / text . py \n def fit_transform ( self , raw_documents , y = None ) : \n vocab = dict ( ( ( t , i ) for i , t in enumerate ( sorted ( terms ) ) ) ) \n if not vocab : \n raise ValueError ( \" empty vocabulary ; training set may have \" \n - \" contained only stop words \" ) \n + \" contained only stop words or min_df ( resp . \" \n + \" max_df ) may be too high ( resp . too low ) . \" ) \n self . vocabulary_ = vocab \n \n # the term_counts and document_counts might be useful statistics , are", "msg": "Improve error message when vocabulary is empty .\n"}
{"diff": "a / IPython / core / builtin_trap . py \n  b / IPython / core / builtin_trap . py \n \n import __builtin__ \n \n from IPython . config . configurable import Configurable \n - from IPython . core . quitter import Quitter \n \n from IPython . utils . traitlets import Instance \n \n similarity index 100 % \n rename from IPython / core / quitter . py \n rename to IPython / deathrow / quitter . py", "msg": "moved Quitter to deathrow as it is no longer used by anything\n"}
{"diff": "a / scrapy / commands / parse . py \n  b / scrapy / commands / parse . py \n def add_options ( self , parser ) : \n parser . add_option ( \" - m \" , \" - - meta \" , dest = \" meta \" , \n help = \" inject extra meta into the Request , it must be a valid raw json string \" ) \n parser . add_option ( \" - - cbkwargs \" , dest = \" cbkwargs \" , \n - help = \" inject extra cbkwargs into the Request , it must be a valid raw json string \" ) \n + help = \" inject extra callback kwargs into the Request , it must be a valid raw json string \" ) \n parser . add_option ( \" - d \" , \" - - depth \" , dest = \" depth \" , type = \" int \" , default = 1 , \n help = \" maximum depth for parsing requests [ default : % default ] \" ) \n parser . add_option ( \" - v \" , \" - - verbose \" , dest = \" verbose \" , action = \" store_true \" ,", "msg": "parse command : improve option description\n"}
{"diff": "a / Lib / distutils / core . py \n  b / Lib / distutils / core . py \n class from it , and returns the class object . \n expected class was not found in it . \" \" \" \n \n module_name = ' distutils . command . ' + command \n - klass_name = string . join \\ \n - ( map ( string . capitalize , string . split ( command , ' _ ' ) ) , ' ' ) \n + klass_name = command \n \n try : \n __import__ ( module_name )", "msg": "Command classes are now named identically to their commands , so reflect this\n"}
{"diff": "a / IPython / zmq / ipkernel . py \n  b / IPython / zmq / ipkernel . py \n def embed_kernel ( module = None , local_ns = None ) : \n if local_ns is None : \n local_ns = caller_locals \n app = IPKernelApp . instance ( user_module = module , user_ns = local_ns ) \n - app . initialize ( ) \n + app . initialize ( [ ] ) \n app . start ( ) \n \n def main ( ) :", "msg": "embed_kernel : pass [ ] to IPKernelApp . initialize to prevent argv parsing\n"}
{"diff": "a / sklearn / pipeline . py \n  b / sklearn / pipeline . py \n def make_pipeline ( * steps ) : \n \" \" \" Construct a Pipeline from the given estimators . \n \n This is a shorthand for the Pipeline constructor ; it does not require , and \n - does not permit , naming the estimators . Instead , they will be given names \n - automatically based on their types . \n + does not permit , naming the estimators . Instead , their names will be set \n + to the lowercase of their types automatically . \n \n Examples \n mmmmmm - -", "msg": "edited docstring for clarity regarding the naming of the pipeline components .\n"}
{"diff": "a / lib / ansible / modules / extras / cloud / vmware / vmware_vm_shell . py \n  b / lib / ansible / modules / extras / cloud / vmware / vmware_vm_shell . py \n \n vm_id_type : \n description : \n - The identification tag for the VM \n - default : dns_name \n + default : vm_name \n choices : \n - ' uuid ' \n - ' dns_name ' \n - ' inventory_path ' \n + - ' vm_name ' \n required : False \n vm_username : \n description :", "msg": "Changing docs to reflect vm_name as the default vm_id_type\n"}
{"diff": "a / zephyr / views . py \n  b / zephyr / views . py \n def api_github_landing ( request , user_profile , event = POST , \n \n subject = \" % s : pull request % d \" % ( repository [ ' name ' ] , \n pull_req [ ' number ' ] ) \n - content = ( \" New [ pull request ] ( % s ) from % s : \\ n \\ n % s \\ n \\ n > % s \" \n - % ( pull_req [ ' html_url ' ] , \n - pull_req [ ' user ' ] [ ' login ' ] , \n + content = ( \" Pull request from % s [ % s ] ( % s ) : \\ n \\ n % s \\ n \\ n > % s \" \n + % ( pull_req [ ' user ' ] [ ' login ' ] , \n + payload [ ' action ' ] , \n + pull_req [ ' html_url ' ] , \n pull_req [ ' title ' ] , \n pull_req [ ' body ' ] ) ) \n elif event = = ' push ' :", "msg": "github hooks : Display different actions for pull requests sanely\n"}
{"diff": "a / dash / dash . py \n  b / dash / dash . py \n def _add_url ( self , name , view_func , methods = ( ' GET ' , ) ) : \n # e . g . for adding authentication with flask_login \n self . routes . append ( name ) \n \n + @ property \n + def layout ( self ) : \n + return self . _layout \n + \n def _layout_value ( self ) : \n if isinstance ( self . _layout , _patch_collections_abc ( ' Callable ' ) ) : \n self . _cached_layout = self . _layout ( ) \n def _layout_value ( self ) : \n self . _cached_layout = self . _layout \n return self . _cached_layout \n \n - @ property \n - def layout ( self ) : \n - return self . _layout \n - \n @ layout . setter \n def layout ( self , value ) : \n if ( not isinstance ( value , Component ) and", "msg": "Revert \" mypy wants property getter and setter to be consecutive to each other . \"\n"}
{"diff": "a / spacy / en / download . py \n  b / spacy / en / download . py \n def install_data ( url , extract_path , download_path ) : \n assert tmp = = download_path \n t = tarfile . open ( download_path ) \n t . extractall ( extract_path ) \n + os . unlink ( download_path ) \n \n \n @ plac . annotations (", "msg": "* Remove the archive after download , to save disk space\n"}
{"diff": "a / Lib / test / test_unary . py \n  b / Lib / test / test_unary . py \n def test_invert ( self ) : \n self . assert_ ( - - 2 = = 2 ) \n self . assert_ ( - 2L = = 0 - 2L ) \n \n - def test_overflow ( self ) : \n - self . assertRaises ( OverflowError , eval , \" + \" + ( \" 9 \" * 32 ) ) \n - self . assertRaises ( OverflowError , eval , \" - \" + ( \" 9 \" * 32 ) ) \n - self . assertRaises ( OverflowError , eval , \" ~ \" + ( \" 9 \" * 32 ) ) \n + def test_no_overflow ( self ) : \n + nines = \" 9 \" * 32 \n + self . assert_ ( eval ( \" + \" + nines ) = = eval ( \" + \" + nines + \" L \" ) ) \n + self . assert_ ( eval ( \" - \" + nines ) = = eval ( \" - \" + nines + \" L \" ) ) \n + self . assert_ ( eval ( \" ~ \" + nines ) = = eval ( \" ~ \" + nines + \" L \" ) ) \n \n def test_bad_types ( self ) : \n for op in ' + ' , ' - ' , ' ~ ' :", "msg": "Change test_overflow to test_no_overflow ; looks like big int literals\n"}
{"diff": "a / homeassistant / components / zha / helpers . py \n  b / homeassistant / components / zha / helpers . py \n async def configure_reporting ( entity_id , cluster , attr , skip_bind = False , \n \n attr_name = cluster . attributes . get ( attr , [ attr ] ) [ 0 ] \n cluster_name = cluster . ep_attribute \n + kwargs = { } \n + if manufacturer : \n + kwargs [ ' manufacturer ' ] = manufacturer \n try : \n res = await cluster . configure_reporting ( attr , min_report , \n max_report , reportable_change , \n - manufacturer = manufacturer ) \n + * * kwargs ) \n _LOGGER . debug ( \n \" % s : reporting ' % s ' attr on ' % s ' cluster : % d / % d / % d : Result : ' % s ' \" , \n entity_id , attr_name , cluster_name , min_report , max_report ,", "msg": "Use manufacturer id only for configure_reporting only when specified . ( )\n"}
{"diff": "a / lib / ansible / modules / extras / cloud / docker / docker_login . py \n  b / lib / ansible / modules / extras / cloud / docker / docker_login . py \n def login ( self ) : \n reauth = self . reauth , \n dockercfg_path = self . dockercfg_path \n ) \n + except DockerAPIError as e : \n + self . module . fail_json ( msg = \" Docker API Error : % s \" % e . explanation ) \n except Exception as e : \n self . module . fail_json ( msg = \" failed to login to the remote registry \" , error = repr ( e ) )", "msg": "Added more meaningful fail messages on Docker API\n"}
{"diff": "a / lib / matplotlib / backends / _backend_tk . py \n  b / lib / matplotlib / backends / _backend_tk . py \n def destroy ( self , * args ) : \n \n self . window . destroy ( ) \n \n - if not Gcf . get_num_fig_managers ( ) and self . _owns_mainloop : \n + if self . _owns_mainloop and not Gcf . get_num_fig_managers ( ) : \n self . window . quit ( ) \n \n def get_window_title ( self ) : \n def mainloop ( ) : \n if managers : \n first_manager = managers [ 0 ] \n manager_class = type ( first_manager ) \n + if manager_class . _owns_mainloop : \n + return \n manager_class . _owns_mainloop = True \n - first_manager . window . mainloop ( ) \n - manager_class . _owns_mainloop = False \n + try : \n + first_manager . window . mainloop ( ) \n + finally : \n + manager_class . _owns_mainloop = False", "msg": "Use finally to reset mainloop ownership\n"}
{"diff": "a / gym / benchmarks / __init__ . py \n  b / gym / benchmarks / __init__ . py \n \n tasks = [ \n { ' env_id ' : ' MinecraftObstacles - v0 ' , \n ' trials ' : 1 , \n - ' max_timesteps ' : 900 , \n + ' max_timesteps ' : 900000 , \n ' reward_floor ' : - 1000 . 0 , \n ' reward_ceiling ' : 2080 . 0 , \n } , \n { ' env_id ' : ' MinecraftSimpleRoomMaze - v0 ' , \n ' trials ' : 1 , \n - ' max_timesteps ' : 900 , \n + ' max_timesteps ' : 900000 , \n ' reward_floor ' : - 1000 . 0 , \n ' reward_ceiling ' : 4160 . 0 , \n } , \n { ' env_id ' : ' MinecraftAttic - v0 ' , \n ' trials ' : 1 , \n - ' max_timesteps ' : 600 , \n + ' max_timesteps ' : 600000 , \n ' reward_floor ' : - 1000 . 0 , \n ' reward_ceiling ' : 1040 . 0 , \n } , \n { ' env_id ' : ' MinecraftComplexityUsage - v0 ' , \n ' trials ' : 1 , \n - ' max_timesteps ' : 600 , \n + ' max_timesteps ' : 600000 , \n ' reward_floor ' : - 1000 . 0 , \n ' reward_ceiling ' : 1000 . 0 , \n } ,", "msg": "Fixed timesteps in MinecraftHard - v0 benchmark .\n"}
{"diff": "a / pipenv / core . py \n  b / pipenv / core . py \n def warn_in_virtualenv ( ) : \n ' { 0 } : Pipenv found itself running within a virtual environment , ' \n ' so it will automatically use that environment , instead of ' \n ' creating its own for any project . You can set ' \n - ' PIPENV_IGNORE_VIRTUALENVS = 1 to force pipenv to ignore that ' \n - ' environment and create its own instead . ' . format ( \n - crayons . green ( ' Courtesy Notice ' ) \n + ' { 1 } to force pipenv to ignore that environment and create ' \n + ' its own instead . ' . format ( \n + crayons . green ( ' Courtesy Notice ' ) , \n + crayons . normal ( ' PIPENV_IGNORE_VIRTUALENVS = 1 ' , bold = True ) , \n ) , \n err = True , \n )", "msg": "Use bold formatting for required variable\n"}
{"diff": "a / lib / ansible / modules / cloud / azure / azure . py \n  b / lib / ansible / modules / cloud / azure / azure . py \n def create_virtual_machine ( module , azure ) : \n authorized_keys_path = u ' / home / % s / . ssh / authorized_keys ' % user \n ssh_config . public_keys . public_keys . append ( PublicKey ( path = authorized_keys_path , fingerprint = fingerprint ) ) \n # Append ssh config to linux machine config \n - linux_config . ssh = ssh_config \n + vm_config . ssh = ssh_config \n \n # Create network configuration \n network_config = ConfigurationSetInputEndpoints ( ) \n def main ( ) : \n cloud_service_raw = None \n if module . params . get ( ' state ' ) = = ' absent ' : \n ( changed , public_dns_name , deployment ) = terminate_virtual_machine ( module , azure ) \n - \n + \n elif module . params . get ( ' state ' ) = = ' present ' : \n # Changed is always set to true when provisioning new instances \n if not module . params . get ( ' name ' ) :", "msg": "enable azure to provision windows instances\n"}
{"diff": "a / youtube_dl / extractor / youtube . py \n  b / youtube_dl / extractor / youtube . py \n def add_dash_mpd ( video_info ) : \n add_dash_mpd ( video_info ) \n else : \n age_gate = False \n + video_info = None \n # Try looking directly into the video webpage \n mobj = re . search ( r ' ; ytplayer \\ . config \\ s * = \\ s * ( { . * ? } ) ; ' , video_webpage ) \n if mobj :", "msg": "[ youtube ] Fix reference before assignment for video_info\n"}
{"diff": "a / zulip_tools . py \n  b / zulip_tools . py \n def su_to_zulip ( ) : \n pwent = pwd . getpwnam ( \" zulip \" ) \n os . setgid ( pwent . pw_gid ) \n os . setuid ( pwent . pw_uid ) \n + os . environ [ ' HOME ' ] = os . path . abspath ( os . path . join ( DEPLOYMENTS_DIR , ' . . ' ) ) \n \n def make_deploy_path ( ) : \n # type : ( ) - > str", "msg": "Improve su_to_zulip setting of home directory .\n"}
{"diff": "a / zerver / data_import / mattermost . py \n  b / zerver / data_import / mattermost . py \n def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : \n continue \n \n pub_date = raw_message [ ' pub_date ' ] \n + sender_user_id = raw_message [ ' sender_id ' ] \n try : \n recipient_id = get_recipient_id_from_receiver_name ( raw_message [ \" receiver_id \" ] , Recipient . STREAM ) \n except KeyError : \n def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : \n rendered_content = None \n \n topic_name = ' imported from mattermost ' \n - user_id = raw_message [ ' sender_id ' ] \n \n message = build_message ( \n content = content , \n def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : \n recipient_id = recipient_id , \n rendered_content = rendered_content , \n topic_name = topic_name , \n - user_id = user_id , \n + user_id = sender_user_id , \n has_attachment = False , \n ) \n zerver_message . append ( message )", "msg": "mattermost : Rename user_id to sender_user_id in process_raw_message_batch .\n"}
{"diff": "a / celery / events / state . py \n  b / celery / events / state . py \n def __init__ ( self , uuid = None , cluster_state = None , children = None , * * kwargs ) : \n ) \n self . _serializer_handlers = { \n ' children ' : self . _serializable_children , \n + ' root ' : self . _serializable_root , \n + ' parent ' : self . _serializable_parent , \n } \n if kwargs : \n self . __dict__ . update ( kwargs ) \n def as_dict ( self ) : \n def _serializable_children ( self , value ) : \n return [ task . id for task in self . children ] \n \n + def _serializable_root ( self , value ) : \n + return self . root_id \n + \n + def _serializable_parent ( self , value ) : \n + return self . parent_id \n + \n def __reduce__ ( self ) : \n return _depickle_task , ( self . __class__ , self . as_dict ( ) )", "msg": "Add serializer handlers for root and parent ( )\n"}
{"diff": "a / lib / ansible / modules / database / mysql / mysql_user . py \n  b / lib / ansible / modules / database / mysql / mysql_user . py \n class InvalidPrivsError ( Exception ) : \n # MySQL module specific support methods . \n # \n \n - def connect ( module , login_user , login_password , config_file = ' ~ / . my . cnf ' ) : \n + def connect ( module , login_user = None , login_password = None , config_file = ' ~ / . my . cnf ' ) : \n config = { \n ' host ' : module . params [ ' login_host ' ] , \n ' db ' : ' mysql '", "msg": "Allow playbook specified login_user and login_password to override config file settings\n"}
{"diff": "a / Lib / asyncio / unix_events . py \n  b / Lib / asyncio / unix_events . py \n \n \" \" \" Selector event loop for Unix with signal handling . \" \" \" \n \n import errno \n - import fcntl \n import os \n import signal \n import socket \n def create_unix_server ( self , protocol_factory , path = None , * , \n def _set_nonblocking ( fd ) : \n os . set_blocking ( fd , False ) \n else : \n + import fcntl \n + \n def _set_nonblocking ( fd ) : \n flags = fcntl . fcntl ( fd , fcntl . F_GETFL ) \n flags = flags | os . O_NONBLOCK", "msg": "asyncio . unix_events : Move import statement to match tulip code\n"}
{"diff": "a / lib / ansible / inventory / yaml . py \n  b / lib / ansible / inventory / yaml . py \n \n from ansible . inventory . expand_hosts import detect_range \n from ansible . inventory . expand_hosts import expand_hostname_range \n from ansible . parsing . utils . addresses import parse_address \n + from ansible . compat . six import string_types \n \n class InventoryParser ( object ) : \n \" \" \" \n def _parse_groups ( self , group , group_data ) : \n self . groups [ group ] = Group ( name = group ) \n \n if isinstance ( group_data , dict ) : \n + # make sure they are dicts \n + for section in [ ' vars ' , ' children ' , ' hosts ' ] : \n + if section in group_data and isinstance ( group_data [ section ] , string_types ) : \n + group_data [ section ] = { group_data [ section ] : None } \n + \n if ' vars ' in group_data : \n for var in group_data [ ' vars ' ] : \n if var ! = ' ansible_group_priority ' :", "msg": "made format more flexible and allow for non dict entries\n"}
{"diff": "a / numpy / core / setup . py \n  b / numpy / core / setup . py \n def generate_config_h ( ext , build_dir ) : \n tc = generate_testcode ( target ) \n from distutils import sysconfig \n python_include = sysconfig . get_python_inc ( ) \n + python_h = join ( python_include , ' Python . h ' ) \n + if not os . path . isfile ( python_h ) : \n + raise SystemError , \\ \n + \" Non - existing % s . Perhaps you need to install \" \\ \n + \" python - dev | python - devel . \" % ( python_h ) \n result = config_cmd . try_run ( tc , include_dirs = [ python_include ] , \n library_dirs = default_lib_dirs ) \n if not result : \n - raise \" ERROR : Failed to test configuration \" \n + raise SystemError , \" Failed to test configuration . \" \\ \n + \" See previous error messages for more information . \" \n \n # Python 2 . 3 causes a segfault when \n # trying to re - acquire the thread - state", "msg": "Improved error message for missing Python . h .\n"}
{"diff": "a / lib / ansible / runner / action_plugins / fetch . py \n  b / lib / ansible / runner / action_plugins / fetch . py \n def run ( self , conn , tmp , module_name , module_args , inject ) : \n f . close ( ) \n new_md5 = utils . md5 ( dest ) \n if new_md5 ! = remote_md5 : \n - result = dict ( failed = True , md5sum = new_md5 , msg = \" md5 mismatch \" , file = source ) \n + result = dict ( failed = True , md5sum = new_md5 , msg = \" md5 mismatch \" , file = source , dest = dest ) \n return ReturnData ( conn = conn , result = result ) \n - result = dict ( changed = True , md5sum = new_md5 ) \n + result = dict ( changed = True , md5sum = new_md5 , dest = dest ) \n return ReturnData ( conn = conn , result = result ) \n else : \n - result = dict ( changed = False , md5sum = local_md5 , file = source ) \n + result = dict ( changed = False , md5sum = local_md5 , file = source , dest = dest ) \n return ReturnData ( conn = conn , result = result )", "msg": "Add destination path to fetch result\n"}
{"diff": "a / zipline / gens / utils . py \n  b / zipline / gens / utils . py \n \n # \n - # Copyright 2012 Quantopian , Inc . \n + # Copyright 2013 Quantopian , Inc . \n # \n # Licensed under the Apache License , Version 2 . 0 ( the \" License \" ) ; \n # you may not use this file except in compliance with the License .", "msg": "MAINT : Updates copyright year on gens . utils module .\n"}
{"diff": "a / youtube_dl / utils . py \n  b / youtube_dl / utils . py \n \n import StringIO \n \n std_headers = { \n - ' User - Agent ' : ' Mozilla / 5 . 0 ( X11 ; Linux x86_64 ; rv : 5 . 0 . 1 ) Gecko / 20100101 Firefox / 5 . 0 . 1 ' , \n + ' User - Agent ' : ' iTunes / 10 . 6 . 1 ' , \n ' Accept - Charset ' : ' ISO - 8859 - 1 , utf - 8 ; q = 0 . 7 , * ; q = 0 . 7 ' , \n ' Accept ' : ' text / html , application / xhtml + xml , application / xml ; q = 0 . 9 , * / * ; q = 0 . 8 ' , \n ' Accept - Encoding ' : ' gzip , deflate ' ,", "msg": "Use an User - Agent that will allow downloading from blip . tv fixes\n"}
{"diff": "a / tornado / web . py \n  b / tornado / web . py \n def __init__ ( self , application , request , * * kwargs ) : \n self . ui [ \" modules \" ] = self . ui [ \" _modules \" ] \n self . clear ( ) \n # Check since connection is not available in WSGI \n - if hasattr ( self . request , \" connection \" ) : \n + if getattr ( self . request , \" connection \" , None ) : \n self . request . connection . stream . set_close_callback ( \n self . on_connection_close ) \n self . initialize ( * * kwargs )", "msg": "Better connection check in RequestHandler\n"}
{"diff": "a / celery / bin / beat . py \n  b / celery / bin / beat . py \n \n \n . . cmdoption : : - - pidfile \n \n - Optional file used to store the process pid . \n + File used to store the process pid . Defaults to ` celerybeat . pid ` . \n \n The program won ' t start if this file already exists \n and the pid is still alive .", "msg": "Specify default value for pidfile option of celery beat . ( )\n"}
{"diff": "a / zephyr / lib / bugdown / __init__ . py \n  b / zephyr / lib / bugdown / __init__ . py \n def sanitize_url ( self , url ) : \n See the docstring on markdown . inlinepatterns . LinkPattern . sanitize_url . \n \" \" \" \n url = url . replace ( ' ' , ' % 20 ' ) \n - if not self . markdown . safeMode : \n - # Return immediately bipassing parsing . \n - return url \n \n try : \n scheme , netloc , path , params , query , fragment = url = urlparse . urlparse ( url )", "msg": "bugdown : Remove code path to bypass sanitize_url\n"}
{"diff": "a / setupext . py \n  b / setupext . py \n class Png ( SetupPackage ) : \n name = \" png \" \n \n def check ( self ) : \n + status , output = getstatusoutput ( \" libpng - config - - version \" ) \n + if status = = 0 : \n + version = output \n + else : \n + version = None \n + \n try : \n return self . _check_for_pkg_config ( \n ' libpng ' , ' png . h ' , \n - min_version = ' 1 . 2 ' ) \n + min_version = ' 1 . 2 ' , version = version ) \n except CheckFailed as e : \n self . __class__ . found_external = False \n return str ( e ) + ' Using unknown version . ' \n def get_extension ( self ) : \n ] \n ext = make_extension ( ' matplotlib . _png ' , sources ) \n pkg_config . setup_extension ( \n - ext , ' libpng ' , default_libraries = [ ' png ' , ' z ' ] ) \n + ext , ' libpng ' , default_libraries = [ ' png ' , ' z ' ] , \n + alt_exec = ' libpng - config - - ldflags ' ) \n Numpy ( ) . add_flags ( ext ) \n CXX ( ) . add_flags ( ext ) \n return ext", "msg": "Add ability to use libpng - config to get linker flags\n"}
{"diff": "a / lib / ansible / modules / web_infrastructure / supervisorctl . py \n  b / lib / ansible / modules / web_infrastructure / supervisorctl . py \n def take_action_on_processes ( processes , status_filter , action , expected_result ) : \n if module . check_mode : \n module . exit_json ( changed = True ) \n for process_name in to_take_action_on : \n - rc , out , err = run_supervisorctl ( action , process_name ) \n + rc , out , err = run_supervisorctl ( action , process_name , check_rc = True ) \n if ' % s : % s ' % ( process_name , expected_result ) not in out : \n module . fail_json ( msg = out ) \n \n module . exit_json ( changed = True , name = name , state = state , affected = to_take_action_on ) \n \n if state = = ' restarted ' : \n - rc , out , err = run_supervisorctl ( ' update ' ) \n + rc , out , err = run_supervisorctl ( ' update ' , check_rc = True ) \n processes = get_matched_processes ( ) \n take_action_on_processes ( processes , lambda s : True , ' restart ' , ' started ' )", "msg": "Better error handling in supervisorctl module .\n"}
{"diff": "a / Lib / test / support . py \n  b / Lib / test / support . py \n def requires ( resource , msg = None ) : \n return \n if not is_resource_enabled ( resource ) : \n if msg is None : \n - msg = \" Use of the ` % s ' resource not enabled \" % resource \n + msg = \" Use of the % r resource not enabled \" % resource \n raise ResourceDenied ( msg ) \n \n def requires_linux_version ( * min_version ) : \n def check_valid_file ( fn ) : \n f = check_valid_file ( fn ) \n if f is not None : \n return f \n - raise TestFailed ( ' invalid resource \" % s \" ' % fn ) \n + raise TestFailed ( ' invalid resource % r ' % fn ) \n \n \n class WarningsRecorder ( object ) : \n def transient_internet ( resource_name , * , timeout = 30 . 0 , errnos = ( ) ) : \n ( ' WSANO_DATA ' , 11004 ) , \n ] \n \n - denied = ResourceDenied ( \" Resource ' % s ' is not available \" % resource_name ) \n + denied = ResourceDenied ( \" Resource % r is not available \" % resource_name ) \n captured_errnos = errnos \n gai_errnos = [ ] \n if not captured_errnos :", "msg": "Always use repr for regrtest resources names\n"}
{"diff": "a / IPython / core / inputsplitter . py \n  b / IPython / core / inputsplitter . py \n def remove_comments ( src ) : \n \n def get_input_encoding ( ) : \n \" \" \" Return the default standard input encoding . \" \" \" \n - return getattr ( sys . stdin , ' encoding ' , ' ascii ' ) \n + # There are strange environments for which sys . stdin . encoding is None . We \n + # ensure that a valid encoding is returned . \n + encoding = getattr ( sys . stdin , ' encoding ' , None ) \n + if encoding is None : \n + encoding = ' ascii ' \n + return encoding \n \n # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - \n # Classes and functions", "msg": "Made blockbreakers ' input encoding detection more robust to strange\n"}
{"diff": "a / lib / ansible / playbook / task . py \n  b / lib / ansible / playbook / task . py \n def __init__ ( self , play , ds , module_vars = None , additional_conditions = None ) : \n self . args = ds . get ( ' args ' , { } ) \n \n if self . sudo : \n - self . sudo_user = ds . get ( ' sudo_user ' , play . sudo_user ) \n + self . sudo_user = utils . template ( play . basedir , ds . get ( ' sudo_user ' , play . sudo_user ) , play . vars ) \n self . sudo_pass = ds . get ( ' sudo_pass ' , play . playbook . sudo_pass ) \n else : \n self . sudo_user = None", "msg": "Added variable expansion to task sudo_user parameter\n"}
{"diff": "a / Lib / test / test_hotshot . py \n  b / Lib / test / test_hotshot . py \n def get_events_wotime ( self ) : \n \n def check_events ( self , expected ) : \n events = self . get_events_wotime ( ) \n + if not __debug__ : \n + # Running under - O , so we don ' t get LINE events \n + expected = [ ev for ev in expected if ev [ 0 ] ! = LINE ] \n if events ! = expected : \n self . fail ( \n \" events did not match expectation ; got : \\ n % s \\ nexpected : \\ n % s \"", "msg": "Do not expect line number events when running under \" python - O \" .\n"}
{"diff": "a / zerver / lib / event_queue . py \n  b / zerver / lib / event_queue . py \n def load_event_queues ( ) : \n def send_restart_events ( ) : \n event = dict ( type = ' restart ' , server_generation = settings . SERVER_GENERATION ) \n for client in clients . itervalues ( ) : \n - # All clients get restart events \n + if not client . accepts_event_type ( ' restart ' ) : \n + continue \n client . add_event ( event . copy ( ) ) \n \n def setup_event_queue ( ) :", "msg": "Don ' t send server restart events to clients that don ' t request them .\n"}
{"diff": "a / youtube_dl / FileDownloader . py \n  b / youtube_dl / FileDownloader . py \n def extract_info ( self , url ) : \n raise \n if not suitable_found : \n self . trouble ( u ' ERROR : no suitable InfoExtractor : % s ' % url ) \n + def extract_info_iterable ( self , urls ) : \n + ' ' ' \n + Return the videos founded for the urls \n + ' ' ' \n + results = [ ] \n + for url in urls : \n + results . extend ( self . extract_info ( url ) ) \n + return results \n \n def process_info ( self , info_dict ) : \n \" \" \" Process a single dictionary returned by an InfoExtractor . \" \" \"", "msg": "Add a method for extracting info from a list of urls\n"}
{"diff": "a / pavement . py \n  b / pavement . py \n \n \" 2 . 6 \" : [ \" / Library / Frameworks / Python . framework / Versions / 2 . 6 / bin / python \" ] \n } \n \n - SSE3_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' } \n - SSE2_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' } \n + SSE3_CFG = { ' ATLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' } \n + SSE2_CFG = { ' ATLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' } \n NOSSE_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ nosse ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ nosse ' } \n \n SITECFG = { \" sse2 \" : SSE2_CFG , \" sse3 \" : SSE3_CFG , \" nosse \" : NOSSE_CFG }", "msg": "REL : fix atlas detection for SSE2 / SSE3 wininst .\n"}
{"diff": "a / zerver / lib / subdomains . py \n  b / zerver / lib / subdomains . py \n def is_subdomain_root_or_alias ( request ) : \n subdomain = _extract_subdomain ( request ) \n return not subdomain or subdomain in settings . ROOT_SUBDOMAIN_ALIASES \n \n - def check_subdomain ( realm_subdomain , user_subdomain ) : \n - # type : ( Optional [ Text ] , Text ) - > bool \n - if realm_subdomain is not None : \n - if realm_subdomain ! = user_subdomain : \n - return False \n - return True \n - \n def user_matches_subdomain ( realm_subdomain , user_profile ) : \n # type : ( Optional [ Text ] , UserProfile ) - > bool \n - return check_subdomain ( realm_subdomain , user_profile . realm . subdomain ) \n + if realm_subdomain is None : \n + return True \n + return user_profile . realm . subdomain = = realm_subdomain \n \n def is_root_domain_available ( ) : \n # type : ( ) - > bool", "msg": "subdomains : Complete the refactor to user_matches_subdomain .\n"}
{"diff": "a / IPython / core / pylabtools . py \n  b / IPython / core / pylabtools . py \n def pylab_activate ( user_ns , gui = None , import_all = True ) : \n matplotlib . interactive ( True ) \n \n print \" \" \" \n - Welcome to pylab , a matplotlib - based Python environment . \n - Backend in use : % s \n + Welcome to pylab , a matplotlib - based Python environment [ backend : % s ] . \n For more information , type ' help ( pylab ) ' . \" \" \" % backend \n \n return gui", "msg": "Small fix to info message when pylab starts .\n"}
{"diff": "a / homeassistant / components / volumio / media_player . py \n  b / homeassistant / components / volumio / media_player . py \n async def async_media_pause ( self ) : \n else : \n await self . send_volumio_msg ( \" commands \" , params = { \" cmd \" : \" pause \" } ) \n \n + async def async_media_stop ( self ) : \n + \" \" \" Send media_stop command to media player . \" \" \" \n + await self . send_volumio_msg ( \" commands \" , params = { \" cmd \" : \" stop \" } ) \n + \n async def async_set_volume_level ( self , volume ) : \n \" \" \" Send volume_up command to media player . \" \" \" \n await self . send_volumio_msg (", "msg": "Add media_stop for volumio integration ( )\n"}
{"diff": "a / lib / ansible / playbook / play . py \n  b / lib / ansible / playbook / play . py \n def _get_vars ( self ) : \n raise errors . AnsibleError ( \" ' vars_prompt ' item is missing ' name : ' \" ) \n \n vname = var [ ' name ' ] \n - prompt = \" % s : \" % var . get ( \" prompt \" , vname ) \n + prompt = util . template ( None , \" % s : \" % var . get ( \" prompt \" , vname ) , self . vars ) \n private = var . get ( \" private \" , True ) \n \n confirm = var . get ( \" confirm \" , False )", "msg": "Template the variable prompt to customize the message\n"}
{"diff": "a / examples / pylab_examples / quiver_demo . py \n  b / examples / pylab_examples / quiver_demo . py \n \n # 6 \n plt . figure ( ) \n M = np . zeros ( U . shape , dtype = ' bool ' ) \n - M [ U . shape [ 0 ] / 3 : 2 * U . shape [ 0 ] / 3 , \n - U . shape [ 1 ] / 3 : 2 * U . shape [ 1 ] / 3 ] = True \n + XMaskStart = int ( U . shape [ 0 ] / 3 ) \n + YMaskStart = int ( U . shape [ 1 ] / 3 ) \n + XMaskStop = int ( 2 * U . shape [ 0 ] / 3 ) \n + YMaskStop = int ( 2 * U . shape [ 1 ] / 3 ) \n + \n + M [ XMaskStart : XMaskStop , \n + YMaskStart : YMaskStop ] = True \n U = ma . masked_array ( U , mask = M ) \n V = ma . masked_array ( V , mask = M ) \n Q = plt . quiver ( U , V )", "msg": "Quiver_demo cast indices to int before indexing numpy array\n"}
{"diff": "a / Lib / test / test_time . py \n  b / Lib / test / test_time . py \n def test_asctime ( self ) : \n def test_asctime_bounding_check ( self ) : \n self . _bounds_checking ( time . asctime ) \n \n + @ unittest . skipIf ( not hasattr ( time , \" tzset \" ) , \n + \" time module has no attribute tzset \" ) \n def test_tzset ( self ) : \n - if not hasattr ( time , \" tzset \" ) : \n - return # Can ' t test this ; don ' t want the test suite to fail \n \n from os import environ", "msg": "Use skipIf instead of a return when attribute doesn ' t exist .\n"}
{"diff": "a / api / zulip / __init__ . py \n  b / api / zulip / __init__ . py \n def generate_option_group ( parser ) : \n group . add_option ( ' - v ' , ' - - verbose ' , \n action = ' store_true ' , \n help = ' Provide detailed output . ' ) \n + group . add_option ( ' - - client ' , \n + action = ' store ' , \n + default = \" API : Python \" , \n + help = optparse . SUPPRESS_HELP ) \n \n return group \n \n def init_from_options ( options ) : \n return Client ( email = options . email , api_key = options . api_key , config_file = options . config_file , \n - verbose = options . verbose , site = options . site ) \n + verbose = options . verbose , site = options . site , client = options . client ) \n \n class Client ( object ) : \n def __init__ ( self , email = None , api_key = None , config_file = None ,", "msg": "api : Add support for specifying client using zulip . init_from_options .\n"}

{"diff": "a / main.go \n  b / main.go \n@@ -173,8 +173,9 @@ func main() { \n if !shouldrun { \n statusLbl = \"(skipping)\" \n } \n - fmt.Printf(\"----- DNS Provider: %s%s\\n\", prov, statusLbl) \n + fmt.Printf(\"----- DNS Provider: %s... %s\", prov, statusLbl) \n if !shouldrun { \n + fmt.Println() \n continue \n } \n dsp, ok := dsps[prov] \n @@ -183,11 +184,17 @@ func main() { \n } \n corrections, err := dsp.GetDomainCorrections(dc) \n if err != nil { \n + fmt.Println(\"ERROR\") \n anyErrors = true \n fmt.Printf(\"Error getting corrections: %s\\n\", err) \n continue DomainLoop \n } \n totalCorrections += len(corrections) \n + plural := \"s\" \n + if len(corrections) == 1 { \n + plural = \"\" \n + } \n + fmt.Printf(\"%d correction%s\\n\", len(corrections), plural) \n anyErrors = printOrRunCorrections(corrections, command) || anyErrors \n } \n if run := shouldRunProvider(domain.Registrar, domain); !run {", "msg": "Printing correction count for each provider as we go."}
{"diff": "a / main.go \n  b / main.go \n@@ -69,13 +69,13 @@ func main() { \n } \n if flag.NArg() != 1 { \n - fmt.Println(\"Usage: dnscontrol [options] cmd\") \n - fmt.Println(\" cmd:\") \n - fmt.Println(\" preview: Show changed that would happen.\") \n - fmt.Println(\" push: Make changes for real.\") \n - fmt.Println(\" version: Print program version string.\") \n - fmt.Println(\" print: Print compiled data.\") \n - fmt.Println(\"\") \n + fmt.Println(`Usage: dnscontrol [options] cmd \n + cmd: \n + preview: Show changed that would happen. \n + push: Make changes for real. \n + version: Print program version string. \n + print: Print compiled data. \n + `) \n flag.PrintDefaults() \n return \n }", "msg": "Redundant use of multiple fmt.Println commands\nChanged multiple fmt.Println(\"\") commands to one using backticks"}
{"diff": "a / pkg/acme/acme.go \n  b / pkg/acme/acme.go \n@@ -23,6 +23,7 @@ type CertConfig struct { \n CertName string `json:\"cert_name\"` \n Names []string `json:\"names\"` \n UseECC bool `json:\"use_ecc\"` \n + MustStaple bool `json:\"must_staple\"` \n } \n type Client interface { \n @@ -103,7 +104,7 @@ func (c *certManager) IssueOrRenewCert(cfg *CertConfig, renewUnder int, verbose \n var client *acme.Client \n var action = func() (*acme.CertificateResource, error) { \n - return client.ObtainCertificate(cfg.Names, true, nil, true) \n + return client.ObtainCertificate(cfg.Names, true, nil, cfg.MustStaple) \n } \n if existing == nil { \n @@ -125,7 +126,7 @@ func (c *certManager) IssueOrRenewCert(cfg *CertConfig, renewUnder int, verbose \n } else { \n log.Println(\"Renewing cert\") \n action = func() (*acme.CertificateResource, error) { \n - return client.RenewCertificate(*existing, true, true) \n + return client.RenewCertificate(*existing, true, cfg.MustStaple) \n } \n } \n }", "msg": "add must_staple option to cert. Default false"}
{"diff": "a / providers/azuredns/azureDnsProvider.go \n  b / providers/azuredns/azureDnsProvider.go \n@@ -518,16 +518,19 @@ func (a *azureDNSProvider) fetchRecordSets(zoneName string) ([]*adns.RecordSet, \n var records []*adns.RecordSet \n ctx, cancel := context.WithTimeout(context.Background(), 6000*time.Second) \n defer cancel() \n - recordsIterator, recordsErr := a.recordsClient.ListAllByDNSZoneComplete(ctx, *a.resourceGroup, zoneName, to.Int32Ptr(1000), \"\") \n + recordsIterator, recordsErr := a.recordsClient.ListAllByDNSZone(ctx, *a.resourceGroup, zoneName, to.Int32Ptr(1000), \"\") \n if recordsErr != nil { \n return nil, recordsErr \n } \n - recordsResult := recordsIterator.Response() \n + for recordsIterator.NotDone() { \n + recordsResult := recordsIterator.Response() \n for _, r := range *recordsResult.Value { \n record := r \n records = append(records, &record) \n } \n + recordsIterator.NextWithContext(ctx) \n + } \n return records, nil \n }", "msg": "Fix for incomplete results from Azure DNS in fetchRecordSets"}
{"diff": "a / integrationTest/integration_test.go \n  b / integrationTest/integration_test.go \n@@ -786,7 +786,7 @@ func makeTests(t *testing.T) []*TestGroup { \n tc(\"Change Weight\", srv(\"_sip._tcp\", 52, 62, 7, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \"foo4.com.\")), \n tc(\"Change Port\", srv(\"_sip._tcp\", 52, 62, 72, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \"foo4.com.\")), \n ), \n - testgroup(\"SRV w/ null target\", not(\"EXOSCALE\", \"HEXONET\", \"NAMEDOTCOM\"), \n + testgroup(\"SRV w/ null target\", requires(providers.CanUseSRV), not(\"EXOSCALE\", \"HEXONET\", \"NAMEDOTCOM\"), \n tc(\"Null Target\", srv(\"_sip._tcp\", 52, 62, 72, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \".\")), \n ),", "msg": "fix: only run SRV null target test when supported"}
{"diff": "a / providers/hetzner/api.go \n  b / providers/hetzner/api.go \n@@ -157,6 +157,7 @@ func (api *hetznerProvider) getZone(name string) (*zone, error) { \n } \n func (api *hetznerProvider) request(endpoint string, method string, request interface{}, target interface{}) error { \n + for { \n var requestBody io.Reader \n if request != nil { \n requestBodySerialised, err := json.Marshal(request) \n @@ -171,7 +172,6 @@ func (api *hetznerProvider) request(endpoint string, method string, request inte \n } \n req.Header.Add(\"Auth-API-Token\", api.apiKey) \n - for { \n api.requestRateLimiter.beforeRequest() \n resp, err := http.DefaultClient.Do(req) \n api.requestRateLimiter.afterRequest()", "msg": "HETZNER: fix retry of POST/PUT requests -- rebuild request body\nPreviously for any retry the request body was already consumed and\nthe server received an empty body."}
{"diff": "a / providers/dnsmadeeasy/api.go \n  b / providers/dnsmadeeasy/api.go \n@@ -12,13 +12,13 @@ type dnsMadeEasyProvider struct { \n } \n func newProvider(apiKey string, secretKey string, sandbox bool, debug bool) *dnsMadeEasyProvider { \n - fmt.Println(\"creating DNSMADEEASY provider for sandbox\") \n - \n baseURL := baseURLV2_0 \n if sandbox { \n baseURL = sandboxBaseURLV2_0 \n } \n + fmt.Printf(\"Creating DNSMADEEASY provider for %q\\n\", baseURL) \n + \n return &dnsMadeEasyProvider{ \n restAPI: &dnsMadeEasyRestAPI{ \n apiKey: apiKey,", "msg": "Show actual URL to use instead of just \"sandbox\"\n* Show actual URL to use instead of just \"sandbox\"\nInstead of stating \"sandbox\", sometimes incorrectly if sandbox is false, just output the actual URL that will be used.\n* Replace Println with Printf"}
{"diff": "a / providers/powerdns/powerdnsProvider.go \n  b / providers/powerdns/powerdnsProvider.go \n@@ -18,6 +18,7 @@ import ( \n var features = providers.DocumentationNotes{ \n providers.CanUseAlias: providers.Can(\"Needs to be enabled in PowerDNS first\", \"https://doc.powerdns.com/authoritative/guides/alias.html\"), \n providers.CanUseCAA: providers.Can(), \n + providers.CanUseDS: providers.Can(), \n providers.CanUsePTR: providers.Can(), \n providers.CanUseSRV: providers.Can(), \n providers.CanUseTLSA: providers.Can(), \n @@ -263,6 +264,8 @@ func toRecordConfig(domain string, r zones.Record, ttl int, name string, rtype s \n return rc, rc.SetTarget(dnsutil.AddOrigin(content, domain)) \n case \"CAA\": \n return rc, rc.SetTargetCAAString(content) \n + case \"DS\": \n + return rc, rc.SetTargetDSString(content) \n case \"MX\": \n return rc, rc.SetTargetMXString(content) \n case \"SRV\":", "msg": "POWERDNS: Add support for DS records"}
{"diff": "a / pkg/normalize/validate.go \n  b / pkg/normalize/validate.go \n@@ -573,7 +573,7 @@ func checkLabelHasMultipleTTLs(records []*models.RecordConfig) (errs []error) { \n for _, r := range records { \n label := fmt.Sprintf(\"%s %s\", r.GetLabelFQDN(), r.Type) \n - // if we have more records for a given label, append their TTLs here \n + // collect the TTLs at this label. \n m[label] = append(m[label], r.TTL) \n }", "msg": "Emit warning in case of label having multiple TTLs\nAn RRSet (=label) consisting of multiple records with different TTLs is\nsomething not supported by most providers, and should be avoided.\nFurthermore it is deprecated in rfc2181#section-5.2\nEmit a warning for now during validation, eventually turning it into a full-blown error.\nFixes"}
{"diff": "a / providers/hostingde/hostingdeProvider.go \n  b / providers/hostingde/hostingdeProvider.go \n@@ -159,7 +159,7 @@ func (hp *hostingdeProvider) GetDomainCorrections(dc *models.DomainConfig) ([]*m \n var DnsSecOptions *dnsSecOptions = nil \n // ensure that publishKsk is set for domains with AutoDNSSec \n - if existingAutoDNSSecEnabled == desiredAutoDNSSecEnabled == true { \n + if existingAutoDNSSecEnabled && desiredAutoDNSSecEnabled { \n CurrentDnsSecOptions, err := hp.getDNSSECOptions(zone.ZoneConfig.ID) \n if err != nil { \n return nil, err", "msg": "HOSTINGDE: Fix dnssec error resulting from non-go-conformant comparison of three values"}
{"diff": "a / internal/terminal/manager.go \n  b / internal/terminal/manager.go \n@@ -155,7 +155,7 @@ func (tm *manager) Create(ctx context.Context, logger log.Logger, key store.Key, \n err = rc.Stream(opts) \n if err != nil { \n t.SetExitMessage(fmt.Sprintf(\"%s\", err)) \n - logger.Errorf(\"streaming: %+v\", err) \n + logger.Debugf(\"streaming: %+v\", err) \n } \n t.Stop() \n logger.Debugf(\"stopping stream command\")", "msg": "error logging should be debug to avoid spam"}
{"diff": "a / internal/oplog/oplog_tail.go \n  b / internal/oplog/oplog_tail.go \n@@ -114,7 +114,13 @@ func (ot *OplogTail) tail() { \n } \n result := bson.Raw{} \n if iter.Next(&result) { \n + oplog := mdbstructs.Oplog{} \n + err := result.Unmarshal(&oplog) \n + if err != nil { \n + continue \n + } \n ot.dataChan <- result.Data \n + ot.lastOplogEntry = oplog \n continue \n } \n if iter.Timeout() {", "msg": "Store last-tailed oplog entry to the tailer can resume properly"}
{"diff": "a / cli/pbm-agent/main.go \n  b / cli/pbm-agent/main.go \n@@ -141,10 +141,6 @@ func main() { \n } \n } \n - if di.Username == \"\" || di.Password == \"\" { \n - log.Fatalf(\"Not enough data for authentication. Please set correctly either `dns` or `username`/`password` options\") \n - } \n - \n // Test the connection to the MongoDB server before starting the agent. \n // We don't want to wait until backup/restore start to know there is an error with the \n // connection options", "msg": "Remove login/pass checks to work with the no-auth-enabled mongo"}
{"diff": "a / pbm/pbm.go \n  b / pbm/pbm.go \n@@ -119,7 +119,7 @@ func New(ctx context.Context, uri, appName string) (*PBM, error) { \n curi, err := url.Parse(uri) \n if err != nil { \n - return nil, errors.Wrap(err, \"parse mongo-uri\") \n + return nil, errors.Wrapf(err, \"parse mongo-uri '%s'\", uri) \n } \n // Preserving `replicaSet` parameter will causes an error while connecting to the ConfigServer (mismatched replicaset names) \n @@ -129,7 +129,7 @@ func New(ctx context.Context, uri, appName string) (*PBM, error) { \n curi.Host = chost[1] \n pbm.Conn, err = connect(ctx, curi.String(), appName) \n if err != nil { \n - return nil, errors.Wrap(err, \"create mongo connection to configsvr\") \n + return nil, errors.Wrapf(err, \"create mongo connection to configsvr with connection string '%s'\", curi) \n } \n return pbm, errors.Wrap(pbm.setupNewDB(), \"setup a new backups db\")", "msg": "Add more context to connection errors"}
{"diff": "a / cmd/pbm/backup.go \n  b / cmd/pbm/backup.go \n@@ -57,7 +57,10 @@ func backup(cn *pbm.PBM, bcpName, compression string) (string, error) { \n return \"\", err \n } \n - storeString := \"s3://\" \n + storeString := \"\" \n + switch stg.Type { \n + case pbm.StorageS3: \n + storeString = \"s3://\" \n if stg.S3.EndpointURL != \"\" { \n storeString += stg.S3.EndpointURL + \"/\" \n } \n @@ -65,6 +68,9 @@ func backup(cn *pbm.PBM, bcpName, compression string) (string, error) { \n if stg.S3.Prefix != \"\" { \n storeString += \"/\" + stg.S3.Prefix \n } \n + case pbm.StorageFilesystem: \n + storeString = stg.Filesystem.Path \n + } \n return storeString, nil \n }", "msg": "ctl: Fix Filesystem store output for the backup command"}
{"diff": "a / e2e-tests/pkg/pbm/docker.go \n  b / e2e-tests/pkg/pbm/docker.go \n@@ -19,7 +19,7 @@ type Docker struct { \n } \n func NewDocker(ctx context.Context, host string) (*Docker, error) { \n - cn, err := docker.NewClient(host, \"1.40\", nil, nil) \n + cn, err := docker.NewClient(host, \"1.39\", nil, nil) \n if err != nil { \n return nil, errors.Wrap(err, \"docker client\") \n }", "msg": "Tests: downgrade the docker-api version to fit the CI env"}
{"diff": "a / cmd/pbm-speed-test/main.go \n  b / cmd/pbm-speed-test/main.go \n@@ -24,7 +24,7 @@ func main() { \n sampleSizeF = tCmd.Flag(\"size-gb\", \"Set data size in GB. Default 1\").Short('s').Float64() \n compressType = tCmd.Flag(\"compression\", \"Compression type <none>/<gzip>/<snappy>/<lz4>/<s2>/<pgzip>\"). \n - Default(string(pbm.CompressionTypeGZIP)). \n + Default(string(pbm.CompressionTypeS2)). \n Enum(string(pbm.CompressionTypeNone), string(pbm.CompressionTypeGZIP), \n string(pbm.CompressionTypeSNAPPY), string(pbm.CompressionTypeLZ4), \n string(pbm.CompressionTypeS2), string(pbm.CompressionTypePGZIP),", "msg": "align default compression for speed-test with pbm"}
{"diff": "a / cli/backup.go \n  b / cli/backup.go \n@@ -219,6 +219,7 @@ type bcpReplDesc struct { \n LastWriteTS string `json:\"last_write_ts\" yaml:\"last_write_ts\"` \n LastTransitionTS string `json:\"last_transition_ts\" yaml:\"last_transition_ts\"` \n IsConfigSvr *bool `json:\"configsvr,omitempty\" yaml:\"configsvr,omitempty\"` \n + SecurityOpts *pbm.MongodOptsSec `json:\"security,omitempty\" yaml:\"security,omitempty\"` \n Error *string `json:\"error,omitempty\" yaml:\"error,omitempty\"` \n } \n @@ -276,6 +277,9 @@ func describeBackup(cn *pbm.PBM, b *descBcp) (fmt.Stringer, error) { \n if r.Error != \"\" { \n rv.Replsets[i].Error = &r.Error \n } \n + if r.MongodOpts.Security != nil { \n + rv.Replsets[i].SecurityOpts = r.MongodOpts.Security \n + } \n } \n return rv, err", "msg": "show security opts in descibe-backup\nIf there were any security (data-at-rest enc) options ob the backup node,\nthese will be displayed the describe-backup output"}
{"diff": "a / cli/restore.go \n  b / cli/restore.go \n@@ -162,7 +162,7 @@ func waitRestore(cn *pbm.PBM, m *pbm.RestoreMeta) error { \n } \n switch rmeta.Status { \n - case pbm.StatusDone: \n + case pbm.StatusDone, pbm.StatusPartlyDone: \n return nil \n case pbm.StatusError: \n return errRestoreFailed{fmt.Sprintf(\"operation failed with: %s\", rmeta.Error)}", "msg": "phys restore wait for partlyDone\nFor physical restore any of the `done` or `partlyDone` statuses is a success."}
{"diff": "a / e2e-tests/pkg/pbm/pbm_ctl.go \n  b / e2e-tests/pkg/pbm/pbm_ctl.go \n@@ -223,7 +223,7 @@ func (c *Ctl) CheckRestore(bcpName string, waitFor time.Duration) error { \n case pbm.StatusDone: \n return nil \n case pbm.StatusError: \n - errors.Errorf(\"failed with %s\", r.Error) \n + return errors.Errorf(\"failed with %s\", r.Error) \n } \n } \n }", "msg": "Fix CheckRestore function in e2e-tests\nAdd missing return statement if restore error has been found.\nWithout a return statement restore is not recognized as a failure until\nit times out (by default 25 minutes)."}
{"diff": "a / pbm/restore/physical.go \n  b / pbm/restore/physical.go \n@@ -1032,7 +1032,7 @@ func (r *PhysRestore) startMongo(opts ...string) error { \n const hbFrameSec = 60 * 2 \n func (r *PhysRestore) init(name string, opid pbm.OPID, l *log.Event) (err error) { \n - r.stg, err = r.cn.GetStorage(r.log) \n + r.stg, err = r.cn.GetStorage(l) \n if err != nil { \n return errors.Wrap(err, \"get storage\") \n }", "msg": "fix panic during phys restore S3 retries\nPhysical restore wasn't properly initialising the logger for storage.\nThat caused panics when S3 storage tried to log some actions."}
{"diff": "a / agent/snapshot.go \n  b / agent/snapshot.go \n@@ -126,11 +126,9 @@ func (a *Agent) Backup(cmd *pbm.BackupCmd, opid pbm.OPID, ep pbm.Epoch) { \n // try backup anyway \n l.Warning(\"define source backup: %v\", err) \n } else { \n + c = make(map[string]float64) \n for _, rs := range src.Replsets { \n - if rs.Name == nodeInfo.SetName { \n - c = map[string]float64{rs.Node: srcHostMultiplier} \n - break \n - } \n + c[rs.Node] = srcHostMultiplier \n } \n } \n }", "msg": "fix incr backups voting for all rs\nApply upvote for all replica sets, not primary (config server) only"}
{"diff": "a / cli/backup.go \n  b / cli/backup.go \n@@ -282,16 +282,20 @@ func describeBackup(cn *pbm.PBM, b *descBcp) (fmt.Stringer, error) { \n } \n if bcp.Size == 0 { \n + switch bcp.Status { \n + case pbm.StatusDone, pbm.StatusCancelled, pbm.StatusError: \n stg, err := cn.GetStorage(cn.Logger().NewEvent(\"\", \"\", \"\", primitive.Timestamp{})) \n if err != nil { \n return nil, errors.WithMessage(err, \"get storage\") \n } \n rv.Size, err = getLegacySnapshotSize(bcp, stg) \n - if err != nil { \n + if errors.Is(err, errMissedFile) && bcp.Status != pbm.StatusDone { \n + // canceled/failed backup can be incomplete. ignore \n return nil, errors.WithMessage(err, \"get snapshot size\") \n } \n } \n + } \n rv.Replsets = make([]bcpReplDesc, len(bcp.Replsets)) \n for i, r := range bcp.Replsets {", "msg": "describe-backup: ignore missed file for running backup"}
{"diff": "a / pbm/backup/physical.go \n  b / pbm/backup/physical.go \n@@ -181,6 +181,21 @@ func (b *Backup) doPhysical(ctx context.Context, bcp *pbm.BackupCmd, opid pbm.OP \n } \n } \n currOpts = append(currOpts, bson.E{\"srcBackupName\", pbm.BackupCursorName(src.Name)}) \n + } else { \n + // We don't need any previous incremental backup history if \n + // this is a base backup. So we can flush it to free up resources. \n + l.Debug(\"flush incremental backup history\") \n + cr, err := NewBackupCursor(b.node, l, bson.D{ \n + {\"disableIncrementalBackup\", true}, \n + }).create(ctx, cursorCreateRetries) \n + if err != nil { \n + l.Warning(\"flush incremental backup history error: %v\", err) \n + } else { \n + err = cr.Close(ctx) \n + if err != nil { \n + l.Warning(\"close cursor disableIncrementalBackup: %v\", err) \n + } \n + } \n } \n } \n cursor := NewBackupCursor(b.node, l, currOpts)", "msg": "flush incremental backup history\nWe don't need any previous incremental backup history when we start a\nbase backup. So we can flush it to free up resources."}
{"diff": "a / jsonschema.go \n  b / jsonschema.go \n@@ -25,7 +25,7 @@ type SchemaValidator struct{} \n func (v SchemaValidator) ValidateStruct(data interface{}, schemaType string) (ok bool, errs []string) { \n s, err := api.Schema(schemaType) \n if err != nil { \n - return false, []string{\"unknown schema type %s\", schemaType} \n + return false, []string{fmt.Sprintf(\"unknown schema type %s\", schemaType)} \n } \n ls := gojsonschema.NewBytesLoader(s)", "msg": "Fix error formatting (missing a call to Sprintf)\nJust a minor thing I noticed while skimming over the code..."}
{"diff": "a / nats/nats_test.go \n  b / nats/nats_test.go \n@@ -570,8 +570,8 @@ func TestCLIConsumerCopy(t *testing.T) { \n t.Fatalf(\"Expected pull1 to be pull-based, got %v\", pull1.Configuration()) \n } \n - if pull1.MaxAckPending() != 0 { \n - t.Fatalf(\"Expected pull1 to have 0 Ack outstanding, got %v\", pull1.MaxAckPending()) \n + if pull1.MaxAckPending() != 20000 { \n + t.Fatalf(\"Expected pull1 to have 20000 Ack outstanding, got %v\", pull1.MaxAckPending()) \n } \n }", "msg": "update tests for new JS defaults"}
{"diff": "a / nats/object_command.go \n  b / nats/object_command.go \n@@ -81,6 +81,7 @@ NOTE: This is an experimental feature. \n get.Arg(\"file\", \"The file to retrieve\").Required().StringVar(&c.file) \n get.Flag(\"output\", \"Override the output file name\").Short('O').StringVar(&c.overrideName) \n get.Flag(\"no-progress\", \"Disables progress bars\").Default(\"false\").BoolVar(&c.noProgress) \n + get.Flag(\"force\", \"Act without confirmation\").Short('f').BoolVar(&c.force) \n info := obj.Command(\"info\", \"Get information about a bucket or object\").Action(c.infoAction) \n info.Arg(\"bucket\", \"The bucket to act on\").Required().StringVar(&c.bucket)", "msg": "fix force for obj get"}
{"diff": "a / cli/stream_command.go \n  b / cli/stream_command.go \n@@ -1714,7 +1714,7 @@ func (c *streamCmd) prepareConfig(pc *kingpin.ParseContext) api.StreamConfig { \n var maxAge time.Duration \n if c.maxBytesLimit == 0 { \n - c.maxBytesLimit, err = askOneBytes(\"Message size limit\", \"-1\", \"Defines the combined size of all messages in a Stream, when exceeded oldest messages are removed, -1 for unlimited. Settable using --max-bytes\") \n + c.maxBytesLimit, err = askOneBytes(\"Stream size limit\", \"-1\", \"Defines the combined size of all messages in a Stream, when exceeded messages are removed or new ones are rejected, -1 for unlimited. Settable using --max-bytes\") \n kingpin.FatalIfError(err, \"invalid input\") \n if c.maxBytesLimit <= 0 {", "msg": "wording tweaks for max stream size question"}
{"diff": "a / cli/server_request_command.go \n  b / cli/server_request_command.go \n@@ -174,13 +174,18 @@ func (c *SrvRequestCmd) jsz(_ *fisk.ParseContext) error { \n } \n func (c *SrvRequestCmd) reqFilter() server.EventFilterOptions { \n - return server.EventFilterOptions{ \n + opt := server.EventFilterOptions{ \n Name: c.name, \n Host: c.host, \n Cluster: c.cluster, \n Tags: c.tags, \n Domain: opts.Config.JSDomain(), \n } \n + if opts.Config != nil { \n + opt.Domain = opts.Config.JSDomain() \n + } \n + \n + return opt \n } \n func (c *SrvRequestCmd) accountz(_ *fisk.ParseContext) error {", "msg": "fix nil pointer issue when constructing server filters"}
{"diff": "a / cli/reply_command.go \n  b / cli/reply_command.go \n@@ -52,6 +52,14 @@ This will request the weather for london when invoked as: \n nats request weather.london '' \n +Use {{.Request}} to access the request body within the --command \n + \n +The command gets also spawned with two ENVs: \n + NATS_REQUEST_SUBJECT \n + NATS_REQUEST_BODY \n + \n + nats reply 'echo' --command=\"printenv NATS_REQUEST_BODY\" \n + \n The body and Header values of the messages may use Go templates to create unique messages. \n nats reply test \"Message {{Count}} @ {{Time}}\"", "msg": "add some help comments to use request body aand subject within reply --command"}
{"diff": "a / cli/server_account_command.go \n  b / cli/server_account_command.go \n@@ -206,6 +206,7 @@ func (c *srvAccountCommand) renderExport(exp *server.ExtExport) { \n } else { \n fmt.Printf(\" Accounts:\\n\") \n } \n + fmt.Println() \n var list []string \n var cnt int \n @@ -228,6 +229,7 @@ func (c *srvAccountCommand) renderExport(exp *server.ExtExport) { \n } else { \n fmt.Printf(\" Revocations:\\n\") \n } \n + fmt.Println() \n var list []string \n var cnt int", "msg": "minor ux updates for server account info"}
{"diff": "a / cmd/rqlited/main.go \n  b / cmd/rqlited/main.go \n@@ -280,24 +280,31 @@ func main() { \n } \n func determineJoinAddresses() ([]string, error) { \n - advAddr := raftAddr \n - if raftAdv != \"\" { \n - advAddr = raftAdv \n + apiAdv := httpAddr \n + if httpAdv != \"\" { \n + apiAdv = httpAdv \n + } \n + \n + var addrs []string \n + if joinAddr != \"\" { \n + // An explicit join address is first priority. \n + addrs = append(addrs, joinAddr) \n } \n - addrs := []string{} \n if discoID != \"\" { \n c := disco.New(discoURL) \n - r, err := c.Register(discoID, advAddr) \n + r, err := c.Register(discoID, apiAdv) \n if err != nil { \n return nil, err \n } \n - addrs = append(addrs, r.Nodes...) \n + for _, a := range r.Nodes { \n + if a != apiAdv { \n + // Only other nodes can be joined. \n + addrs = append(addrs, a) \n + } \n } \n - \n - if joinAddr != \"\" { \n - addrs = append([]string{joinAddr}, addrs...) \n } \n + \n return addrs, nil \n }", "msg": "Supply HTTP advertised address to disco service"}
{"diff": "a / disco/client.go \n  b / disco/client.go \n@@ -74,7 +74,7 @@ func (c *Client) Register(id, addr string) (*Response, error) { \n c.logger.Printf(\"discovery client successfully registered %s at %s\", addr, url) \n return r, nil \n case http.StatusMovedPermanently: \n - url = c.registrationURL(resp.Header.Get(\"location\"), id) \n + url = resp.Header.Get(\"location\") \n c.logger.Println(\"discovery client redirecting to\", url) \n continue \n default:", "msg": "Fix reference to redirect URL in disco client"}
{"diff": "a / cmd/rqlited/main.go \n  b / cmd/rqlited/main.go \n@@ -227,6 +227,8 @@ func main() { \n } else { \n log.Println(\"node is already member of cluster, ignoring any join requests\") \n } \n + } else { \n + log.Println(\"no join addresses available\") \n } \n // Publish to the cluster the mapping between this Raft address and API address.", "msg": "More join logging"}
{"diff": "a / tcp/mux_test.go \n  b / tcp/mux_test.go \n@@ -2,6 +2,7 @@ package tcp \n import ( \n \"bytes\" \n + \"crypto/tls\" \n \"io\" \n \"io/ioutil\" \n \"log\" \n @@ -176,10 +177,24 @@ func TestTLSMux(t *testing.T) { \n key := x509.KeyFile() \n defer os.Remove(key) \n - _, err := NewTLSMux(tcpListener, nil, cert, key) \n + mux, err := NewTLSMux(tcpListener, nil, cert, key) \n if err != nil { \n t.Fatalf(\"failed to create mux: %s\", err.Error()) \n } \n + go mux.Serve() \n + \n + // Verify that the listener is secured. \n + conn, err := tls.Dial(\"tcp\", tcpListener.Addr().String(), &tls.Config{ \n + InsecureSkipVerify: true, \n + }) \n + if err != nil { \n + t.Fatal(err) \n + } \n + \n + state := conn.ConnectionState() \n + if !state.HandshakeComplete { \n + t.Fatal(\"connection handshake failed to complete\") \n + } \n } \n func TestTLSMux_Fail(t *testing.T) {", "msg": "More unit testing of TLS mux"}
{"diff": "a / system_test/helpers.go \n  b / system_test/helpers.go \n@@ -94,6 +94,9 @@ func (n *Node) Status() (string, error) { \n if err != nil { \n return \"\", err \n } \n + if resp.StatusCode != 200 { \n + return \"\", fmt.Errorf(\"status endpoint returned: %s\", resp.Status) \n + } \n defer resp.Body.Close() \n body, err := ioutil.ReadAll(resp.Body) \n if err != nil { \n @@ -110,6 +113,9 @@ func (n *Node) Expvar() (string, error) { \n if err != nil { \n return \"\", err \n } \n + if resp.StatusCode != 200 { \n + return \"\", fmt.Errorf(\"expvar endpoint returned: %s\", resp.Status) \n + } \n defer resp.Body.Close() \n body, err := ioutil.ReadAll(resp.Body) \n if err != nil { \n @@ -286,6 +292,7 @@ func mustNewNode(enableSingle bool) *Node { \n node.RaftAddr = node.Store.Addr().String() \n node.Service = httpd.New(\"localhost:0\", node.Store, nil) \n + node.Service.Expvar = true \n if err := node.Service.Start(); err != nil { \n node.Deprovision() \n panic(fmt.Sprintf(\"failed to start HTTP server: %s\", err.Error()))", "msg": "Enable expvar endpoint for testing"}
{"diff": "a / cmd/rqlite/main.go \n  b / cmd/rqlite/main.go \n@@ -133,6 +133,10 @@ func sendRequest(ctx *cli.Context, urlStr string, line string, argv *argT, ret i \n } \n defer resp.Body.Close() \n + if resp.StatusCode == http.StatusUnauthorized { \n + return fmt.Errorf(\"unauthorized\") \n + } \n + \n // Check for redirect. \n if resp.StatusCode == http.StatusMovedPermanently { \n nRedirect++ \n @@ -190,6 +194,10 @@ func cliJSON(ctx *cli.Context, cmd, line, url string, argv *argT) error { \n } \n defer resp.Body.Close() \n + if resp.StatusCode == http.StatusUnauthorized { \n + return fmt.Errorf(\"unauthorized\") \n + } \n + \n body, err := ioutil.ReadAll(resp.Body) \n if err != nil { \n return err", "msg": "CLI checks for HTTP 401\nThe CLI doesn't yet support passing authentication credentials, but this change means the user will know what is happening."}
{"diff": "a / store/store.go \n  b / store/store.go \n@@ -1121,12 +1121,18 @@ func subCommandToStatements(d *databaseSub) []sql.Statement { \n stmts := make([]sql.Statement, len(d.Queries)) \n for i := range d.Queries { \n stmts[i].Query = d.Queries[i] \n - stmts[i].Parameters = make([]gosql.Value, len(d.Parameters[i])) \n + // Support backwards-compatibility, since previous versions didn't \n + // have Parameters in Raft commands. \n + if len(d.Parameters) == 0 { \n + stmts[i].Parameters = make([]gosql.Value, 0) \n + } else { \n + stmts[i].Parameters = make([]gosql.Value, len(d.Parameters[i])) \n for j := range d.Parameters[i] { \n stmts[i].Parameters[j] = d.Parameters[i][j] \n } \n } \n + } \n return stmts \n }", "msg": "Handle Raft commands missing Parameterized values\nThis allows this code to run with Raft logs from previous versions."}
{"diff": "a / cluster/join.go \n  b / cluster/join.go \n@@ -8,7 +8,6 @@ import ( \n \"fmt\" \n \"io/ioutil\" \n \"log\" \n - \"net\" \n \"net/http\" \n \"os\" \n \"strings\" \n @@ -55,12 +54,6 @@ func join(joinAddr, id, addr string, voter bool, meta map[string]string, tlsConf \n return \"\", fmt.Errorf(\"node ID not set\") \n } \n - // Join using IP address, as that is what Hashicorp Raft works in. \n - resv, err := net.ResolveTCPAddr(\"tcp\", addr) \n - if err != nil { \n - return \"\", err \n - } \n - \n // Check for protocol scheme, and insert default if necessary. \n fullAddr := httpd.NormalizeAddr(fmt.Sprintf(\"%s/join\", joinAddr)) \n @@ -76,7 +69,7 @@ func join(joinAddr, id, addr string, voter bool, meta map[string]string, tlsConf \n for { \n b, err := json.Marshal(map[string]interface{}{ \n \"id\": id, \n - \"addr\": resv.String(), \n + \"addr\": addr, \n \"voter\": voter, \n \"meta\": meta, \n })", "msg": "Do not use resolved address in join request\nIt's not clear why this code was put in place in the first place, but it\nmay not be necessary."}
{"diff": "a / cmd/rqlited/main.go \n  b / cmd/rqlited/main.go \n@@ -130,7 +130,7 @@ func init() { \n flag.StringVar(&memProfile, \"mem-profile\", \"\", \"Path to file for memory profiling information\") \n flag.Usage = func() { \n fmt.Fprintf(os.Stderr, \"\\n%s\\n\\n\", desc) \n - fmt.Fprintf(os.Stderr, \"Usage: %s [arguments] <data directory>\\n\", name) \n + fmt.Fprintf(os.Stderr, \"Usage: %s [flags] <data directory>\\n\", name) \n flag.PrintDefaults() \n } \n } \n @@ -145,8 +145,14 @@ func main() { \n } \n // Ensure the data path is set. \n - if flag.NArg() == 0 { \n - flag.Usage() \n + if flag.NArg() < 1 { \n + fmt.Fprintf(os.Stderr, \"fatal: no data directory set\\n\") \n + os.Exit(1) \n + } \n + \n + // Ensure no args come after the data directory. \n + if flag.NArg() > 1 { \n + fmt.Fprintf(os.Stderr, \"fatal: arguments after data directory are not accepted\\n\") \n os.Exit(1) \n }", "msg": "Exit if arguments are passed after data directory\nFixes issue"}
{"diff": "a / store/store.go \n  b / store/store.go \n@@ -125,6 +125,7 @@ type Store struct { \n boltStore *raftboltdb.BoltStore // Physical store. \n lastIdxOnOpen uint64 // Last index on log when Store opens. \n + firstLogAppliedT time.Time // Time first log is applied \n appliedOnOpen uint64 // Number of logs applied at open. \n openT time.Time // Timestamp when Store opens. \n @@ -881,11 +882,16 @@ func (s *Store) Apply(l *raft.Log) interface{} { \n if l.Index <= s.lastIdxOnOpen { \n s.appliedOnOpen++ \n if l.Index == s.lastIdxOnOpen { \n - s.logger.Printf(\"%d committed log entries applied in %s\", \n - s.appliedOnOpen, time.Since(s.openT)) \n + s.logger.Printf(\"%d committed log entries applied in %s, took %s since open\", \n + s.appliedOnOpen, time.Since(s.firstLogAppliedT), time.Since(s.openT)) \n } \n } \n }() \n + \n + if s.firstLogAppliedT.IsZero() { \n + s.firstLogAppliedT = time.Now() \n + } \n + \n var c command.Command \n if err := legacy.Unmarshal(l.Data, &c); err != nil {", "msg": "Better timing from first to last log"}
{"diff": "a / store/store_test.go \n  b / store/store_test.go \n@@ -1261,10 +1261,7 @@ func Test_State(t *testing.T) { \n } \n } \n -func mustNewStore(inmem bool) *Store { \n - path := mustTempDir() \n - defer os.RemoveAll(path) \n - \n +func mustNewStoreAtPath(path string, inmem bool) *Store { \n cfg := NewDBConfig(\"\", inmem) \n s := New(mustMockLister(\"localhost:0\"), &StoreConfig{ \n DBConf: cfg, \n @@ -1277,6 +1274,10 @@ func mustNewStore(inmem bool) *Store { \n return s \n } \n +func mustNewStore(inmem bool) *Store { \n + return mustNewStoreAtPath(mustTempDir(), inmem) \n +} \n + \n type mockSnapshotSink struct { \n *os.File \n }", "msg": "More useful store test helper functions"}
{"diff": "a / store/store.go \n  b / store/store.go \n@@ -498,6 +498,11 @@ func (s *Store) Stats() (map[string]interface{}, error) { \n return nil, err \n } \n + dirSz, err := dirSize(s.raftDir) \n + if err != nil { \n + return nil, err \n + } \n + \n status := map[string]interface{}{ \n \"node_id\": s.raftID, \n \"raft\": raftStats, \n @@ -516,6 +521,7 @@ func (s *Store) Stats() (map[string]interface{}, error) { \n \"metadata\": s.meta, \n \"nodes\": nodes, \n \"dir\": s.raftDir, \n + \"dir_size\": dirSz, \n \"sqlite3\": dbStatus, \n \"db_conf\": s.dbConf, \n } \n @@ -1421,3 +1427,18 @@ func pathExists(p string) bool { \n } \n return true \n } \n + \n +// dirSize returns the total size of all files in the given directory \n +func dirSize(path string) (int64, error) { \n + var size int64 \n + err := filepath.Walk(path, func(_ string, info os.FileInfo, err error) error { \n + if err != nil { \n + return err \n + } \n + if !info.IsDir() { \n + size += info.Size() \n + } \n + return err \n + }) \n + return size, err \n +}", "msg": "Add total Raft directory size to Store stats"}
{"diff": "a / system_test/helpers.go \n  b / system_test/helpers.go \n@@ -54,7 +54,7 @@ func (n *Node) Close(graceful bool) error { \n // Deprovision shuts down and removes all resources associated with the node. \n func (n *Node) Deprovision() { \n - n.Store.Close(false) \n + n.Store.Close(true) \n n.Service.Close() \n os.RemoveAll(n.Dir) \n }", "msg": "Shutdown gracefully to prevent races\nSee"}
{"diff": "a / command/marshal.go \n  b / command/marshal.go \n@@ -182,19 +182,22 @@ func UnmarshalSubCommand(c *Command, m proto.Message) error { \n if c.Compressed { \n gz, err := gzip.NewReader(bytes.NewReader(b)) \n if err != nil { \n - return err \n + fmt.Errorf(\"unmarshal sub gzip NewReader: %s\", err) \n } \n ub, err := ioutil.ReadAll(gz) \n if err != nil { \n - return err \n + fmt.Errorf(\"unmarshal sub gzip ReadAll: %s\", err) \n } \n if err := gz.Close(); err != nil { \n - return err \n + fmt.Errorf(\"unmarshal sub gzip Close: %s\", err) \n } \n b = ub \n } \n - return proto.Unmarshal(b, m) \n + if err := proto.Unmarshal(b, m); err != nil { \n + return fmt.Errorf(\"proto unmarshal: %s\", err) \n + } \n + return nil \n }", "msg": "Better error messages during unmarshal"}
{"diff": "a / command/marshal.go \n  b / command/marshal.go \n@@ -97,10 +97,10 @@ func (m *RequestMarshaler) Marshal(r Requester) ([]byte, bool, error) { \n var buf bytes.Buffer \n gzw.Reset(&buf) \n if _, err := gzw.Write(b); err != nil { \n - return nil, false, err \n + return nil, false, fmt.Errorf(\"gzip Write: %s\", err) \n } \n if err := gzw.Close(); err != nil { \n - return nil, false, err \n + return nil, false, fmt.Errorf(\"gzip Close: %s\", err) \n } \n // Is compression better?", "msg": "Better error messages during gzip"}
{"diff": "a / db/db.go \n  b / db/db.go \n@@ -75,7 +75,8 @@ type PoolStats struct { \n MaxLifetimeClosed int64 `json:\"max_lifetime_closed\"` \n } \n -// Open opens a file-based database, creating it if it does not exist. \n +// Open opens a file-based database, creating it if it does not exist. After this \n +// function returns, an actual SQLite file will always exist. \n func Open(dbPath string, fkEnabled bool) (*DB, error) { \n rwDSN := fmt.Sprintf(\"file:%s?_fk=%s\", dbPath, strconv.FormatBool(fkEnabled)) \n rwDB, err := sql.Open(\"sqlite3\", rwDSN)", "msg": "Better comment in DB code"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -923,8 +923,7 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { \n // Get the query statement(s), and do tx if necessary. \n queries, err := requestQueries(r) \n if err != nil { \n - w.WriteHeader(http.StatusBadRequest) \n - w.Write([]byte(err.Error())) \n + http.Error(w, err.Error(), http.StatusBadRequest) \n return \n }", "msg": "Use better function"}
{"diff": "a / http/request_parser.go \n  b / http/request_parser.go \n@@ -11,6 +11,9 @@ var ( \n // ErrNoStatements is returned when a request is empty \n ErrNoStatements = errors.New(\"no statements\") \n + // ErrInvalidJSON is returned when a body is not valid JSON \n + ErrInvalidJSON = errors.New(\"invalid JSON body\") \n + \n // ErrInvalidRequest is returned when a request cannot be parsed. \n ErrInvalidRequest = errors.New(\"invalid request\") \n @@ -45,7 +48,7 @@ func ParseRequest(b []byte) ([]*command.Statement, error) { \n // Next try parameterized form. \n if err := json.Unmarshal(b, &parameterized); err != nil { \n - return nil, ErrInvalidRequest \n + return nil, ErrInvalidJSON \n } \n stmts := make([]*command.Statement, len(parameterized))", "msg": "Better error codes"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -115,7 +115,7 @@ func (d *DBResults) MarshalJSON() ([]byte, error) { \n } else if d.QueryRows != nil { \n return encoding.JSONMarshal(d.QueryRows) \n } \n - return nil, fmt.Errorf(\"no DB results set\") \n + return json.Marshal(make([]interface{}, 0)) // Any empty list. \n } \n // Response represents a response from the HTTP service.", "msg": "Return a clearer response if no DB results\nThis shouldn't happen, but it might, this will allow the underlying\nerror to be returned to the caller. The body of the response will now\nlook something like:\n{\n\"results\": [],\n\"error\": \"some error\",\n\"time\": 0.021976516\n}"}
{"diff": "a / system_test/request_forwarding_test.go \n  b / system_test/request_forwarding_test.go \n@@ -232,14 +232,21 @@ func Test_MultiNodeClusterQueuedRequestForwardOK(t *testing.T) { \n t.Fatalf(\"failed to insert record: %s\", err.Error()) \n } \n - time.Sleep(1 * time.Second) \n - \n - rows, err = leader.Query(`SELECT COUNT(*) FROM foo`) \n + ticker := time.NewTicker(10 * time.Millisecond) \n + timer := time.NewTimer(5 * time.Second) \n + for { \n + select { \n + case <-ticker.C: \n + r, err := leader.Query(`SELECT COUNT(*) FROM foo`) \n if err != nil { \n t.Fatalf(\"failed to query for count: %s\", err.Error()) \n } \n - if exp, got := `{\"results\":[{\"columns\":[\"COUNT(*)\"],\"types\":[\"\"],\"values\":[[1]]}]}`, rows; exp != got { \n - t.Fatalf(\"got incorrect response from follower exp: %s, got: %s\", exp, got) \n + if r == `{\"results\":[{\"columns\":[\"COUNT(*)\"],\"types\":[\"\"],\"values\":[[1]]}]}` { \n + return \n + } \n + case <-timer.C: \n + t.Fatalf(\"timed out waiting for queued writes\") \n + } \n } \n }", "msg": "Better test polling"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -331,11 +331,14 @@ func (s *Service) Start() error { \n // Close closes the service. \n func (s *Service) Close() { \n s.stmtQueue.Close() \n + \n select { \n case <-s.queueDone: \n default: \n close(s.closeCh) \n } \n + <-s.queueDone \n + \n s.ln.Close() \n return \n }", "msg": "Fix more races"}
{"diff": "a / queue/queue_test.go \n  b / queue/queue_test.go \n@@ -253,6 +253,9 @@ func Test_NewQueueWriteNilSingleChan(t *testing.T) { \n if req.Statements != nil { \n t.Fatalf(\"statements slice is not nil\") \n } \n + if len(req.flushChans) != 1 && req.flushChans[0] != fc { \n + t.Fatalf(\"flush chans is not correct\") \n + } \n req.Close() \n case <-time.NewTimer(5 * time.Second).C: \n t.Fatalf(\"timed out waiting for statement\")", "msg": "More queue unit testing"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -1103,6 +1103,10 @@ func (s *Service) execute(w http.ResponseWriter, r *http.Request) { \n } \n results, resultsErr = s.cluster.Execute(er, addr, username, password, timeout) \n + if resultsErr.Error() == \"Unauthorized\" { \n + http.Error(w, ErrLeaderNotFound.Error(), http.StatusUnauthorized) \n + return \n + } \n stats.Add(numRemoteExecutions, 1) \n w.Header().Add(ServedByHTTPHeader, addr) \n }", "msg": "Implement handling of error unauthorized by client forwarding request to remote tcp service. Bubble this up as a 401 to the caller"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -1109,7 +1109,7 @@ func (s *Service) execute(w http.ResponseWriter, r *http.Request) { \n results, resultsErr = s.cluster.Execute(er, addr, username, password, timeout) \n if resultsErr != nil && resultsErr.Error() == \"unauthorized\" { \n - w.WriteHeader(http.StatusUnauthorized) \n + http.Error(w, \"remote execute not authorized\", http.StatusUnauthorized) \n return \n } \n stats.Add(numRemoteExecutions, 1) \n @@ -1206,7 +1206,7 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { \n } \n results, resultsErr = s.cluster.Query(qr, addr, username, password, timeout) \n if resultsErr != nil && resultsErr.Error() == \"unauthorized\" { \n - w.WriteHeader(http.StatusUnauthorized) \n + http.Error(w, \"remote query not authorized\", http.StatusUnauthorized) \n return \n } \n stats.Add(numRemoteQueries, 1)", "msg": "Better remote auth fail HTTP messages"}
{"diff": "a / store/store.go \n  b / store/store.go \n@@ -825,6 +825,7 @@ func (s *Store) Backup(br *command.BackupRequest, dst io.Writer) error { \n if err := f.Close(); err != nil { \n return err \n } \n + defer os.Remove(f.Name()) \n if err := s.db.Backup(f.Name()); err != nil { \n return err \n @@ -834,7 +835,6 @@ func (s *Store) Backup(br *command.BackupRequest, dst io.Writer) error { \n if err != nil { \n return err \n } \n - defer os.Remove(f.Name()) \n defer of.Close() \n _, err = io.Copy(dst, of)", "msg": "Even better defer sequence"}
{"diff": "a / store/store.go \n  b / store/store.go \n@@ -860,6 +860,7 @@ func (s *Store) Load(lr *command.LoadRequest) error { \n b, err := command.MarshalLoadRequest(lr) \n if err != nil { \n + s.logger.Printf(\"load failed during load-request marshalling %s\", err.Error()) \n return err \n } \n @@ -878,6 +879,7 @@ func (s *Store) Load(lr *command.LoadRequest) error { \n if af.Error() == raft.ErrNotLeader { \n return ErrNotLeader \n } \n + s.logger.Printf(\"load failed during Apply: %s\", af.Error()) \n return af.Error() \n }", "msg": "More store-level load operation logging"}
{"diff": "a / http/service.go \n  b / http/service.go \n@@ -959,8 +959,11 @@ func (s *Service) handleNodes(w http.ResponseWriter, r *http.Request) { \n // Get nodes in the cluster, and possibly filter out non-voters. \n nodes, err := s.store.Nodes() \n if err != nil { \n - http.Error(w, fmt.Sprintf(\"store nodes: %s\", err.Error()), \n - http.StatusInternalServerError) \n + statusCode := http.StatusInternalServerError \n + if err == store.ErrNotOpen { \n + statusCode = http.StatusServiceUnavailable \n + } \n + http.Error(w, fmt.Sprintf(\"store nodes: %s\", err.Error()), statusCode) \n return \n }", "msg": "Better error from /nodes when Store is not open"}
{"diff": "a / cluster/join.go \n  b / cluster/join.go \n@@ -104,6 +104,8 @@ func (j *Joiner) Do(joinAddrs []string, id, addr string, voter bool) (string, er \n if err == nil { \n // Success! \n return joinee, nil \n + } else { \n + j.logger.Printf(\"failed to join via node at %s: %s\", a, err) \n } \n } \n if i+1 < j.numAttempts { \n @@ -166,14 +168,14 @@ func (j *Joiner) join(joinAddr, id, addr string, voter bool) (string, error) { \n // protocol a registered node is actually using. \n if strings.HasPrefix(fullAddr, \"https://\") { \n // It's already HTTPS, give up. \n - return \"\", fmt.Errorf(\"failed to join, node returned: %s: (%s)\", resp.Status, string(b)) \n + return \"\", fmt.Errorf(\"%s: (%s)\", resp.Status, string(b)) \n } \n j.logger.Print(\"join via HTTP failed, trying via HTTPS\") \n fullAddr = rurl.EnsureHTTPS(fullAddr) \n continue \n default: \n - return \"\", fmt.Errorf(\"failed to join, node returned: %s: (%s)\", resp.Status, string(b)) \n + return \"\", fmt.Errorf(\"%s: (%s)\", resp.Status, string(b)) \n } \n } \n }", "msg": "Better logging for failure-to-join"}
{"diff": "a / config/config.go \n  b / config/config.go \n@@ -61,6 +61,8 @@ type userConfig struct { \n Mesosphere *struct { \n Cluster string \n Role string \n + SystemHealth bool `yaml:\"systemHealth,omitempty\"` \n + Verbose bool `yaml:\"verbose,omitempty\"` \n } \n } \n @@ -146,6 +148,8 @@ func loadUserConfig(pair *store.KVPair) error { \n // Set the cluster name for the mesos default plugin config \n staticPlugins[\"mesos\"] = map[string]interface{}{ \n \"cluster\": mesos.Cluster, \n + \"systemhealth\": mesos.SystemHealth, \n + \"verbose\": mesos.Verbose, \n } \n }", "msg": "add verbose and system health options to mesos userconfig"}
{"diff": "a / monitors/collectd/collectd.go \n  b / monitors/collectd/collectd.go \n@@ -142,6 +142,7 @@ func (cm *Manager) Shutdown() { \n if cm.State() != Stopped { \n cm.stopChan <- 0 \n cm.restartDebouncedStop <- struct{}{} \n + cm.restartDebouncedStop = nil \n } \n } \n @@ -196,19 +197,23 @@ func (cm *Manager) rerenderConf() bool { \n func (cm *Manager) runCollectd() { \n stoppedCh := make(chan struct{}, 1) \n + stop := func() { \n + cm.killChildProc() \n + <-stoppedCh \n + } \n + \n go cm.runAsChildProc(stoppedCh) \n for { \n select { \n case <-cm.stopChan: \n cm.setState(ShuttingDown) \n - cm.killChildProc() \n + stop() \n cm.setState(Stopped) \n return \n case <-cm.reloadChan: \n cm.setState(Restarting) \n - cm.killChildProc() \n - <-stoppedCh \n + stop() \n go cm.runAsChildProc(stoppedCh) \n cm.setState(Running) \n }", "msg": "Making collectd restart properly when agent is reset with SIGHUP"}
{"diff": "a / core/common/kubernetes/client.go \n  b / core/common/kubernetes/client.go \n@@ -35,6 +35,9 @@ type APIConfig struct { \n // Validate validates the K8s API config \n func (c *APIConfig) Validate() error { \n + if c.AuthType != AuthTypeNone && c.AuthType != AuthTypeTLS && c.AuthType != AuthTypeServiceAccount { \n + return errors.New(\"Invalid authType for kubernetes: \" + string(c.AuthType)) \n + } \n if c.AuthType == AuthTypeTLS && (c.ClientCertPath == \"\" || c.ClientKeyPath == \"\") { \n return errors.New(\"For TLS auth, you must set both the kubernetesAPI.clientCertPath \" + \n \"and kubernetesAPI.clientKeyPath config values\")", "msg": "Adding better validation of k8s authType config"}
{"diff": "a / core/config/config.go \n  b / core/config/config.go \n@@ -53,15 +53,22 @@ type Config struct { \n } \n func (c *Config) setDefaultHostname() { \n - fqdn := fqdn.Get() \n - if fqdn == \"unknown\" { \n - log.Info(\"Error getting fully qualified hostname\") \n - } else { \n - log.Infof(\"Using hostname %s\", fqdn) \n - c.Hostname = fqdn \n + host := fqdn.Get() \n + if host == \"unknown\" || host == \"localhost\" { \n + log.Info(\"Error getting fully qualified hostname, using plain hostname\") \n + \n + var err error \n + host, err = os.Hostname() \n + if err != nil { \n + log.Error(\"Error getting system simple hostname, cannot set hostname\") \n + return \n } \n } \n + log.Infof(\"Using hostname %s\", host) \n + c.Hostname = host \n +} \n + \n func (c *Config) initialize(metaStore *stores.MetaStore) (*Config, error) { \n c.overrideFromEnv()", "msg": "Making sure hostname is never resolved to localhost"}
{"diff": "a / monitors/diagnostics.go \n  b / monitors/diagnostics.go \n@@ -37,6 +37,13 @@ func (mm *MonitorManager) DiagnosticText() string { \n mm.lock.Lock() \n defer mm.lock.Unlock() \n + configurationText := \"\\n\" \n + for i := range mm.monitorConfigs { \n + configurationText += fmt.Sprintf( \n + \"%s\\n\", \n + utils.IndentLines(config.ToString(mm.monitorConfigs[i]), 2)) \n + } \n + \n activeMonText := \"\" \n for i := range mm.activeMonitors { \n am := mm.activeMonitors[i] \n @@ -70,12 +77,15 @@ func (mm *MonitorManager) DiagnosticText() string { \n } \n return fmt.Sprintf( \n + \"Monitor Configurations (Not necessarily active):\\n\"+ \n + \"%s\"+ \n \"Active Monitors:\\n\"+ \n \"%s\"+ \n \"Discovered Endpoints:\\n\"+ \n \"%s\\n\"+ \n \"Bad Monitor Configurations:\\n\"+ \n \"%s\\n\", \n + configurationText, \n activeMonText, \n discoveredEndpointsText, \n badConfigText(mm.badConfigs))", "msg": "Adding diagnostic output for configured monitors"}
{"diff": "a / internal/core/config/writer.go \n  b / internal/core/config/writer.go \n@@ -26,9 +26,9 @@ type WriterConfig struct { \n // The agent does not send datapoints immediately upon a monitor generating \n // them, but buffers them and sends them in batches. The lower this \n // number, the less delay for datapoints to appear in SignalFx. \n - DatapointSendIntervalSeconds int `yaml:\"datapointSendIntervalSeconds\" default:\"5\"` \n + DatapointSendIntervalSeconds int `yaml:\"datapointSendIntervalSeconds\" default:\"1\"` \n // The analogue of `datapointSendIntervalSeconds` for events \n - EventSendIntervalSeconds int `yaml:\"eventSendIntervalSeconds\" default:\"5\"` \n + EventSendIntervalSeconds int `yaml:\"eventSendIntervalSeconds\" default:\"1\"` \n // If the log level is set to `debug` and this is true, all datapoints \n // generated by the agent will be logged. \n LogDatapoints bool `yaml:\"logDatapoints\"`", "msg": "Reducing default batching time for dps and events"}
{"diff": "a / internal/monitors/kubernetes/cluster/metrics/cache.go \n  b / internal/monitors/kubernetes/cluster/metrics/cache.go \n@@ -138,7 +138,12 @@ func (dc *DatapointCache) AllDatapoints() []*datapoint.Datapoint { \n for k := range dc.dpCache { \n if dc.dpCache[k] != nil { \n - dps = append(dps, dc.dpCache[k]...) \n + for i := range dc.dpCache[k] { \n + // Copy the datapoint since nothing in datapoints is thread \n + // safe. \n + dp := *dc.dpCache[k][i] \n + dps = append(dps, &dp) \n + } \n } \n }", "msg": "Copying k8s dps since they aren't thread safe\nThis only becomes an issue if intervals start overlapping due to CPU limits"}
{"diff": "a / cmd/agent/main.go \n  b / cmd/agent/main.go \n@@ -8,6 +8,7 @@ import ( \n \"os/signal\" \n \"strings\" \n \"syscall\" \n + \"time\" \n log \"github.com/sirupsen/logrus\" \n @@ -130,7 +131,14 @@ func runAgent() { \n case <-interruptCh: \n log.Info(\"Interrupt signal received, stopping agent\") \n shutdown() \n - <-shutdownComplete \n + select { \n + case <-shutdownComplete: \n + break \n + case <-time.After(10 * time.Second): \n + log.Error(\"Shutdown timed out, forcing process down\") \n + break \n + } \n + \n close(exit) \n } \n }()", "msg": "Force agent to shutdown after 10 seconds of interrupt"}
{"diff": "a / internal/core/config/loader.go \n  b / internal/core/config/loader.go \n@@ -133,6 +133,10 @@ var envVarRE = regexp.MustCompile(`\\${\\s*([\\w-]+?)\\s*}`) \n // still get to them when we need to rerender config \n var envVarCache = make(map[string]string) \n +var envVarWhitelist = map[string]bool{ \n + \"MY_NODE_NAME\": true, \n +} \n + \n // Replaces envvar syntax with the actual envvars \n func preprocessConfig(content []byte) []byte { \n return envVarRE.ReplaceAllFunc(content, func(bs []byte) []byte { \n @@ -149,8 +153,10 @@ func preprocessConfig(content []byte) []byte { \n \"envvar\": envvar, \n }).Debug(\"Sanitizing envvar from agent\") \n + if !envVarWhitelist[envvar] { \n os.Unsetenv(envvar) \n } \n + } \n return []byte(val) \n })", "msg": "Allow node name to stay in the env if referenced in config"}
{"diff": "a / internal/monitors/collectd/solr/solr.go \n  b / internal/monitors/collectd/solr/solr.go \n@@ -39,8 +39,6 @@ type Config struct { \n IncludeMetric string `yaml:\"includeMetric\"` \n // ExcludeMetric metric name from the /admin/metrics endpoint to exclude (valid when EnhancedMetrics is \"true\") \n ExcludeMetric string `yaml:\"excludeMetric\"` \n - // Dimension space-separated key-value pair for a user-defined dimension \n - Dimension string `yaml:\"dimension\"` \n } \n // PythonConfig returns the embedded python.Config struct from the interface \n @@ -69,7 +67,6 @@ func (m *Monitor) Configure(conf *Config) error { \n \"EnhancedMetrics\": conf.EnhancedMetrics, \n \"IncludeMetric\": conf.IncludeMetric, \n \"ExcludeMetric\": conf.ExcludeMetric, \n - \"Dimension\": conf.Dimension, \n \"Interval\": conf.IntervalSeconds, \n }, \n }", "msg": "dimension field not necessary because embedded extraDimensions field"}
{"diff": "a / internal/core/writer/writer.go \n  b / internal/core/writer/writer.go \n@@ -225,7 +225,7 @@ func (sw *SignalFxWriter) preprocessDatapoint(dp *datapoint.Datapoint) { \n func (sw *SignalFxWriter) sendDatapoints(dps []*datapoint.Datapoint) error { \n // This sends synchonously \n - err := sw.client.AddDatapoints(context.Background(), dps) \n + err := sw.client.AddDatapoints(sw.ctx, dps) \n if err != nil { \n log.WithFields(log.Fields{ \n \"error\": err,", "msg": "Use writer context when sending datapointsg"}
{"diff": "a / internal/core/writer/spans.go \n  b / internal/core/writer/spans.go \n@@ -7,6 +7,7 @@ import ( \n \"github.com/davecgh/go-spew/spew\" \n \"github.com/signalfx/golib/datapoint\" \n \"github.com/signalfx/golib/trace\" \n + \"github.com/signalfx/signalfx-agent/internal/core/common/constants\" \n \"github.com/signalfx/signalfx-agent/internal/core/common/dpmeta\" \n \"github.com/signalfx/signalfx-agent/internal/core/writer/tracetracker\" \n \"github.com/signalfx/signalfx-agent/internal/utils\" \n @@ -151,6 +152,8 @@ func (sw *SignalFxWriter) preprocessSpan(span *trace.Span) { \n delete(span.Tags, dpmeta.NotHostSpecificMeta) \n } \n + // adding smart agent version as a tag \n + span.Tags[\"signalfx.smartagent.version\"] = constants.Version \n if sw.conf.LogTraceSpans { \n log.Debugf(\"Sending trace span:\\n%s\", spew.Sdump(span)) \n }", "msg": "Adding smart agent version as a tag to spans"}
{"diff": "a / internal/monitors/kubernetes/cluster/metrics/pods.go \n  b / internal/monitors/kubernetes/cluster/metrics/pods.go \n@@ -65,11 +65,6 @@ func dimPropsForPod(cachedPod *k8sutil.CachedPod, sc *k8sutil.ServiceCache, \n props[utils.LowercaseFirstChar(or.Kind)+\"_uid\"] = string(or.UID) \n } \n - tolerationProps := getPropsFromTolerations(cachedPod.Tolerations) \n - for k, v := range tolerationProps { \n - props[k] = v \n - } \n - \n // if pod is selected by a service, sync service as a tag \n serviceTags := sc.GetMatchingServices(cachedPod) \n for _, tag := range serviceTags {", "msg": "Remove K8s pod toleration properties until some better way of storing them can be found"}
{"diff": "a / pkg/core/writer/spans.go \n  b / pkg/core/writer/spans.go \n@@ -18,8 +18,18 @@ func (sw *SignalFxWriter) sendSpans(ctx context.Context, spans []*trace.Span) er \n sw.serviceTracker.AddSpans(sw.ctx, spans) \n } \n - // This sends synchronously \n - return sw.client.AddSpans(context.Background(), spans) \n + // This sends synchonously \n + err := sw.client.AddSpans(context.Background(), spans) \n + if err != nil { \n + log.WithFields(log.Fields{ \n + \"error\": err, \n + }).Error(\"Error shipping spans to SignalFx\") \n + // If there is an error sending spans then just forget about them. \n + return err \n + } \n + log.Debugf(\"Sent %d spans out of the agent\", len(spans)) \n + \n + return nil \n } \n // Mutates span tags in place to add global span tags. Also", "msg": "Add log error msg when there are errors sending spans\nThis was done symmetrically to what is done for SFx datapoints."}
{"diff": "a / pkg/monitors/kubernetes/kubeletmetrics/metrics.go \n  b / pkg/monitors/kubernetes/kubeletmetrics/metrics.go \n@@ -13,6 +13,7 @@ func convertContainerMetrics(c *v1alpha1.ContainerStats, status *v1.ContainerSta \n if status != nil { \n dims[\"container_id\"] = k8sutil.StripContainerID(status.ContainerID) \n + dims[\"container_image\"] = status.Image \n } \n dims[\"container_spec_name\"] = c.Name", "msg": "kubelet-metrics monitor: Add container_image dim when using pod endpoint"}
{"diff": "a / pkg/monitors/vsphere/service/inventory.go \n  b / pkg/monitors/vsphere/service/inventory.go \n@@ -131,6 +131,12 @@ func (svc *InventorySvc) followHost( \n if err != nil { \n return err \n } \n + \n + if host.Runtime.PowerState == types.HostSystemPowerStatePoweredOff { \n + svc.log.Debugf(\"inventory: host powered off: name=[%s]\", host.Name) \n + return nil \n + } \n + \n svc.debug(&host) \n dims = append(dims, pair{\"esx_ip\", host.Name}) \n hostDims := map[string]string{} \n @@ -156,6 +162,12 @@ func (svc *InventorySvc) followVM( \n if err != nil { \n return err \n } \n + \n + if vm.Runtime.PowerState == types.VirtualMachinePowerStatePoweredOff { \n + svc.log.Debugf(\"inventory: vm powered off: name=[%s]\", vm.Name) \n + return nil \n + } \n + \n svc.debug(&vm) \n vmDims := map[string]string{ \n \"vm_name\": vm.Name, // e.g. \"MyDebian10Host\"", "msg": "Skip powered-off VMs and hosts\nThe vmomi sdk reports CPU values of -0.01 for powered off VMs. This change\nskips adding powered off VMs and hosts to the monitor's inventory cache."}
{"diff": "a / pkg/monitors/prometheusexporter/prometheus.go \n  b / pkg/monitors/prometheusexporter/prometheus.go \n@@ -7,6 +7,7 @@ import ( \n \"io\" \n \"io/ioutil\" \n \"net/http\" \n + \"strings\" \n \"time\" \n \"github.com/signalfx/signalfx-agent/pkg/core/common/auth\" \n @@ -170,8 +171,14 @@ func doFetch(fetch fetcher) ([]*dto.MetricFamily, error) { \n return nil, err \n } \n defer body.Close() \n + var decoder expfmt.Decoder \n + // some \"text\" responses are missing \\n from the last line \n + if expformat != expfmt.FmtProtoDelim { \n + decoder = expfmt.NewDecoder(io.MultiReader(body, strings.NewReader(\"\\n\")), expformat) \n + } else { \n + decoder = expfmt.NewDecoder(body, expformat) \n + } \n - decoder := expfmt.NewDecoder(body, expformat) \n var mfs []*dto.MetricFamily \n for {", "msg": "Append new line to the reader stream to make sure it's prometheus happy"}
{"diff": "a / pkg/core/hostid/aws.go \n  b / pkg/core/hostid/aws.go \n@@ -44,6 +44,7 @@ func AWSUniqueID(cloudMetadataTimeout timeutil.Duration) string { \n if err != nil { \n log.WithFields(log.Fields{ \n \"error\": err, \n + \"body\": string(body), \n }).Debug(\"Failed to unmarshal AWS instance-identity response\") \n return \"\" \n }", "msg": "log the body for better debugging"}
{"diff": "a / pkg/monitors/prometheusexporter/prometheus.go \n  b / pkg/monitors/prometheusexporter/prometheus.go \n@@ -142,10 +142,6 @@ func (m *Monitor) Configure(conf *Config) error { \n return \n } \n - now := time.Now() \n - for i := range dps { \n - dps[i].Timestamp = now \n - } \n m.Output.SendDatapoints(dps...) \n }, time.Duration(conf.IntervalSeconds)*time.Second)", "msg": "prometheus-exporter monitor: Don't set timestamp\nLet it be unset so it represents the 'latest available'."}
{"diff": "a / pkg/monitors/kubernetes/events/events.go \n  b / pkg/monitors/kubernetes/events/events.go \n@@ -177,7 +177,6 @@ func k8sEventToSignalFxEvent(ev *v1.Event) *event.Event { \n dims[\"kubernetes_name\"] = ev.InvolvedObject.Name \n dims[\"kubernetes_uid\"] = string(ev.InvolvedObject.UID) \n } \n - \n properties := utils.RemoveEmptyMapValues(map[string]string{ \n \"message\": ev.Message, \n \"source_component\": ev.Source.Component, \n @@ -186,8 +185,14 @@ func k8sEventToSignalFxEvent(ev *v1.Event) *event.Event { \n \"kubernetes_resource_version\": ev.InvolvedObject.ResourceVersion, \n }) \n + eventType := ev.Reason \n + if eventType == \"\" { \n + logger.Debug(\"ev.Reason is not set; setting event type to unknown_reason\") \n + eventType = \"unknown_reason\" \n + } \n + \n return event.NewWithProperties( \n - ev.Reason, \n + eventType, \n event.AGENT, \n utils.RemoveEmptyMapValues(dims), \n utils.StringMapToInterfaceMap(properties),", "msg": "k8s events monitor handle empty event type"}
{"diff": "a / window.go \n  b / window.go \n@@ -401,7 +401,7 @@ func (wt *windowTriangles) flush() { \n wt.attrData[i] = make(map[pixelgl.Attr]interface{}) \n } \n p := v.Position \n - c := NRGBAModel.Convert(v.Color).(NRGBA) \n + c := v.Color \n t := v.Texture \n wt.attrData[i][positionVec2] = mgl32.Vec2{float32(p.X()), float32(p.Y())} \n wt.attrData[i][colorVec4] = mgl32.Vec4{float32(c.R), float32(c.G), float32(c.B), float32(c.A)} \n @@ -448,9 +448,6 @@ func (wt *windowTriangles) Update(t Triangles) { \n } \n wt.data = append(wt.data, newData...) \n } \n - if t.Len() < wt.Len() { \n - wt.data = wt.data[:t.Len()] \n - } \n wt.data.Update(t) \n } \n @@ -497,6 +494,9 @@ func (w *Window) SetTransform(t ...Transform) { \n // SetMaskColor sets a global mask color for the Window. \n func (w *Window) SetMaskColor(c color.Color) { \n + if c == nil { \n + c = NRGBA{1, 1, 1, 1} \n + } \n nrgba := NRGBAModel.Convert(c).(NRGBA) \n r := float32(nrgba.R) \n g := float32(nrgba.G)", "msg": "fix passing nil color to Window"}
{"diff": "a / graphics.go \n  b / graphics.go \n@@ -3,6 +3,7 @@ package pixel \n // Sprite is a drawable Picture. It's always anchored by the center of it's Picture. \n type Sprite struct { \n tri *TrianglesData \n + bounds Rect \n d Drawer \n } \n @@ -20,18 +21,17 @@ func NewSprite(pic Picture) *Sprite { \n // SetPicture changes the Sprite's Picture. The new Picture may have a different size, everything \n // works. \n func (s *Sprite) SetPicture(pic Picture) { \n - oldPic := s.d.Picture \n s.d.Picture = pic \n - if oldPic != nil && oldPic.Bounds() == pic.Bounds() { \n + if s.bounds == pic.Bounds() { \n return \n } \n + s.bounds = pic.Bounds() \n var ( \n - bounds = pic.Bounds() \n - center = bounds.Center() \n - horizontal = V(bounds.W()/2, 0) \n - vertical = V(0, bounds.H()/2) \n + center = s.bounds.Center() \n + horizontal = V(s.bounds.W()/2, 0) \n + vertical = V(0, s.bounds.H()/2) \n ) \n (*s.tri)[0].Position = -horizontal - vertical", "msg": "fix Sprite for changing Picture bounds"}
{"diff": "a / imdraw/imdraw.go \n  b / imdraw/imdraw.go \n@@ -433,9 +433,13 @@ func (imd *IMDraw) outlineEllipseArc(radius pixel.Vec, low, high, thickness floa \n func (imd *IMDraw) polyline(thickness float64, closed bool) { \n points := imd.getAndClearPoints() \n - if len(points) < 2 { \n + if len(points) == 0 { \n return \n } \n + if len(points) == 1 { \n + // one point special case \n + points = append(points, points[0]) \n + } \n // first point \n j, i := 0, 1", "msg": "add support for one point lines in imdraw"}
{"diff": "a / batch.go \n  b / batch.go \n@@ -5,8 +5,7 @@ import ( \n \"image/color\" \n ) \n -// Batch is a Target that allows for efficient drawing of many objects with the same Picture (but \n -// different slices of the same Picture are allowed). \n +// Batch is a Target that allows for efficient drawing of many objects with the same Picture. \n // \n // To put an object into a Batch, just draw it onto it: \n // object.Draw(batch)", "msg": "fix obsolete Batch doc"}
{"diff": "a / drawer.go \n  b / drawer.go \n@@ -14,7 +14,7 @@ package pixel \n // Picture. \n // \n // Whenever you change the Triangles, call Dirty to notify Drawer that Triangles changed. You don't \n -// need to notify Drawer about a change of Picture. \n +// need to notify Drawer about a change of the Picture. \n type Drawer struct { \n Triangles Triangles \n Picture Picture", "msg": "fix grammar in Drawer doc"}
{"diff": "a / pixelgl/window.go \n  b / pixelgl/window.go \n@@ -70,7 +70,7 @@ func NewWindow(cfg WindowConfig) (*Window, error) { \n false: glfw.False, \n } \n - w := &Window{bounds: cfg.Bounds} \n + w := &Window{bounds: cfg.Bounds, cursorVisible: true} \n err := mainthread.CallErr(func() error { \n var err error", "msg": "fix Window.CursorVisible intial value (was false)"}
{"diff": "a / text/text.go \n  b / text/text.go \n@@ -64,8 +64,8 @@ func New(face font.Face, runeSets ...[]rune) *Text { \n txt := &Text{ \n atlas: atlas, \n color: pixel.Alpha(1), \n - lineHeight: 1, \n - tabWidth: atlas.mapping[' '].Advance * 4, \n + lineHeight: atlas.LineHeight(), \n + tabWidth: atlas.Glyph(' ').Advance * 4, \n } \n txt.glyph.SetLen(6) \n txt.d.Picture = txt.atlas.pic \n @@ -155,7 +155,7 @@ func (txt *Text) WriteRune(r rune) (n int, err error) { \n switch r { \n case '\\n': \n - txt.Dot -= pixel.Y(txt.atlas.lineHeight * txt.lineHeight) \n + txt.Dot -= pixel.Y(txt.lineHeight) \n txt.Dot = txt.Dot.WithX(txt.Orig.X()) \n return \n case '\\r':", "msg": "change Text.LineHeight to use actual units instead of scale (such as 1.5)"}
{"diff": "a / text/text.go \n  b / text/text.go \n@@ -43,7 +43,7 @@ func RangeTable(table *unicode.RangeTable) []rune { \n // txt := text.New(face, text.ASCII) \n // \n // As suggested by the constructor, a Text object is always associated with one font face and a \n -// fixed set of runes. For example, the Text we create above can draw text using the font face \n +// fixed set of runes. For example, the Text we created above can draw text using the font face \n // contained in the `face` variable and is capable of drawing ASCII characters. \n // \n // Here we create a Text object which can draw ASCII and Katakana characters:", "msg": "fix typo in Text doc"}
{"diff": "a / drawer_test.go \n  b / drawer_test.go \npackage pixel_test \n import ( \n + \"image\" \n \"testing\" \n \"github.com/faiface/pixel\" \n ) \n func BenchmarkSpriteDrawBatch(b *testing.B) { \n - sprite := pixel.NewSprite(nil, pixel.R(0, 0, 64, 64)) \n - batch := pixel.NewBatch(&pixel.TrianglesData{}, nil) \n + img := image.NewRGBA(image.Rect(0, 0, 64, 64)) \n + pic := pixel.PictureDataFromImage(img) \n + sprite := pixel.NewSprite(pic, pixel.R(0, 0, 64, 64)) \n + batch := pixel.NewBatch(&pixel.TrianglesData{}, pic) \n for i := 0; i < b.N; i++ { \n sprite.Draw(batch, pixel.IM) \n }", "msg": "improve sprite.Draw(batch) benchmark"}
{"diff": "a / audio/speaker/speaker.go \n  b / audio/speaker/speaker.go \n@@ -84,7 +84,7 @@ func Update() error { \n if val > +1 { \n val = +1 \n } \n - valInt16 := int16(val * (1 << 15)) \n + valInt16 := int16(val * (1<<15 - 1)) \n low := byte(valInt16 % (1 << 8)) \n high := byte(valInt16 / (1 << 8)) \n buf[i*4+c*2+0] = low", "msg": "fix edge sample value (-1 and +1) overflow"}
{"diff": "a / audio/wav/decode.go \n  b / audio/wav/decode.go \n@@ -146,6 +146,7 @@ func (s *decoder) Stream(samples [][2]float64) (n int, ok bool) { \n samples[j][1] = float64(int16(p[i+2])+int16(p[i+3])*(1<<8)) / (1<<15 - 1) \n } \n } \n + s.pos += int32(n) \n return n / bytesPerFrame, true \n }", "msg": "audio: wav: fix Stream (move position, forgot to do it previously)"}
{"diff": "a / geometry.go \n  b / geometry.go \n@@ -84,6 +84,14 @@ func (u Vec) Sub(v Vec) Vec { \n } \n } \n +// Floor returns converts x and y to their integer equivalents. \n +func (u Vec) Floor(v Vec) Vec { \n + return Vec{ \n + math.Floor(u.X), \n + math.Floor(u.Y), \n + } \n +} \n + \n // To returns the vector from u to v. Equivalent to v.Sub(u). \n func (u Vec) To(v Vec) Vec { \n return Vec{", "msg": "Adding a pixel.Vec Floor method to geometry.go"}
{"diff": "a / geometry.go \n  b / geometry.go \n@@ -85,7 +85,7 @@ func (u Vec) Sub(v Vec) Vec { \n } \n // Floor returns converts x and y to their integer equivalents. \n -func (u Vec) Floor(v Vec) Vec { \n +func (u Vec) Floor() Vec { \n return Vec{ \n math.Floor(u.X), \n math.Floor(u.Y),", "msg": "updated Floor method"}
{"diff": "a / sprite.go \n  b / sprite.go \n@@ -102,10 +102,10 @@ func (s *Sprite) calcData() { \n (*s.tri)[5].Position = Vec{}.Sub(horizontal).Add(vertical) \n for i := range *s.tri { \n - (*s.tri)[i].Position = s.matrix.Project((*s.tri)[i].Position) \n (*s.tri)[i].Color = s.mask \n (*s.tri)[i].Picture = center.Add((*s.tri)[i].Position) \n (*s.tri)[i].Intensity = 1 \n + (*s.tri)[i].Position = s.matrix.Project((*s.tri)[i].Position) \n } \n s.d.Dirty()", "msg": "Setting position in correct order"}
{"diff": "a / geometry.go \n  b / geometry.go \n@@ -479,6 +479,9 @@ type Rect struct { \n Min, Max Vec \n } \n +// ZR is a zero rectangle. \n +var ZR = Rect{Min:ZV, Max:ZV} \n + \n // R returns a new Rect with given the Min and Max coordinates. \n // \n // Note that the returned rectangle is not automatically normalized.", "msg": "Added ZR for zero rect\nAdded the ZR for rectangle for both utility and consistensy with pixel.ZV"}
{"diff": "a / pixelgl/window.go \n  b / pixelgl/window.go \n@@ -173,6 +173,13 @@ func (w *Window) Destroy() { \n // Update swaps buffers and polls events. Call this method at the end of each frame. \n func (w *Window) Update() { \n + w.SwapBuffers() \n + w.UpdateInput() \n +} \n + \n +// SwapBuffers swaps buffers. Call this to swap buffers without polling window events. \n +// Note that Update invokes SwapBuffers. \n +func (w *Window) SwapBuffers() { \n mainthread.Call(func() { \n _, _, oldW, oldH := intBounds(w.bounds) \n newW, newH := w.window.GetSize() \n @@ -207,8 +214,6 @@ func (w *Window) Update() { \n w.window.SwapBuffers() \n w.end() \n }) \n - \n - w.UpdateInput() \n } \n // SetClosed sets the closed flag of the Window.", "msg": "Expose pixelgl.Window.SwapBuffers\nAllow buffers to be swapped without polling input, offering decoupling symmetrical with that provided by UpdateInput."}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -397,6 +397,7 @@ func (c *Cluster) BuildEtcdProcess(host *hosts.Host, etcdHosts []*hosts.Host) v3 \n Args: args, \n Binds: Binds, \n NetworkMode: \"host\", \n + RestartPolicy: \"always\", \n Image: c.Services.Etcd.Image, \n HealthCheck: healthCheck, \n }", "msg": "Fix etcd process regression"}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -69,7 +69,7 @@ func BuildRKEConfigNodePlan(ctx context.Context, myCluster *Cluster, host *hosts \n k8s.ExternalAddressAnnotation: host.Address, \n k8s.InternalAddressAnnotation: host.InternalAddress, \n }, \n - Labels: host.Labels, \n + Labels: host.ToAddLabels, \n } \n }", "msg": "Add role labels to node plan"}
{"diff": "a / cluster/defaults.go \n  b / cluster/defaults.go \n@@ -25,7 +25,7 @@ const ( \n DefaultAuthStrategy = \"x509\" \n DefaultAuthorizationMode = \"rbac\" \n - DefaultNetworkPlugin = \"flannel\" \n + DefaultNetworkPlugin = \"canal\" \n DefaultNetworkCloudProvider = \"none\" \n DefaultIngressController = \"nginx\"", "msg": "Change to canal as default network plugin"}
{"diff": "a / k8s/job.go \n  b / k8s/job.go \n@@ -84,7 +84,7 @@ func ensureJobDeleted(k8sClient *kubernetes.Clientset, j interface{}) error { \n } \n return err \n } \n - return fmt.Errorf(\"[k8s] Job [%s] deletion timedout. Consider increasing addon_job_timout value\", job.Name) \n + return fmt.Errorf(\"[k8s] Job [%s] deletion timed out. Consider increasing addon_job_timeout value\", job.Name) \n } \n func deleteK8sJob(k8sClient *kubernetes.Clientset, name, namespace string) error {", "msg": "Fix typo in timeout setting"}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -314,7 +314,7 @@ func (c *Cluster) BuildKubeletProcess(host *hosts.Host, prefixPath string) v3.Pr \n \"kubeconfig\": pki.GetConfigPath(pki.KubeNodeCertName), \n \"client-ca-file\": pki.GetCertPath(pki.CACertName), \n \"anonymous-auth\": \"false\", \n - \"volume-plugin-dir\": \"/var/lib/kubelet/volumeplugins\", \n + \"volume-plugin-dir\": path.Join(prefixPath, \"/var/lib/kubelet/volumeplugins\"), \n \"fail-swap-on\": strconv.FormatBool(c.Services.Kubelet.FailSwapOn), \n \"root-dir\": path.Join(prefixPath, \"/var/lib/kubelet\"), \n }", "msg": "adjust kubelet volume-plugin-dir to match root-dir"}
{"diff": "a / templates/vsphere.go \n  b / templates/vsphere.go \n@@ -41,9 +41,7 @@ vm-name = \"{{ .VsphereConfig.Global.VMName }}\" \n {{- if ne $v.Datacenters \"\" }} \n datacenters = \"{{ $v.Datacenters }}\" \n {{- end }} \n - {{- if ne $v.Datacenters \"\" }} \n - soap-roundtrip-count = \"{{ $v.Datacenters }}\" \n - {{- end }} \n + soap-roundtrip-count = \"{{ $v.RoundTripperCount }}\" \n {{- end }} \n [Workspace]", "msg": "fixes wrong value for soap-roundtrip-count in vsphere config"}
{"diff": "a / hosts/dialer.go \n  b / hosts/dialer.go \n@@ -40,7 +40,7 @@ func newDialer(h *Host, kind string) (*dialer, error) { \n netConn: \"tcp\", \n useSSHAgentAuth: h.SSHAgentAuth, \n } \n - if bastionDialer.sshKeyString == \"\" { \n + if bastionDialer.sshKeyString == \"\" && !bastionDialer.useSSHAgentAuth { \n var err error \n bastionDialer.sshKeyString, err = privateKeyPath(h.BastionHost.SSHKeyPath) \n if err != nil { \n @@ -59,7 +59,7 @@ func newDialer(h *Host, kind string) (*dialer, error) { \n bastionDialer: bastionDialer, \n } \n - if dialer.sshKeyString == \"\" { \n + if dialer.sshKeyString == \"\" && !dialer.useSSHAgentAuth { \n var err error \n dialer.sshKeyString, err = privateKeyPath(h.SSHKeyPath) \n if err != nil {", "msg": "Skip check for private key if using ssh agent"}
{"diff": "a / templates/nginx-ingress.go \n  b / templates/nginx-ingress.go \n@@ -203,6 +203,9 @@ spec: \n - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services \n - --udp-services-configmap=$(POD_NAMESPACE)/udp-services \n - --annotations-prefix=nginx.ingress.kubernetes.io \n + {{ range $k, $v := .ExtraArgs }} \n + - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}} \n + {{ end }} \n securityContext: \n capabilities: \n drop: \n @@ -210,9 +213,6 @@ spec: \n add: \n - NET_BIND_SERVICE \n runAsUser: 33 \n - {{ range $k, $v := .ExtraArgs }} \n - - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}} \n - {{ end }} \n env: \n - name: POD_NAME \n valueFrom:", "msg": "Moving extra_args iteration to allow extra_args to work successfully"}
{"diff": "a / cluster/certificates.go \n  b / cluster/certificates.go \n@@ -212,6 +212,12 @@ func GetClusterCertsFromNodes(ctx context.Context, kubeCluster *Cluster) (map[st \n for _, host := range backupHosts { \n certificates, err = pki.FetchCertificatesFromHost(ctx, kubeCluster.EtcdHosts, host, kubeCluster.SystemImages.Alpine, kubeCluster.LocalKubeConfigPath, kubeCluster.PrivateRegistriesMap) \n if certificates != nil { \n + // Handle service account token key issue \n + kubeAPICert := certificates[pki.KubeAPICertName] \n + if certificates[pki.ServiceAccountTokenKeyName].Key == nil { \n + log.Infof(ctx, \"[certificates] Creating service account token key\") \n + certificates[pki.ServiceAccountTokenKeyName] = pki.ToCertObject(pki.ServiceAccountTokenKeyName, pki.ServiceAccountTokenKeyName, \"\", kubeAPICert.Certificate, kubeAPICert.Key, nil) \n + } \n return certificates, nil \n } \n }", "msg": "Handle missing service account token key when fetching certs from nodes"}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -544,6 +544,7 @@ func (c *Cluster) BuildKubeProxyProcess(host *hosts.Host, prefixPath string) v3. \n } \n Binds := []string{ \n fmt.Sprintf(\"%s:/etc/kubernetes:z\", path.Join(prefixPath, \"/etc/kubernetes\")), \n + \"/run:/run\", \n } \n for arg, value := range c.Services.Kubeproxy.ExtraArgs {", "msg": "Fix: kube-proxy not mounting /run/xtables.lock leading to racy iptables access\nkube-proxy and other processes invoking iptables (e.g. flannel, weave) must share the host fs `/run/xtables.lock` to prevent concurrent access to iptables resulting in errors like \"iptables: Resource temporarily unavailable\"."}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -702,15 +702,11 @@ func (c *Cluster) BuildSidecarProcess() v3.Process { \n func (c *Cluster) BuildEtcdProcess(host *hosts.Host, etcdHosts []*hosts.Host, prefixPath string) v3.Process { \n nodeName := pki.GetEtcdCrtName(host.InternalAddress) \n initCluster := \"\" \n - architecture := \"amd64\" \n + architecture := host.DockerInfo.Architecture \n if len(etcdHosts) == 0 { \n initCluster = services.GetEtcdInitialCluster(c.EtcdHosts) \n - if len(c.EtcdHosts) > 0 { \n - architecture = c.EtcdHosts[0].DockerInfo.Architecture \n - } \n } else { \n initCluster = services.GetEtcdInitialCluster(etcdHosts) \n - architecture = etcdHosts[0].DockerInfo.Architecture \n } \n clusterState := \"new\"", "msg": "Use the node's architecture to build etcd process\nThis allows for mixed-architecture etcd clusters."}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -18,7 +18,7 @@ import ( \n \"github.com/rancher/rke/pki\" \n \"github.com/rancher/rke/services\" \n \"github.com/rancher/rke/util\" \n - \"github.com/rancher/types/apis/management.cattle.io/v3\" \n + v3 \"github.com/rancher/types/apis/management.cattle.io/v3\" \n \"github.com/sirupsen/logrus\" \n ) \n @@ -100,7 +100,7 @@ func BuildRKEConfigNodePlan(ctx context.Context, myCluster *Cluster, host *hosts \n func (c *Cluster) BuildKubeAPIProcess(host *hosts.Host, prefixPath string) v3.Process { \n // check if external etcd is used \n - etcdConnectionString := services.GetEtcdConnString(c.EtcdHosts, host.Address) \n + etcdConnectionString := services.GetEtcdConnString(c.EtcdHosts, host.InternalAddress) \n etcdPathPrefix := EtcdPathPrefix \n etcdClientCert := pki.GetCertPath(pki.KubeNodeCertName) \n etcdClientKey := pki.GetKeyPath(pki.KubeNodeCertName)", "msg": "Use Internal Addresses to sort the etcd connection string"}
{"diff": "a / cmd/common.go \n  b / cmd/common.go \n@@ -89,7 +89,7 @@ func ClusterInit(ctx context.Context, rkeConfig *v3.RancherKubernetesEngineConfi \n if strings.Contains(err.Error(), \"aborting upgrade\") { \n return err \n } \n - log.Warnf(ctx, \"[state] can't fetch legacy cluster state from Kubernetes\") \n + log.Warnf(ctx, \"[state] can't fetch legacy cluster state from Kubernetes: %v\", err) \n } \n // check if certificate rotate or normal init \n if kubeCluster.RancherKubernetesEngineConfig.RotateCertificates != nil {", "msg": "Log error message on failure to get legacy cluster state from k8s"}
{"diff": "a / hosts/tunnel.go \n  b / hosts/tunnel.go \n@@ -36,7 +36,6 @@ func (h *Host) TunnelUp(ctx context.Context, dialerFactory DialerFactory, cluste \n // set Docker client \n logrus.Debugf(\"Connecting to Docker API for host [%s]\", h.Address) \n h.DClient, err = client.NewClientWithOpts( \n - client.WithHost(\"unix:///var/run/docker.sock\"), \n client.WithVersion(DockerAPIVersion), \n client.WithHTTPClient(httpClient)) \n if err != nil {", "msg": "Dont set withHost on Docker client\nProblem: Because of the unix socket reference, Windows breaks on rke up\nRoot cause: Vendor of docker/docker\nResolution: I was checking what was changed in vendor but it seems this setting is not used, as we configure Docker socket per host."}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -585,6 +585,14 @@ func (c *Cluster) BuildKubeProxyProcess(host *hosts.Host, prefixPath string, svc \n CommandArgs[k] = v \n } \n } \n + // If cloudprovider is set (not empty), set the bind address because the node will not be able to retrieve it's IP address in case cloud provider changes the node object name (i.e. AWS and Openstack) \n + if c.CloudProvider.Name != \"\" { \n + if host.InternalAddress != \"\" && host.Address != host.InternalAddress { \n + CommandArgs[\"bind-address\"] = host.InternalAddress \n + } else { \n + CommandArgs[\"bind-address\"] = host.Address \n + } \n + } \n // Best security practice is to listen on localhost, but DinD uses private container network instead of Host. \n if c.DinD {", "msg": "Provide IP for kube-proxy if cloudprovider is set\nIf cloudprovider is set (not empty), set the bind address because the node will not be able to retrieve it's IP address because the nodename could be set by the cloud provider (e.g. AWS and Openstack)"}
{"diff": "a / cluster/cluster.go \n  b / cluster/cluster.go \n@@ -156,6 +156,7 @@ func (c *Cluster) DeployWorkerPlane(ctx context.Context, svcOptionData map[strin \n func parseAuditLogConfig(clusterFile string, rkeConfig *v3.RancherKubernetesEngineConfig) error { \n if rkeConfig.Services.KubeAPI.AuditLog != nil && \n rkeConfig.Services.KubeAPI.AuditLog.Enabled && \n + rkeConfig.Services.KubeAPI.AuditLog.Configuration != nil && \n rkeConfig.Services.KubeAPI.AuditLog.Configuration.Policy == nil { \n return nil \n }", "msg": "add null check for audit log config"}
{"diff": "a / services/node_util.go \n  b / services/node_util.go \n@@ -49,10 +49,14 @@ func cordonAndDrainNode(kubeClient *kubernetes.Clientset, host *hosts.Host, drai \n } \n func getDrainHelper(kubeClient *kubernetes.Clientset, upgradeStrategy v3.NodeUpgradeStrategy) drain.Helper { \n + var ignoreDaemonSets bool \n + if upgradeStrategy.DrainInput == nil || *upgradeStrategy.DrainInput.IgnoreDaemonSets { \n + ignoreDaemonSets = true \n + } \n drainHelper := drain.Helper{ \n Client: kubeClient, \n Force: upgradeStrategy.DrainInput.Force, \n - IgnoreAllDaemonSets: *upgradeStrategy.DrainInput.IgnoreDaemonSets, \n + IgnoreAllDaemonSets: ignoreDaemonSets, \n DeleteLocalData: upgradeStrategy.DrainInput.DeleteLocalData, \n GracePeriodSeconds: upgradeStrategy.DrainInput.GracePeriod, \n Timeout: time.Second * time.Duration(upgradeStrategy.DrainInput.Timeout),", "msg": "Check drainInput for nil to avoid panic"}
{"diff": "a / k8s/node.go \n  b / k8s/node.go \n@@ -24,8 +24,8 @@ const ( \n ) \n func DeleteNode(k8sClient *kubernetes.Clientset, nodeName, cloudProvider string) error { \n - \n - if cloudProvider == AWSCloudProvider { \n + // If cloud provider is configured, the node name can be set by the cloud provider, which can be different from the original node name \n + if cloudProvider != \"\" { \n node, err := GetNode(k8sClient, nodeName) \n if err != nil { \n return err", "msg": "Use node label when cloudprovider is configured\nWhen cloud provider is configured, the node name can be set by the cloud provider. If we don't account for this, the node cannot be found when we do cluster operation (for example, node delete)"}
{"diff": "a / pki/util.go \n  b / pki/util.go \n@@ -592,7 +592,7 @@ func ReadCertsAndKeysFromDir(certDir string) (map[string]CertificatePKI, error) \n for _, file := range files { \n logrus.Debugf(\"[certificates] reading file %s from directory [%s]\", file.Name(), certDir) \n - if !strings.HasSuffix(file.Name(), \"-key.pem\") && !strings.HasSuffix(file.Name(), \"-csr.pem\") { \n + if strings.HasSuffix(file.Name(), \".pem\") && !strings.HasSuffix(file.Name(), \"-key.pem\") && !strings.HasSuffix(file.Name(), \"-csr.pem\") { \n // fetching cert \n cert, err := getCertFromFile(certDir, file.Name()) \n if err != nil {", "msg": "Update cert filename validation to *.pem\nUsers leveraging custom certs directories face errors when deploying\nif the directory contains any files that do not end in .pem. This\nchange adds additional validation to ensure files are *.pem before\nattempting further logic."}
{"diff": "a / types/rke_types.go \n  b / types/rke_types.go \n@@ -34,7 +34,7 @@ type RancherKubernetesEngineConfig struct { \n // Enable/disable strict docker version checking \n IgnoreDockerVersion *bool `yaml:\"ignore_docker_version\" json:\"ignoreDockerVersion\" norman:\"default=true\"` \n // Enable/disable using cri-dockerd \n - EnableCRIDockerd *bool `yaml:\"enable_cri_dockerd\" json:\"enableCRIDockerd\" norman:\"default=false\"` \n + EnableCRIDockerd *bool `yaml:\"enable_cri_dockerd\" json:\"enableCriDockerd\" norman:\"default=false\"` \n // Kubernetes version to use (if kubernetes image is specified, image version takes precedence) \n Version string `yaml:\"kubernetes_version\" json:\"kubernetesVersion,omitempty\"` \n // List of private registries and their credentials", "msg": "Fix case for CRI Dockerd JSON key"}
{"diff": "a / cluster/plan.go \n  b / cluster/plan.go \n@@ -512,7 +512,8 @@ func (c *Cluster) BuildKubeletProcess(host *hosts.Host, serviceOptions v3.Kubern \n // cri-dockerd must be enabled if the cluster version is 1.24 and higher \n if parsedRangeAtLeast124(parsedVersion) { \n CommandArgs[\"container-runtime-endpoint\"] = \"unix:///var/run/cri-dockerd.sock\" \n - Binds = []string{fmt.Sprintf(\"%s:/var/lib/cri-dockerd:z\", path.Join(host.PrefixPath, \"/var/lib/cri-dockerd\"))} \n + Binds = []string{fmt.Sprintf(\"%s:/var/lib/cri-dockerd:z\", path.Join(host.PrefixPath, \"/var/lib/cri-dockerd\")), \n + fmt.Sprintf(\"%s:%s\", path.Join(host.PrefixPath, KubeletDockerConfigPath), \"/.docker/config.json\")} \n } \n }", "msg": "bind rke kubelet docker config path to default docker config path\nrke stores auth info in /var/lib/kubelet/config.json but cri-dockerd\nrelies on k8.io credential provider which uses only default config\nprovider, this allows cri-dockerd to pull sandbox pause image using\nprivate registry"}
{"diff": "a / pki/cert/cert.go \n  b / pki/cert/cert.go \n@@ -149,7 +149,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS \n CommonName: fmt.Sprintf(\"%s-ca@%d\", host, time.Now().Unix()), \n }, \n NotBefore: time.Now(), \n - NotAfter: time.Now().Add(time.Hour * 24 * 365), \n + NotAfter: time.Now().Add(duration365d), \n KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, \n BasicConstraintsValid: true, \n @@ -177,7 +177,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS \n CommonName: fmt.Sprintf(\"%s@%d\", host, time.Now().Unix()), \n }, \n NotBefore: time.Now(), \n - NotAfter: time.Now().Add(time.Hour * 24 * 365), \n + NotAfter: time.Now().Add(duration365d), \n KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, \n ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth},", "msg": "Use constant instead of value for cert duration"}
{"diff": "a / cluster/validation.go \n  b / cluster/validation.go \n@@ -681,7 +681,7 @@ func validatePodSecurityPolicy(c *Cluster) error { \n kubeClient, err := k8s.NewClient(c.LocalKubeConfigPath, c.K8sWrapTransport) \n if err != nil { \n // we can not tell this is invoked when creating a new cluster or updating an existing one, so skip this check \n - logrus.Warnf(\"Skip the check for PSP resource due to the failire of initializing the kubernetes client\") \n + logrus.Debugf(\"Skip the check for PSP resource due to the failire of initializing the kubernetes client\") \n return nil \n } \n pspList, _ := k8s.GetPSPList(kubeClient)", "msg": "change Warning to Debug to reduce noise in logs"}
{"diff": "a / pkg/command/install.go \n  b / pkg/command/install.go \n@@ -79,6 +79,16 @@ func createInstallerOptions(clusterFile string, cluster *config.Cluster, ctx *cl \n backupFile = filepath.Join(filepath.Dir(fullPath), fmt.Sprintf(\"%s.tar.gz\", clusterName)) \n } \n + // refuse to overwrite existing backups (NB: since we attempt to \n + // write to the file later on to check for write permissions, we \n + // inadvertently create a zero byte file even if the first step \n + // of the installer fails; for this reason it's okay to find an \n + // existing, zero byte backup) \n + stat, err := os.Stat(backupFile) \n + if err != nil && stat.Size() > 0 { \n + return nil, fmt.Errorf(\"backup %s already exists, refusing to overwrite\", backupFile) \n + } \n + \n // try to write to the file before doing anything else \n f, err := os.OpenFile(backupFile, os.O_RDWR|os.O_CREATE, 0600) \n if err != nil {", "msg": "check for existing backups to prevent accidental overwrites"}
{"diff": "a / pkg/templates/machinecontroller/webhook.go \n  b / pkg/templates/machinecontroller/webhook.go \n@@ -104,6 +104,14 @@ func WebhookDeployment(cluster *config.Cluster) (*appsv1.Deployment, error) { \n }, \n } \n + dep.Spec.Template.Spec.Tolerations = []corev1.Toleration{ \n + { \n + Key: \"node-role.kubernetes.io/master\", \n + Operator: corev1.TolerationOpExists, \n + Effect: corev1.TaintEffectNoSchedule, \n + }, \n + } \n + \n dep.Spec.Template.Spec.Containers = []corev1.Container{ \n { \n Name: \"machine-controller-webhook\",", "msg": "make the webhook actually be able to be scheduled in a master-only cluster"}
{"diff": "a / pkg/installer/installation/install.go \n  b / pkg/installer/installation/install.go \n@@ -43,6 +43,7 @@ func Install(ctx *util.Context) error { \n {Fn: util.BuildKubernetesClientset, ErrMsg: \"unable to build kubernetes clientset\", Retries: 3}, \n {Fn: features.Activate, ErrMsg: \"unable to activate features\"}, \n {Fn: applyCanalCNI, ErrMsg: \"failed to install cni plugin canal\", Retries: 3}, \n + {Fn: patchCoreDNS, ErrMsg: \"failed to patch CoreDNS\", Retries: 3}, \n {Fn: machinecontroller.EnsureMachineController, ErrMsg: \"failed to install machine-controller\", Retries: 3}, \n {Fn: machinecontroller.WaitReady, ErrMsg: \"failed to wait for machine-controller\", Retries: 3}, \n {Fn: createWorkerMachines, ErrMsg: \"failed to create worker machines\", Retries: 3},", "msg": "patch coreDNS deployment for external cloud provider"}
{"diff": "a / pkg/templates/metricsserver/deployment.go \n  b / pkg/templates/metricsserver/deployment.go \n@@ -208,6 +208,7 @@ func metricsServerDeployment() *appsv1.Deployment { \n ImagePullPolicy: corev1.PullIfNotPresent, \n Args: []string{ \n \"--kubelet-insecure-tls\", \n + \"--kubelet-preferred-address-types=InternalIP,InternalDNS,ExternalDNS,ExternalIP\", \n }, \n VolumeMounts: []corev1.VolumeMount{ \n {", "msg": "Fix metrics-server for all providers"}
{"diff": "a / hack/tools.go \n  b / hack/tools.go \n@@ -14,14 +14,13 @@ See the License for the specific language governing permissions and \n limitations under the License. \n */ \n - \n // +build tools \n package tools \n import ( \n - _ \"k8s.io/code-generator/cmd/deepcopy-gen\" \n + _ \"k8s.io/code-generator\" \n _ \"k8s.io/code-generator/cmd/conversion-gen\" \n + _ \"k8s.io/code-generator/cmd/deepcopy-gen\" \n _ \"k8s.io/code-generator/cmd/defaulter-gen\" \n - _ \"k8s.io/code-generator\" \n )", "msg": "Run gofmt for hack/tools.go"}
{"diff": "a / pkg/cmd/root.go \n  b / pkg/cmd/root.go \n@@ -74,7 +74,7 @@ func newRoot() *cobra.Command { \n fs := rootCmd.PersistentFlags() \n fs.StringVarP(&opts.TerraformState, globalTerraformFlagName, \"t\", \"\", \n - \"Source for terrafor output JSON. - to read from stdin. If path is file, contents will be used. If path is dictionary, `terraform output -json` is executed in this path\") \n + \"Source for terraform output in JSON - to read from stdin. If path is a file, contents will be used. If path is a dictionary, `terraform output -json` is executed in this path\") \n fs.StringVarP(&opts.CredentialsFilePath, globalCredentialsFlagName, \"c\", \"\", \"File to source credentials and secrets from\") \n fs.BoolVarP(&opts.Verbose, globalVerboseFlagName, \"v\", false, \"verbose\") \n fs.BoolVarP(&opts.Debug, globalDebugFlagName, \"d\", false, \"debug\")", "msg": "pkg/cmd: Fix typo in help text of flag --tfjson\nThis commit also improves the grammatical structure of the help text."}
{"diff": "a / pkg/apis/kubeone/scheme/scheme.go \n  b / pkg/apis/kubeone/scheme/scheme.go \n@@ -31,7 +31,7 @@ import ( \n var Scheme = runtime.NewScheme() \n // Codecs is a CodecFactory object used to provide encoding and decoding for the scheme \n -var Codecs = serializer.NewCodecFactory(Scheme) \n +var Codecs = serializer.NewCodecFactory(Scheme, serializer.EnableStrict) \n func init() { \n metav1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: \"v1\"})", "msg": "Enable Strict mode to decode KubeOne API"}
{"diff": "a / pkg/cmd/apply.go \n  b / pkg/cmd/apply.go \n@@ -89,7 +89,8 @@ func applyCmd(rootFlags *pflag.FlagSet) *cobra.Command { \n Use: \"apply\", \n Short: \"Reconcile the cluster\", \n Long: ` \n -Reconcile (Install/Upgrade/Repair/Restore) Kubernetes cluster on pre-existing machines \n +Reconcile (Install/Upgrade/Repair/Restore) Kubernetes cluster on pre-existing machines. MachineDeployments get \n +initialized but won't get modified by default, see '--upgrade-machine-deployments'. \n This command takes KubeOne manifest which contains information about hosts and how the cluster should be provisioned. \n It's possible to source information about hosts from Terraform output, using the '--tfjson' flag.", "msg": "add info for apply, that machine deployments won't get modified"}
{"diff": "a / pkg/terraform/v1beta2/config.go \n  b / pkg/terraform/v1beta2/config.go \n@@ -178,6 +178,7 @@ func NewConfigFromJSON(buf []byte) (*Config, error) { \n KubeoneAPI interface{} `json:\"kubeone_api\"` \n KubeoneHosts interface{} `json:\"kubeone_hosts\"` \n KubeoneWorkers interface{} `json:\"kubeone_workers\"` \n + KubeoneStaticWorkers interface{} `json:\"kubeone_static_workers\"` \n }{} \n // cat off all the excessive fields from the terraform output JSON that will prevent otherwise strict unmarshalling", "msg": "Fixed missing reading of the static workers defined in terraform"}
{"diff": "a / renderer/render.go \n  b / renderer/render.go \n@@ -80,6 +80,7 @@ func applyRenderers(data []byte, renderers []string, settings map[string]interfa \n if err != nil { \n return nil, err \n } \n + tmpl.Option(\"missingkey=error\") \n yaml := bytes.NewBuffer(nil) \n err = tmpl.Execute(yaml, settings) \n if err != nil {", "msg": "renderer: set gotemplate to err on missing variable."}
{"diff": "a / internal/packager/extract.go \n  b / internal/packager/extract.go \n@@ -118,7 +118,7 @@ func ExtractWithOrigin(appname string) (ExtractedApp, error) { \n } \n } \n originalAppname := appname \n - \n + appname = filepath.Clean(appname) \n // try appending our extension \n appname = internal.DirNameFromAppName(appname) \n s, err := os.Stat(appname)", "msg": "Extract: clean path, mainly to remove eventual trailing separator."}
{"diff": "a / e2e/commands_test.go \n  b / e2e/commands_test.go \n@@ -261,8 +261,8 @@ func TestSplitMerge(t *testing.T) { \n result = icmd.RunCommand(dockerApp, \"inspect\", \"remerged\").Assert(t, icmd.Success) \n assert.Assert(t, golden.String(result.Combined(), \"envvariables-inspect.golden\")) \n // test inplace \n - icmd.RunCommand(dockerApp, \"merge\", \"split\") \n - icmd.RunCommand(dockerApp, \"split\", \"split\") \n + icmd.RunCommand(dockerApp, \"merge\", \"split\").Assert(t, icmd.Success) \n + icmd.RunCommand(dockerApp, \"split\", \"split\").Assert(t, icmd.Success) \n } \n func TestURL(t *testing.T) {", "msg": "Add missing asserts to split/merge test (missed off as part of a previous refactor)"}
{"diff": "a / internal/packager/packing.go \n  b / internal/packager/packing.go \n@@ -14,10 +14,15 @@ import ( \n \"github.com/pkg/errors\" \n ) \n -var dockerFile = `FROM docker/cnab-app-base:` + internal.Version + ` \n -COPY . .` \n +const ( \n + // CNABBaseImageName is the name of the base invocation image. \n + CNABBaseImageName = \"docker/cnab-app-base\" \n + \n + dockerIgnore = \"Dockerfile\" \n +) \n -const dockerIgnore = \"Dockerfile\" \n +var dockerFile = `FROM ` + CNABBaseImageName + `:` + internal.Version + ` \n +COPY . .` \n func tarAdd(tarout *tar.Writer, path, file string) error { \n payload, err := ioutil.ReadFile(file)", "msg": "Use const for base invocation image name"}
{"diff": "a / types/parameters/merge_test.go \n  b / types/parameters/merge_test.go \npackage parameters \n import ( \n + \"fmt\" \n \"testing\" \n \"gotest.tools/assert\" \n @@ -42,12 +43,15 @@ func TestMerge(t *testing.T) { \n assert.NilError(t, err) \n parameters, err := Merge(m1, m2, m3) \n assert.NilError(t, err) \n + fmt.Println(parameters) \n assert.Check(t, is.DeepEqual(parameters.Flatten(), map[string]string{ \n \"foo\": \"bar\", \n \"bar.baz\": \"boz\", \n \"bar.port\": \"10\", \n \"bar.foo\": \"toto\", \n - \"baz.0\": \"c\", \n + \"baz.0\": \"a\", \n + \"baz.1\": \"b\", \n + \"baz.2\": \"c\", \n \"banana\": \"monkey\", \n })) \n }", "msg": "Update unit test since mergo 0.3.7 changed slice merge order (https://github.com/imdario/mergo/pull/102)"}
{"diff": "a / types/metadata/metadata.go \n  b / types/metadata/metadata.go \n@@ -2,6 +2,8 @@ package metadata \n import ( \n \"strings\" \n + \n + \"github.com/deislabs/cnab-go/bundle\" \n ) \n // Maintainer represents one of the apps's maintainers \n @@ -38,3 +40,19 @@ type AppMetadata struct { \n Description string `json:\"description,omitempty\"` \n Maintainers Maintainers `json:\"maintainers,omitempty\"` \n } \n + \n +// Metadata extracts the docker-app metadata from the bundle \n +func FromBundle(bndl *bundle.Bundle) AppMetadata { \n + meta := AppMetadata{ \n + Name: bndl.Name, \n + Version: bndl.Version, \n + Description: bndl.Description, \n + } \n + for _, m := range bndl.Maintainers { \n + meta.Maintainers = append(meta.Maintainers, Maintainer{ \n + Name: m.Name, \n + Email: m.Email, \n + }) \n + } \n + return meta \n +}", "msg": "metadata: Add helper to extract docker-app metadata from a bundle."}
{"diff": "a / internal/commands/cnab.go \n  b / internal/commands/cnab.go \n@@ -54,6 +54,9 @@ func addNamedCredentialSets(credStore appstore.CredentialStore, namedCredentials \n c, err = credentials.Load(file) \n } else { \n c, err = credStore.Read(file) \n + if os.IsNotExist(err) { \n + err = e \n + } \n } \n if err != nil { \n return err", "msg": "Print the original error message, the one checking the file locally, instead of the one looking for the credential set in the credential store."}
{"diff": "a / internal/packager/init.go \n  b / internal/packager/init.go \n@@ -45,11 +45,6 @@ func Init(name string, composeFile string) (string, error) { \n return \"\", err \n } \n - if composeFile == \"\" { \n - if _, err := os.Stat(internal.ComposeFileName); err == nil { \n - composeFile = internal.ComposeFileName \n - } \n - } \n if composeFile == \"\" { \n err = initFromScratch(name) \n } else {", "msg": "Do not search implicitly for a compose file\nThe user needs to explicitly give the path to the docker-compose file\nthat they want to base the app on."}
{"diff": "a / internal/commands/image/rm.go \n  b / internal/commands/image/rm.go \n@@ -14,8 +14,9 @@ import ( \n func rmCmd() *cobra.Command { \n return &cobra.Command{ \n - Use: \"rm [APP_IMAGE] [APP_IMAGE...]\", \n Short: \"Remove an application image\", \n + Use: \"rm [APP_IMAGE] [APP_IMAGE...]\", \n + Aliases: []string{\"remove\"}, \n Args: cli.RequiresMinArgs(1), \n Example: `$ docker app image rm myapp \n $ docker app image rm myapp:1.0.0", "msg": "Add \"remove\" as alias for app image rm"}
{"diff": "a / internal/commands/init.go \n  b / internal/commands/init.go \n@@ -17,7 +17,7 @@ func initCmd(dockerCli command.Cli) *cobra.Command { \n cmd := &cobra.Command{ \n Use: \"init APP_NAME [--compose-file COMPOSE_FILE] [OPTIONS]\", \n Short: \"Initialize Docker Application definition\", \n - Long: `Start building a Docker Application package. If there is a docker-compose.yml file in the current directory it will be copied and used.`, \n + Long: `Start building a Docker Application package.`, \n Example: `$ docker app init myapp`, \n Args: cli.ExactArgs(1), \n RunE: func(cmd *cobra.Command, args []string) error {", "msg": "Fix init help message.\nDocker App doesn't take a docker-compose file implicitly any more."}
{"diff": "a / internal/commands/image/inspect.go \n  b / internal/commands/image/inspect.go \n@@ -87,7 +87,7 @@ func runInspect(dockerCli command.Cli, appname string, opts inspectOptions) erro \n installation.SetParameter(internal.ParameterInspectFormatName, format) \n - if err := a.Run(&installation.Claim, nil, nil); err != nil { \n + if err := a.Run(&installation.Claim, nil); err != nil { \n return fmt.Errorf(\"inspect failed: %s\\n%s\", err, errBuf) \n } \n return nil", "msg": "cnab-go OperationConfigs panic if we pass a nil function"}
{"diff": "a / internal/commands/run.go \n  b / internal/commands/run.go \n@@ -73,7 +73,7 @@ func runCmd(dockerCli command.Cli) *cobra.Command { \n cmd.Flags().StringArrayVar(&opts.labels, \"label\", nil, \"Label to add to services\") \n //nolint:errcheck \n - cmd.Flags().SetAnnotation(\"cnab-bundle-json\", \"experimental\", []string{\"true\"}) \n + cmd.Flags().SetAnnotation(\"cnab-bundle-json\", \"experimentalCLI\", []string{\"true\"}) \n return cmd \n }", "msg": "Experimental flags are hidden by experimentalCLI\nThe code had the (wrong) annotation \"experimental\""}
{"diff": "a / internal/commands/remove.go \n  b / internal/commands/remove.go \n@@ -36,7 +36,7 @@ func removeCmd(dockerCli command.Cli, installerContext *cliopts.InstallerContext \n }, \n } \n opts.credentialOptions.addFlags(cmd.Flags()) \n - cmd.Flags().BoolVar(&opts.force, \"force\", false, \"Force the removal of a running App\") \n + cmd.Flags().BoolVarP(&opts.force, \"force\", \"f\", false, \"Force the removal of a running App\") \n return cmd \n }", "msg": "Add -f shorthand to app rm --force\nMakes -f the shorthand for --force in `docker app rm`, same as in `docker rm` and `docker app image rm`"}
{"diff": "a / pkg/daemon/rpm-ostree.go \n  b / pkg/daemon/rpm-ostree.go \n@@ -6,12 +6,14 @@ import ( \n \"strings\" \n ) \n +// RpmOstreeState houses zero or more RpmOstreeDeployments \n // Subset of `rpm-ostree status --json` \n // https://github.com/projectatomic/rpm-ostree/blob/bce966a9812df141d38e3290f845171ec745aa4e/src/daemon/rpmostreed-deployment-utils.c#L227 \n type RpmOstreeState struct { \n Deployments []RpmOstreeDeployment \n } \n +// RpmOstreeDeployment represents a single deployment on a node \n type RpmOstreeDeployment struct { \n Id string `json:\"id\"` \n OSName string `json:\"osname\"`", "msg": "daemon/rpm-ostree.go: Add doc for exported structs"}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -438,21 +438,7 @@ func getFileOwnership(file ignv2_2types.File) (int, int, error) { \n // updateOS updates the system OS to the one specified in newConfig \n func (dn *Daemon) updateOS(oldConfig, newConfig *mcfgv1.MachineConfig) error { \n - // XXX(jl): don't re-ask rpm-ostree here, just cache from checkOS() \n - bootedOSImageURL, _, err := getBootedOSImageURL() \n - if err != nil { \n - glog.Warningf(\"Cannot retrieve bootedOSImageURL.\") \n - return err \n - } \n - glog.V(2).Infof(\"Retrieved Booted OS Image URL: %s\", bootedOSImageURL) \n - \n - // see similar block in checkOS() \n - if bootedOSImageURL == \"\" { \n - bootedOSImageURL = \"://dummy\" \n - glog.V(2).Infof(\"Assigned empty Booted OS Image URL to: %s\", bootedOSImageURL) \n - } \n - \n - if newConfig.Spec.OSImageURL == bootedOSImageURL { \n + if newConfig.Spec.OSImageURL == dn.bootedOSImageURL { \n return nil \n }", "msg": "cmd/daemon: simplify OSImageURL querying\nThe booted OSImageURL is clearly not going to change during the same run\nof the daemon, so query it just once at the beginning instead. This\nsimplifies comparisons later on against new machine configs and avoids\nrunning `rpm-ostree status --json` twice."}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -134,6 +134,16 @@ func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) (bool \n return false, nil \n } \n + // Special case files append: if the new config wants us to append, then we \n + // have to force a reprovision since it's not idempotent \n + for _, f := range newIgn.Storage.Files { \n + if f.Append { \n + glog.Warningf(\"daemon can't reconcile state!\") \n + glog.Warningf(\"Ignition files includes append\") \n + return false, nil \n + } \n + } \n + \n // Systemd section \n // we can reconcile any state changes in the systemd section.", "msg": "daemon: don't try to reconcile file appends\nIt's inherently non-idempotent. See upstream Ignition issue on this:"}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -80,6 +80,12 @@ func (dn *Daemon) update(oldConfig, newConfig *mcfgv1.MachineConfig) error { \n func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) (bool, error) { \n glog.Info(\"Checking if configs are reconcilable\") \n + // We skip out of reconcilable if there is no Kind and we are in runOnce mode. The \n + // reason is that there is a good chance a previous state is not available to match against. \n + if oldConfig.Kind == \"\" && dn.onceFrom != \"\" { \n + glog.Infof(\"Missing kind in old config. Assuming reconcilable with new.\") \n + return true, nil \n + } \n oldIgn := oldConfig.Spec.Config \n newIgn := newConfig.Spec.Config", "msg": "daemon/update: Allow reconcile skip\nWhen we are in runOnce mode AND the previous MachineConfig does\nnot have a Kind we can assume that there was no previous config\nto check against."}
{"diff": "a / cmd/machine-config-daemon/start.go \n  b / cmd/machine-config-daemon/start.go \n@@ -73,7 +73,7 @@ func runStartCmd(cmd *cobra.Command, args []string) { \n // If we are asked to run once and it's a valid file system path use \n // the bare Daemon \n - if startOpts.onceFrom != \"\" && daemon.ValidPath(startOpts.onceFrom) { \n + if startOpts.onceFrom != \"\" { \n dn, err = daemon.New( \n startOpts.rootMount, \n startOpts.nodeName,", "msg": "MCD: Fix once-from start\nCurrently, if an invalid path is provided to mcd with\nonce-from, mcd will attempt to start in normal mode.\nWe validate the path later in the process, this commit\nensures we don't start mcd in normal mode by mistake."}
{"diff": "a / pkg/operator/operator.go \n  b / pkg/operator/operator.go \n@@ -270,7 +270,9 @@ func (optr *Operator) getCAsFromConfigMap(namespace, name, key string) ([]byte, \n } else if d, dok := cm.Data[key]; dok { \n raw, err := base64.StdEncoding.DecodeString(d) \n if err != nil { \n - return nil, err \n + // this is actually the result of a bad assumption. configmap values are not encoded. \n + // After the installer pull merges, this entire attempt to decode can go away. \n + return []byte(d), nil \n } \n return raw, nil \n } else {", "msg": "accept unencoded configmaps\nconfigmap values are raw strings, not base64 encoded.\nThis makes the MCO bilingual until the installer pull merges"}
{"diff": "a / cmd/machine-config-daemon/start.go \n  b / cmd/machine-config-daemon/start.go \n@@ -100,7 +100,7 @@ func runStartCmd(cmd *cobra.Command, args []string) { \n } else { \n cb, err := common.NewClientBuilder(startOpts.kubeconfig) \n if err != nil { \n - glog.Fatalf(\"failed to initialize daemon: %v\", err) \n + glog.Fatalf(\"failed to initialize ClientBuilder: %v\", err) \n } \n ctx = common.CreateControllerContext(cb, stopCh, componentName) \n // create the daemon instance. this also initializes kube client items", "msg": "daemon: tweak log message\nWe had two \"failed to initialize daemon\" log messages in the\n`runStartCmd()` function. One can still distinguish which one was\nemitted since glog does print the line number too, but really that first\none wasn't accurate, so tweak it."}
{"diff": "a / pkg/daemon/daemon.go \n  b / pkg/daemon/daemon.go \n@@ -426,7 +426,7 @@ func (dn *Daemon) executeUpdateFromClusterWithMachineConfig(desiredConfig *mcfgv \n if err := dn.triggerUpdateWithMachineConfig(desiredConfig); err != nil { \n glog.Errorf(\"Marking degraded due to: %v\", err) \n if errSet := dn.nodeWriter.SetUpdateDegraded(dn.kubeClient.CoreV1().Nodes(), dn.name); errSet != nil { \n - glog.Errorf(\"Futher error attempting to set the node to degraded: %v\", errSet) \n + glog.Errorf(\"Further error attempting to set the node to degraded: %v\", errSet) \n } \n // reboot the node, which will catch the degraded state and sleep \n dn.reboot()", "msg": "daemon: fix typo in error message"}
{"diff": "a / pkg/daemon/writer.go \n  b / pkg/daemon/writer.go \n@@ -105,7 +105,7 @@ func (nw *NodeWriter) SetUpdateDegradedIgnoreErr(err error, client corev1.NodeIn \n // log error here since the caller won't look at it \n degraded_err := nw.SetUpdateDegraded(err, client, node) \n if degraded_err != nil { \n - glog.Error(\"Error while setting degraded: %v\", degraded_err) \n + glog.Errorf(\"Error while setting degraded: %v\", degraded_err) \n } \n return err \n }", "msg": "daemon: Fix format message for degraded message\nWe were using `%v` so need `f` for format."}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -491,7 +491,10 @@ func (dn *Daemon) updateOS(oldConfig, newConfig *mcfgv1.MachineConfig) error { \n // cleans up the agent's connections, and then sleeps for 7 days. if it wakes up \n // and manages to return, it returns a scary error message. \n func (dn *Daemon) reboot(rationale string) error { \n + // We'll only have a recorder if we're cluster driven \n + if (dn.recorder != nil) { \n dn.recorder.Eventf(&corev1.Node{ObjectMeta: metav1.ObjectMeta{Name: dn.name}}, corev1.EventTypeNormal, \"Reboot\", rationale) \n + } \n glog.Infof(\"Rebooting: %s\", rationale) \n // reboot", "msg": "daemon: Only log to recorder in cluster driven model\nI was looking at this code again and realized this is probably\nan issue."}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -281,7 +281,7 @@ func (dn *Daemon) deleteStaleData(oldConfig, newConfig *mcfgv1.MachineConfig) { \n glog.V(2).Info(\"Removing stale config storage files\") \n for _, f := range oldConfig.Spec.Config.Storage.Files { \n if _, ok := newFileSet[f.Path]; !ok { \n - dn.fileSystemClient.RemoveAll(path) \n + dn.fileSystemClient.RemoveAll(f.Path) \n } \n }", "msg": "daemon: fix deletion of stale config files\nMinor regression from We weren't passing the right variable to\n`RemoveAll()`.\nCloses:"}
{"diff": "a / pkg/daemon/daemon.go \n  b / pkg/daemon/daemon.go \n@@ -141,9 +141,10 @@ func New( \n } \n osImageURL := \"\" \n + osVersion := \"\" \n // Only pull the osImageURL from OSTree when we are on RHCOS \n if operatingSystem == MachineConfigDaemonOSRHCOS { \n - osImageURL, osVersion, err := nodeUpdaterClient.GetBootedOSImageURL(rootMount) \n + osImageURL, osVersion, err = nodeUpdaterClient.GetBootedOSImageURL(rootMount) \n if err != nil { \n return nil, fmt.Errorf(\"Error reading osImageURL from rpm-ostree: %v\", err) \n }", "msg": "daemon: Fix load of booted OS\nWe can almost update but go degraded on boot because we had\nincorrect variable scoping here :cry:"}
{"diff": "a / pkg/daemon/daemon.go \n  b / pkg/daemon/daemon.go \n@@ -219,6 +219,7 @@ func NewClusterDrivenDaemon( \n eventBroadcaster.StartRecordingToSink(&clientsetcorev1.EventSinkImpl{Interface: kubeClient.CoreV1().Events(\"\")}) \n dn.recorder = eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: \"machineconfigdaemon\", Host: nodeName}) \n + glog.Infof(\"Managing node: %s\", nodeName) \n if err = loadNodeAnnotations(dn.kubeClient.CoreV1().Nodes(), nodeName); err != nil { \n return nil, err \n }", "msg": "daemon: Log the node name\nI often look at a MCD log and it'd be helpful to have the node\nright there rather than needing to describe the object.  Unlike\nmost pods, the node we're managing is rather critical information."}
{"diff": "a / pkg/daemon/node.go \n  b / pkg/daemon/node.go \n@@ -6,6 +6,7 @@ import ( \n \"io/ioutil\" \n \"time\" \n + \"github.com/golang/glog\" \n core_v1 \"k8s.io/api/core/v1\" \n metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \n \"k8s.io/apimachinery/pkg/util/wait\" \n @@ -40,6 +41,7 @@ func loadNodeAnnotations(client corev1.NodeInterface, node string) error { \n return fmt.Errorf(\"Failed to unmarshal initial annotations: %v\", err) \n } \n + glog.Infof(\"Setting initial node config: %s\", initial[CurrentMachineConfigAnnotationKey]) \n err = setNodeAnnotations(client, node, initial) \n if err != nil { \n return fmt.Errorf(\"Failed to set initial annotations: %v\", err)", "msg": "daemon: Log initial node config\nThis is also helpful to understand the flow; whether we are\nusing an initial config or pulling down a new one."}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -18,6 +18,7 @@ import ( \n \"time\" \n ignv2_2types \"github.com/coreos/ignition/config/v2_2/types\" \n + \"github.com/coreos/ignition/config/validate\" \n \"github.com/golang/glog\" \n drain \"github.com/openshift/kubernetes-drain\" \n mcfgv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" \n @@ -206,6 +207,11 @@ func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) error \n newIgn := newConfig.Spec.Config \n // Ignition section \n + // First check if this is a generally valid Ignition Config \n + rpt := validate.ValidateWithoutSource(reflect.ValueOf(newIgn)) \n + if rpt.IsFatal() { \n + return errors.Errorf(\"Invalid Ignition config found: %v\", rpt) \n + } \n // if the config versions are different, all bets are off. this probably \n // shouldn't happen, but if it does, we can't deal with it.", "msg": "add ign validation check to reconcilable()\nAdd a check using Ignition's internal validation function to ensure\nthat machineconfigs contain valid Ignition configs. If Ignition config\nis invalid, a message containing the error report will be logged.\nCloses:"}
{"diff": "a / test/e2e/mcd_test.go \n  b / test/e2e/mcd_test.go \n@@ -124,7 +124,7 @@ func TestMCDeployed(t *testing.T) { \n if err := wait.Poll(2*time.Second, 10*time.Minute, func() (bool, error) { \n nodes, err := getNodesByRole(cs, \"worker\") \n if err != nil { \n - return false, err \n + return false, nil \n } \n for _, node := range nodes { \n if visited[node.Name] {", "msg": "test/e2e: skip error check on MC deployed test\nCall can fail if network is off try reaching the nodes as happened in\nso just return  false, nil as we do in other places where we know there\nmay be a network error."}
{"diff": "a / pkg/controller/node/node_controller.go \n  b / pkg/controller/node/node_controller.go \n@@ -200,7 +200,14 @@ func (ctrl *Controller) updateNode(old, cur interface{}) { \n if pool == nil { \n return \n } \n + \n + // Specifically log when a node has completed an update so the MCC logs are a useful central aggregate of state changes \n + if oldNode.Annotations[daemonconsts.CurrentMachineConfigAnnotationKey] != oldNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey] && \n + curNode.Annotations[daemonconsts.CurrentMachineConfigAnnotationKey] == curNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey] { \n + glog.Infof(\"Pool %s: node %s has completed update to %s\", pool.Name, curNode.Name, curNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey]) \n + } else { \n glog.V(4).Infof(\"Node %s updated\", curNode.Name) \n + } \n ctrl.enqueueMachineConfigPool(pool) \n }", "msg": "controller: Log when a node has been updated\nThe MCC logs show when we tell a node to start, but don't show us\nwhen it's done.  This helps the MCC logs be a centralized\naggregate of events.\nDoing this now to aid debugging slowness in updates in our `e2e-aws-op`\ntest suite."}
{"diff": "a / pkg/daemon/daemon_test.go \n  b / pkg/daemon/daemon_test.go \n@@ -128,38 +128,6 @@ func TestCompareOSImageURL(t *testing.T) { \n } \n } \n -func TestDaemonOnceFromNoPanic(t *testing.T) { \n - if _, err := os.Stat(\"/proc/sys/kernel/random/boot_id\"); os.IsNotExist(err) { \n - t.Skip(\"we're not on linux\") \n - } \n - \n - exitCh := make(chan error) \n - defer close(exitCh) \n - stopCh := make(chan struct{}) \n - defer close(stopCh) \n - \n - // This is how a onceFrom daemon is initialized \n - // and it shouldn't panic assuming kubeClient is there \n - dn, err := New( \n - \"/\", \n - \"testnodename\", \n - \"testos\", \n - NewNodeUpdaterClient(), \n - \"\", \n - \"test\", \n - false, \n - nil, \n - k8sfake.NewSimpleClientset(), \n - false, \n - \"\", \n - nil, \n - exitCh, \n - stopCh, \n - ) \n - require.Nil(t, err) \n - require.NotPanics(t, func() { dn.triggerUpdateWithMachineConfig(&mcfgv1.MachineConfig{}, &mcfgv1.MachineConfig{}) }) \n -} \n - \n type fixture struct { \n t *testing.T", "msg": "daemon: Remove dummy OnceFrom Test\nNow that the daemon tries to talk to the systemd journal, this unit\ntest hangs in my dev environment.  We can't really support this\nwithout mocking a lot more, and it's not really covering much\ninteresting right now.  Delete it."}
{"diff": "a / pkg/daemon/daemon.go \n  b / pkg/daemon/daemon.go \n@@ -778,7 +778,6 @@ func (dn *Daemon) getPendingConfig() (string, error) { \n if !os.IsNotExist(err) { \n return \"\", errors.Wrapf(err, \"loading transient state\") \n } \n - dn.logSystem(\"error loading pending config %v\", err) \n return \"\", nil \n } \n var p pendingConfigState", "msg": "daemon: Drop unnecessary logSystem\nWe show this log in the normal case where the file exists.\nI don't think we need to `logSystem` this anyways."}
{"diff": "a / cmd/gcp-routes-controller/run.go \n  b / cmd/gcp-routes-controller/run.go \n@@ -82,6 +82,7 @@ func runRunCmd(cmd *cobra.Command, args []string) error { \n errCh := make(chan error) \n tracker := &healthTracker{ \n + state: unknownTrackerState, \n ErrCh: errCh, \n SuccessThreshold: 2, \n FailureThreshold: 10,", "msg": "cmd/gcp-routes-controller: init tracker state to unknown\ngolangci-lint doesn't allow deafult enum value to be unsed. :("}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -161,9 +161,11 @@ func (dn *Daemon) drain() error { \n if err == wait.ErrWaitTimeout { \n failMsg := fmt.Sprintf(\"%d tries: %v\", backoff.Steps, lastErr) \n MCDDrainErr.WithLabelValues(failTime, failMsg).SetToCurrentTime() \n + dn.recorder.Eventf(getNodeRef(dn.node), corev1.EventTypeWarning, \"FailedToDrain\", failMsg) \n return errors.Wrapf(lastErr, \"failed to drain node (%d tries): %v\", backoff.Steps, err) \n } \n MCDDrainErr.WithLabelValues(failTime, err.Error()).SetToCurrentTime() \n + dn.recorder.Eventf(getNodeRef(dn.node), corev1.EventTypeWarning, \"FailedToDrain\", err.Error()) \n return errors.Wrap(err, \"failed to drain node\") \n }", "msg": "pkg/daemon: Add event for drain failures"}
{"diff": "a / pkg/controller/common/helpers.go \n  b / pkg/controller/common/helpers.go \n@@ -33,7 +33,7 @@ func MergeMachineConfigs(configs []*mcfgv1.MachineConfig, osImageURL string) (*m \n if len(configs) == 0 { \n return nil, nil \n } \n - sort.Slice(configs, func(i, j int) bool { return configs[i].Name < configs[j].Name }) \n + sort.SliceStable(configs, func(i, j int) bool { return configs[i].Name < configs[j].Name }) \n var fips, ok bool \n var kernelType string", "msg": "pkg/controller/common: use sort.SliceStable to keep same elements original order"}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -347,6 +347,7 @@ func (dn *Daemon) applyOSChanges(oldConfig, newConfig *mcfgv1.MachineConfig) (re \n // Update OS \n if err := dn.updateOS(newConfig, osImageContentDir); err != nil { \n + MCDPivotErr.WithLabelValues(dn.node.Name, newConfig.Spec.OSImageURL, err.Error()).SetToCurrentTime() \n return err \n }", "msg": "daemon: add back metrics for pivot error"}
{"diff": "a / pkg/operator/status.go \n  b / pkg/operator/status.go \n@@ -72,6 +72,11 @@ func (optr *Operator) syncRelatedObjects() error { \n {Group: \"machineconfiguration.openshift.io\", Resource: \"machineconfigs\"}, \n // gathered because the machineconfigs created container bootstrap credentials and node configuration that gets reflected via the API and is needed for debugging \n {Group: \"\", Resource: \"nodes\"}, \n + // Gathered for the on-prem services running in static pods. \n + {Resource: \"namespaces\", Name: \"openshift-kni-infra\"}, \n + {Resource: \"namespaces\", Name: \"openshift-openstack-infra\"}, \n + {Resource: \"namespaces\", Name: \"openshift-ovirt-infra\"}, \n + {Resource: \"namespaces\", Name: \"openshift-vsphere-infra\"}, \n } \n if !equality.Semantic.DeepEqual(coCopy.Status.RelatedObjects, co.Status.RelatedObjects) {", "msg": "Add on-prem namespaces to relatedObjects\nPreviously the kni namespace was explicitly listed in must-gather,\nbut when I went to add the other on-prem ones it was pointed out\nthat this should probably be done with relatedObjects. This adds\nthe namespaces here so we don't need anything in must-gather itself."}
{"diff": "a / pkg/daemon/update.go \n  b / pkg/daemon/update.go \n@@ -46,8 +46,8 @@ const ( \n osImageContentBaseDir = \"/run/mco-machine-os-content/\" \n // These are the actions for a node to take after applying config changes. (e.g. a new machineconfig is applied) \n - // \"None\" means no special action needs to be taken. A drain will still happen. \n - // This currently happens when ssh keys or pull secret (/var/lib/kubelet/config.json) is changed \n + // \"None\" means no special action needs to be taken \n + // This happens for example when ssh keys or the pull secret (/var/lib/kubelet/config.json) is changed \n postConfigChangeActionNone = \"none\" \n // Rebooting is still the default scenario for any other change \n postConfigChangeActionReboot = \"reboot\"", "msg": "Update postConfigChangeActionNone comment\nA drain no longer occurs, and another case, kubelet-ca.crt, has been\nadded"}
{"diff": "a / pkg/controller/drain/drain_controller.go \n  b / pkg/controller/drain/drain_controller.go \n@@ -292,7 +292,6 @@ func (ctrl *Controller) syncNode(key string) error { \n // This is a bit problematic in practice since we don't really have a previous state. \n // TODO (jerzhang) consider using a new CRD for coordination \n - ctrl.logNode(node, \"initiating drain\") \n ongoingDrain := false \n for k, v := range ctrl.ongoingDrains { \n if k != node.Name { \n @@ -318,6 +317,7 @@ func (ctrl *Controller) syncNode(key string) error { \n } \n // Attempt drain \n + ctrl.logNode(node, \"initiating drain\") \n if err := drain.RunNodeDrain(drainer, node.Name); err != nil { \n ctrl.logNode(node, \"Drain failed, but overall timeout has not been reached. Waiting 1 minute then retrying. Error message from drain: %v\", err) \n ctrl.enqueueAfter(node, drainRequeueDelay)", "msg": "Move drain log message to when drain starts"}
{"diff": "a / pkg/daemon/rpm-ostree_test.go \n  b / pkg/daemon/rpm-ostree_test.go \n@@ -46,3 +46,15 @@ func (r RpmOstreeClientMock) GetStatus() (string, error) { \n func (r RpmOstreeClientMock) GetBootedDeployment() (*RpmOstreeDeployment, error) { \n return &RpmOstreeDeployment{}, nil \n } \n + \n +func (r RpmOstreeClientMock) GetBootedAndStagedDeployment() (booted, staged *RpmOstreeDeployment, err error) { \n + return nil, nil, nil \n +} \n + \n +func (r RpmOstreeClientMock) IsBootableImage(string) (bool, error) { \n + return false, nil \n +} \n + \n +func (r RpmOstreeClientMock) RebaseLayered(string) error { \n + return nil \n +}", "msg": "Update rpm-ostree tests with new signatures\nWe changed the interface for RpmOstreeClient, so we need to make sure\nthe mocks are also up to date with the new signatures"}
{"diff": "a / pkg/controller/common/helpers.go \n  b / pkg/controller/common/helpers.go \n@@ -585,8 +585,6 @@ func decompressPayload(r io.Reader) ([]byte, error) { \n return nil, errConfigNotGzipped \n } \n - out := bytes.NewBuffer([]byte{}) \n - \n gz, err := gzip.NewReader(in) \n if err != nil { \n return nil, fmt.Errorf(\"initialize gzip reader failed: %w\", err) \n @@ -594,12 +592,12 @@ func decompressPayload(r io.Reader) ([]byte, error) { \n defer gz.Close() \n - // Decompress our payload. \n - if _, err := io.Copy(out, gz); err != nil { \n + data, err := ioutil.ReadAll(gz) \n + if err != nil { \n return nil, fmt.Errorf(\"decompression failed: %w\", err) \n } \n - return out.Bytes(), nil \n + return data, nil \n } \n // Function to remove duplicated files/units/users from a V2 MC, since the translator", "msg": "fixes config decompression lint issue\nThis was caught by the gosec linter in the Hypershift project; gosec\nG110."}
{"diff": "a / handlers/secrets_api_test.go \n  b / handlers/secrets_api_test.go \n@@ -12,7 +12,7 @@ import ( \n \"strings\" \n \"testing\" \n - \"github.com/openfaas/faas/gateway/requests\" \n + types \"github.com/openfaas/faas-provider/types\" \n v1 \"k8s.io/api/core/v1\" \n metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \n testclient \"k8s.io/client-go/kubernetes/fake\" \n @@ -127,7 +127,7 @@ func Test_SecretsHandler(t *testing.T) { \n decoder := json.NewDecoder(resp.Body) \n - secretList := []requests.Secret{} \n + secretList := []types.Secret{} \n err := decoder.Decode(&secretList) \n if err != nil { \n t.Error(err)", "msg": "Move to faas-provider for the secrets struct"}
{"diff": "a / k8s/proxy.go \n  b / k8s/proxy.go \n@@ -73,7 +73,15 @@ func (l *FunctionLookup) Resolve(name string) (url.URL, error) { \n return url.URL{}, fmt.Errorf(\"error listing %s.%s %s\", name, namespace, err.Error()) \n } \n + if len(svc.Subsets) == 0 { \n + return url.URL{}, fmt.Errorf(\"no subsets available for %s.%s\", name, namespace) \n + } \n + \n all := len(svc.Subsets[0].Addresses) \n + if len(svc.Subsets[0].Addresses) == 0 { \n + return url.URL{}, fmt.Errorf(\"no addresses in subset for %s.%s\", name, namespace) \n + } \n + \n target := rand.Intn(all) \n serviceIP := svc.Subsets[0].Addresses[target].IP", "msg": "Add range checking for endpoints\nThis could fail when endpoints are removed during a scale down\noperation."}
{"diff": "a / main.go \n  b / main.go \n@@ -145,10 +145,10 @@ func runController(setup serverSetup) { \n endpointsInformer := kubeInformerFactory.Core().V1().Endpoints() \n - log.Println(\"waiting for openfaas CRD cache sync\") \n + log.Println(\"Waiting for openfaas CRD cache sync\") \n faasInformerFactory.WaitForCacheSync(stopCh) \n setup.profileInformerFactory.WaitForCacheSync(stopCh) \n - log.Println(\"cache sync complete\") \n + log.Println(\"Cache sync complete\") \n go faasInformerFactory.Start(stopCh) \n go kubeInformerFactory.Start(stopCh) \n go setup.profileInformerFactory.Start(stopCh)", "msg": "Startup log lines start with captial letters\n**What**\nuse capital letters during the server startup log lines. This will\nkeep the logs consistent"}
{"diff": "a / middleware/kubernetes/reverse_test.go \n  b / middleware/kubernetes/reverse_test.go \n@@ -96,6 +96,13 @@ func TestReverse(t *testing.T) { \n test.SOA(\"0.10.in-addr.arpa. 300 IN SOA ns.dns.0.10.in-addr.arpa. hostmaster.0.10.in-addr.arpa. 1502782828 7200 1800 86400 60\"), \n }, \n }, \n + { \n + Qname: \"example.org.cluster.local.\", Qtype: dns.TypePTR, \n + Rcode: dns.RcodeSuccess, \n + Ns: []dns.RR{ \n + test.SOA(\"cluster.local. 300 IN SOA ns.dns.cluster.local. hostmaster.cluster.local. 1502989566 7200 1800 86400 60\"), \n + }, \n + }, \n } \n ctx := context.TODO() \n @@ -109,9 +116,6 @@ func TestReverse(t *testing.T) { \n t.Errorf(\"Test %d: expected no error, got %v\", i, err) \n return \n } \n - if tc.Error != nil { \n - continue \n - } \n resp := w.Msg \n if resp == nil {", "msg": "mw/kubernetes: add reverse test case\nAdd a non-arpa testcase to the reverse test."}
{"diff": "a / core/dnsserver/server.go \n  b / core/dnsserver/server.go \n@@ -289,6 +289,12 @@ func DefaultErrorFunc(w dns.ResponseWriter, r *dns.Msg, rc int) { \n answer := new(dns.Msg) \n answer.SetRcode(r, rc) \n + if r == nil { \n + log.Printf(\"[WARNING] DefaultErrorFunc called with nil *dns.Msg (Remote: %s)\", w.RemoteAddr().String()) \n + w.WriteMsg(answer) \n + return \n + } \n + \n state.SizeAndDo(answer) \n vars.Report(state, vars.Dropped, rcode.ToString(rc), answer.Len(), time.Now())", "msg": "core: add nil check\nCheck if msg is nil in DefaultErrorFunc. If this is the case log this\nand short cut the function.\nHoping to get more insight in"}
{"diff": "a / core/dnsserver/register.go \n  b / core/dnsserver/register.go \n@@ -124,12 +124,6 @@ func (c *Config) AddPlugin(m plugin.Plugin) { \n c.Plugin = append(c.Plugin, m) \n } \n -// AddMiddleware adds a plugin to a site's plugin stack. This method is deprecated, use AddPlugin. \n -func (c *Config) AddMiddleware(m plugin.Plugin) { \n - println(\"deprecated: use AddPlugin\") \n - c.AddPlugin(m) \n -} \n - \n // registerHandler adds a handler to a site's handler registration. Handlers \n // use this to announce that they exist to other plugin. \n func (c *Config) registerHandler(h plugin.Handler) {", "msg": "core: Remove AddMiddleware\nThis does not help to make it backwards compatible. The middleware ->\nplugin rename invalidates all this. External middleware won't compile\neither way."}
{"diff": "a / core/dnsserver/server.go \n  b / core/dnsserver/server.go \n@@ -179,6 +179,13 @@ func (s *Server) Address() string { return s.Addr } \n // defined in the request so that the correct zone \n // (configuration and plugin stack) will handle the request. \n func (s *Server) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) { \n + // our dns library protects us against really invalid packets, we can still \n + // get semi valid packets. Drop them here. \n + if r == nil || len(r.Question) == 0 { \n + DefaultErrorFunc(w, r, dns.RcodeServerFailure) \n + return \n + } \n + \n if !s.debug { \n defer func() { \n // In case the user doesn't enable error plugin, we still", "msg": "core: drop invalid packets\nWe can still be on the receiving end of invalid packet. Drop them\nhere."}
{"diff": "a / core/dnsserver/server-grpc.go \n  b / core/dnsserver/server-grpc.go \n@@ -133,8 +133,7 @@ func (s *ServergRPC) Query(ctx context.Context, in *pb.DnsPacket) (*pb.DnsPacket \n return nil, fmt.Errorf(\"no TCP peer in gRPC context: %v\", p.Addr) \n } \n - r := &net.IPAddr{IP: a.IP} \n - w := &gRPCresponse{localAddr: s.listenAddr, remoteAddr: r, Msg: msg} \n + w := &gRPCresponse{localAddr: s.listenAddr, remoteAddr: a, Msg: msg} \n s.ServeDNS(ctx, w, msg)", "msg": "Pass net.TCPAddr type as response address to gRPCresponse writer\nDnstap require protocol, address and port info about peer. So, I\nremoved conversion of TCPAddr to IPAddr"}
{"diff": "a / plugin/proxy/dns.go \n  b / plugin/proxy/dns.go \n@@ -63,10 +63,12 @@ func (d *dnsEx) Exchange(ctx context.Context, addr string, state request.Request \n if err != nil { \n return nil, err \n } \n - // Make sure it fits in the DNS response. \n - reply, _ = state.Scrub(reply) \n reply.Compress = true \n reply.Id = state.Req.Id \n + // When using force_tcp the upstream can send a message that is too big for \n + // the udp buffer, hence we need to truncate the message to at least make it \n + // fit the udp buffer. \n + reply, _ = state.Scrub(reply) \n return reply, nil \n }", "msg": "plugin/proxy: Fix unnecessary message truncation\nAs plugin/proxy always returns compressed messages, it's important to\nset this before calling Scrub(), as some messages will be unnecessarily\ntruncated otherwise."}
{"diff": "a / plugin/cache/item.go \n  b / plugin/cache/item.go \n@@ -32,7 +32,7 @@ func newItem(m *dns.Msg, now time.Time, d time.Duration) *item { \n i.Answer = m.Answer \n i.Ns = m.Ns \n i.Extra = make([]dns.RR, len(m.Extra)) \n - // Don't copy OPT record as these are hop-by-hop. \n + // Don't copy OPT records as these are hop-by-hop. \n j := 0 \n for _, e := range m.Extra { \n if e.Header().Rrtype == dns.TypeOPT { \n @@ -75,12 +75,11 @@ func (i *item) toMsg(m *dns.Msg, now time.Time) *dns.Msg { \n m1.Ns[j] = dns.Copy(r) \n m1.Ns[j].Header().Ttl = ttl \n } \n + // newItem skips OPT records, so we can just use i.Extra as is. \n for j, r := range i.Extra { \n m1.Extra[j] = dns.Copy(r) \n - if m1.Extra[j].Header().Rrtype != dns.TypeOPT { \n m1.Extra[j].Header().Ttl = ttl \n } \n - } \n return m1 \n }", "msg": "plugin/cache: don't recheck the OPT records\nThese are not stored with newItem so we don't have to check them later."}
{"diff": "a / plugin/forward/metrics.go \n  b / plugin/forward/metrics.go \n@@ -44,7 +44,7 @@ var ( \n SocketGauge = prometheus.NewGaugeVec(prometheus.GaugeOpts{ \n Namespace: plugin.Namespace, \n Subsystem: \"forward\", \n - Name: \"socket_count_total\", \n + Name: \"sockets_open\", \n Help: \"Gauge of open sockets per upstream.\", \n }, []string{\"to\"}) \n )", "msg": "Rename forward metrics socket_count_total to sockets_open\nThe prometheus naming convention states only counters should have a\n`_total` suffix, so that gagues and counters can be easily\ndistinguished."}
{"diff": "a / plugin/file/file.go \n  b / plugin/file/file.go \n@@ -121,6 +121,12 @@ func (s *serialErr) Error() string { \n // it returns an error indicating nothing was read. \n func Parse(f io.Reader, origin, fileName string, serial int64) (*Zone, error) { \n tokens := dns.ParseZone(f, dns.Fqdn(origin), fileName) \n + defer func() { \n + // Drain the tokens chan so that large zone files won't \n + // leak goroutines and memory. \n + for range tokens { \n + } \n + }() \n z := NewZone(origin, fileName) \n seenSOA := false \n for x := range tokens {", "msg": "plugin/file: Fix memory leak in Parse\nFor zone files with more than 10,000 records, the goroutines and memory\npinned by dns.ParseZone won't be released unless the tokens chan is\ndrained. As Parse is called by (*Zone).Reload very frequently, this\ncauses memory leaks and OOM conditions.\nUpdates miekg/dns#786"}
{"diff": "a / test/etcd_credentials_test.go \n  b / test/etcd_credentials_test.go \n@@ -37,9 +37,21 @@ func TestEtcdCredentials(t *testing.T) { \n if _, err := etc.Client.RoleAdd(ctx, \"root\"); err != nil { \n t.Errorf(\"Failed to create root role: %s\", err) \n } \n + defer func() { \n + if _, err := etc.Client.RoleDelete(ctx, \"root\"); err != nil { \n + t.Errorf(\"Failed to delete root role: %s\", err) \n + } \n + }() \n + \n if _, err := etc.Client.UserAdd(ctx, username, password); err != nil { \n t.Errorf(\"Failed to create user: %s\", err) \n } \n + defer func() { \n + if _, err := etc.Client.UserDelete(ctx, username); err != nil { \n + t.Errorf(\"Failed to delete user: %s\", err) \n + } \n + }() \n + \n if _, err := etc.Client.UserGrantRole(ctx, username, \"root\"); err != nil { \n t.Errorf(\"Failed to assign role to root user: %v\", err) \n }", "msg": "Fix etcd_cache_test to runnable multiple times.\nCurrently, when you run `TestEtcdCredentials` at etcd_credentials_test.go multiple times without clearing data of etcd, you will get following errors.\n```\netcd_credentials_test.go:38: Failed to create root role: etcdserver: role name already exists\netcd_credentials_test.go:41: Failed to create user: etcdserver: user name already exists\n```"}
{"diff": "a / plugin/metadata/provider.go \n  b / plugin/metadata/provider.go \n// \n // Basic example: \n // \n -// Implement the Provider interface for a plugin: \n +// Implement the Provider interface for a plugin p: \n +// \n +// func (p P) Metadata(ctx context.Context, state request.Request) context.Context { \n +// metadata.SetValueFunc(ctx, \"test/something\", func() string { return \"myvalue\" }) \n +// return ctx \n +// } \n +// \n +// Basic example with caching: \n // \n // func (p P) Metadata(ctx context.Context, state request.Request) context.Context { \n // cached := \"\" \n // return ctx \n // } \n // \n -// Check the metadata from another plugin: \n +// If you need access to this metadata from another plugin: \n // \n // // ... \n // valueFunc := metadata.ValueFunc(ctx, \"test/something\")", "msg": "plugin/metadata: tweak the docs a little\nAdd a simpler example that returns a static string the for metadata."}
{"diff": "a / plugin/pkg/log/log_test.go \n  b / plugin/pkg/log/log_test.go \n@@ -16,6 +16,7 @@ func TestDebug(t *testing.T) { \n if x := f.String(); x != \"\" { \n t.Errorf(\"Expected no debug logs, got %s\", x) \n } \n + f.Reset() \n D = true \n Debug(\"debug\") \n @@ -34,6 +35,7 @@ func TestDebugx(t *testing.T) { \n if x := f.String(); !strings.Contains(x, debug+\"debug\") { \n t.Errorf(\"Expected debug log to be %s, got %s\", debug+\"debug\", x) \n } \n + f.Reset() \n Debug(\"debug\") \n if x := f.String(); !strings.Contains(x, debug+\"debug\") { \n @@ -50,10 +52,12 @@ func TestLevels(t *testing.T) { \n if x := f.String(); !strings.Contains(x, info+ts) { \n t.Errorf(\"Expected log to be %s, got %s\", info+ts, x) \n } \n + f.Reset() \n Warning(ts) \n if x := f.String(); !strings.Contains(x, warning+ts) { \n t.Errorf(\"Expected log to be %s, got %s\", warning+ts, x) \n } \n + f.Reset() \n Error(ts) \n if x := f.String(); !strings.Contains(x, err+ts) { \n t.Errorf(\"Expected log to be %s, got %s\", err+ts, x)", "msg": "pkg/log: reset the buffer in the tests\nReset the buf otherwise we're not checking the new value."}
{"diff": "a / plugin/health/overloaded.go \n  b / plugin/health/overloaded.go \n@@ -53,7 +53,7 @@ var ( \n Buckets: plugin.SlimTimeBuckets, \n Help: \"Histogram of the time (in seconds) each request took.\", \n }) \n - // HealthFailures is the metric used to count how many times the thealth request failed \n + // HealthFailures is the metric used to count how many times the health request failed \n HealthFailures = promauto.NewCounter(prometheus.CounterOpts{ \n Namespace: plugin.Namespace, \n Subsystem: \"health\",", "msg": "Fix a typo in plugin/health"}
{"diff": "a / coremain/version.go \n  b / coremain/version.go \n@@ -2,7 +2,7 @@ package coremain \n // Various CoreDNS constants. \n const ( \n - CoreVersion = \"1.9.3\" \n + CoreVersion = \"1.9.4\" \n coreName = \"CoreDNS\" \n serverType = \"dns\" \n )", "msg": "Update Release note and prepare 1.9.4 release\n* Update Release note and prepare 1.9.4 release\nThis PR updates release notes and prepare 1.9.4 release\n* Remove spaces\n* Add additional note for header plugin change."}
{"diff": "a / setup.go \n  b / setup.go \n@@ -203,7 +203,14 @@ func getFrontMatter(conf *Config) string { \n log.Println(err) \n fmt.Printf(\"Can't get the default frontmatter from the configuration. %s will be used.\\n\", format) \n } else { \n - bytes = frontmatter.AppendRune(bytes, frontmatter.StringFormatToRune(format)) \n + r, err := frontmatter.StringFormatToRune(format) \n + if err != nil { \n + log.Println(err) \n + fmt.Printf(\"Can't get the default frontmatter from the configuration. %s will be used.\\n\", format) \n + return format \n + } \n + \n + bytes = frontmatter.AppendRune(bytes, r) \n f, err := frontmatter.Unmarshal(bytes) \n if err != nil {", "msg": "fix critical bug"}
{"diff": "a / http/resource.go \n  b / http/resource.go \n@@ -125,7 +125,10 @@ func resourcePostHandler(fileCache FileCache) handleFunc { \n } \n err = d.RunHook(func() error { \n - info, _ := writeFile(d.user.Fs, r.URL.Path, r.Body) \n + info, writeErr := writeFile(d.user.Fs, r.URL.Path, r.Body) \n + if writeErr != nil { \n + return writeErr \n + } \n etag := fmt.Sprintf(`\"%x%x\"`, info.ModTime().UnixNano(), info.Size()) \n w.Header().Set(\"ETag\", etag) \n @@ -155,7 +158,10 @@ var resourcePutHandler = withUser(func(w http.ResponseWriter, r *http.Request, d \n } \n err := d.RunHook(func() error { \n - info, _ := writeFile(d.user.Fs, r.URL.Path, r.Body) \n + info, writeErr := writeFile(d.user.Fs, r.URL.Path, r.Body) \n + if writeErr != nil { \n + return writeErr \n + } \n etag := fmt.Sprintf(`\"%x%x\"`, info.ModTime().UnixNano(), info.Size()) \n w.Header().Set(\"ETag\", etag)", "msg": "fix: error causes panic on upload"}
{"diff": "a / src/integration_tests/doppler/doppler_suite_test.go \n  b / src/integration_tests/doppler/doppler_suite_test.go \n@@ -93,7 +93,7 @@ var _ = BeforeEach(func() { \n conn.Close() \n return true \n } \n - Eventually(dopplerStartedFn).Should(BeTrue()) \n + Eventually(dopplerStartedFn, 3).Should(BeTrue()) \n localIPAddress, _ = localip.LocalIP() \n Eventually(func() error { \n _, err := etcdAdapter.Get(\"healthstatus/doppler/z1/doppler_z1/0\")", "msg": "Allow three seconds for doppler to come up"}
{"diff": "a / src/metron/component_tests/metron_test.go \n  b / src/metron/component_tests/metron_test.go \n@@ -145,13 +145,14 @@ var _ = Describe(\"Metron\", func() { \n } \n client := metronClient(metronConfig) \n - ctx, _ := context.WithDeadline(context.Background(), time.Now().Add(10*time.Second)) \n + ctx, _ := context.WithDeadline(context.Background(), time.Now().Add(20*time.Second)) \n sender, err := client.Sender(ctx) \n Expect(err).ToNot(HaveOccurred()) \n go func() { \n for { \n sender.Send(emitEnvelope) \n + time.Sleep(10 * time.Millisecond) \n } \n }() \n @@ -159,14 +160,13 @@ var _ = Describe(\"Metron\", func() { \n Eventually(consumerServer.V2.SenderInput.Arg0).Should(Receive(&rx)) \n f := func() bool { \n - sender.Send(emitEnvelope) \n envelope, err := rx.Recv() \n Expect(err).ToNot(HaveOccurred()) \n return envelope.GetCounter() != nil && \n envelope.GetCounter().GetTotal() > 5 \n } \n - Eventually(f, 15, \"1ns\").Should(Equal(true)) \n + Eventually(f, 20, \"1ns\").Should(Equal(true)) \n }) \n })", "msg": "Tweak metron component test to prevent deadlock"}
{"diff": "a / src/integration_tests/endtoend/endtoend_suite_test.go \n  b / src/integration_tests/endtoend/endtoend_suite_test.go \n@@ -10,7 +10,9 @@ import ( \n func TestIntegrationTest(t *testing.T) { \n RegisterFailHandler(Fail) \n - RunSpecs(t, \"End to end Integration Test Suite\") \n + \n + // This test is pending for being flaky... \n + // RunSpecs(t, \"End to end Integration Test Suite\") \n } \n var _ = SynchronizedBeforeSuite(func() []byte {", "msg": "Fully pend end2end test\nThe spec is pended, however the setup code is still running"}
{"diff": "a / src/metron/internal/ingress/v2/receiver.go \n  b / src/metron/internal/ingress/v2/receiver.go \n@@ -35,6 +35,8 @@ func (s *Receiver) Sender(sender v2.Ingress_SenderServer) error { \n } \n s.dataSetter.Set(e) \n + // metric-documentation-v2: (loggregator.metron.ingress) The number of \n + // received messages over Metrons V2 gRPC API. \n s.ingressMetric.Increment(1) \n } \n @@ -53,6 +55,8 @@ func (s *Receiver) BatchSender(sender v2.Ingress_BatchSenderServer) error { \n s.dataSetter.Set(e) \n } \n + // metric-documentation-v2: (loggregator.metron.ingress) The number of \n + // received messages over Metrons V2 gRPC API. \n s.ingressMetric.Increment(uint64(len(envelopes.Batch))) \n }", "msg": "add back metric documentation for metron ingress"}
{"diff": "a / src/metron/internal/clientpool/v1/pusher_fetcher_test.go \n  b / src/metron/internal/clientpool/v1/pusher_fetcher_test.go \n@@ -44,7 +44,7 @@ var _ = Describe(\"PusherFetcher\", func() { \n Expect(registry.GetValue(\"doppler_v1_streams\")).To(Equal(int64(1))) \n }) \n - It(\"decremtns a counter when a connection is closed\", func() { \n + It(\"decremetns a counter when a connection is closed\", func() { \n server := newSpyIngestorServer() \n Expect(server.Start()).To(Succeed()) \n defer server.Stop() \n @@ -108,7 +108,7 @@ func newSpyIngestorServer() *SpyIngestorServer { \n } \n func (s *SpyIngestorServer) Start() error { \n - lis, err := net.Listen(\"tcp\", \":0\") \n + lis, err := net.Listen(\"tcp\", \"localhost:0\") \n if err != nil { \n return err \n }", "msg": "explicitly set host to localhost in clientpool test"}
{"diff": "a / src/tools/udpwriter/main.go \n  b / src/tools/udpwriter/main.go \n@@ -20,7 +20,7 @@ import ( \n ) \n var ( \n - target = flag.String(\"target\", \"\", \"the host:port of the target metron\") \n + target = flag.String(\"target\", \"localhost:3457\", \"the host:port of the target metron\") \n fast = flag.Duration(\"fast\", time.Second, \"the delay of the fast writer\") \n slow = flag.Duration(\"slow\", time.Second, \"the delay of the slow writer\") \n )", "msg": "Default udpwriter to talking to local metron"}
{"diff": "a / src/rlp/internal/egress/server.go \n  b / src/rlp/internal/egress/server.go \n@@ -72,15 +72,15 @@ func (s *Server) Receiver(r *v2.EgressRequest, srv v2.Egress_ReceiverServer) err \n return io.ErrUnexpectedEOF \n } \n - // metric-documentation-v2: (egress) Number of v2 envelopes sent to RLP \n - // consumers. \n + // metric-documentation-v2: (loggregator.rlp.egress) Number of v2 \n + // envelopes sent to RLP consumers. \n s.egressMetric.Increment(1) \n } \n } \n func (s *Server) Alert(missed int) { \n - // metric-documentation-v2: (dropped) Number of v2 envelopes dropped \n - // while egressing to a consumer. \n + // metric-documentation-v2: (loggregator.rlp.dropped) Number of v2 \n + // envelopes dropped while egressing to a consumer. \n s.droppedMetric.Increment(uint64(missed)) \n log.Printf(\"Dropped (egress) %d envelopes\", missed) \n }", "msg": "Updates metric documentation\nTo be consistent with other metric docs."}
{"diff": "a / src/tools/reliability/server/internal/api/integration_test.go \n  b / src/tools/reliability/server/internal/api/integration_test.go \n@@ -43,7 +43,7 @@ func initiateTest(createTestHandler http.Handler) *httptest.ResponseRecorder { \n } \n func attachWorker(workerHandler http.Handler) func() { \n - d := wstest.NewDialer(workerHandler) \n + d := wstest.NewDialer(workerHandler, nil) \n c, _, err := d.Dial(\"ws://localhost:8080/ws\", nil) \n Expect(err).ToNot(HaveOccurred()) \n return func() {", "msg": "Fix reliability server integration tests"}
{"diff": "a / src/integration_tests/endtoend/endtoend_test.go \n  b / src/integration_tests/endtoend/endtoend_test.go \n@@ -25,10 +25,9 @@ var _ = Describe(\"End to end tests\", func() { \n defer ingressCleanup() \n trafficcontrollerCleanup, tcPorts := testservers.StartTrafficController( \n - testservers.BuildTrafficControllerConf( \n + testservers.BuildTrafficControllerConfWithoutLogCache( \n fmt.Sprintf(\"127.0.0.1:%d\", dopplerPorts.GRPC), \n 0, \n - fmt.Sprintf(\"127.0.0.1:%d\", 0), \n ), \n ) \n defer trafficcontrollerCleanup()", "msg": "update test to not fail because blocking on connection to log cache"}
{"diff": "a / src/trafficcontroller/app/traffic_controller.go \n  b / src/trafficcontroller/app/traffic_controller.go \npackage app \n import ( \n - \"code.cloudfoundry.org/tlsconfig\" \n \"crypto/tls\" \n \"fmt\" \n \"log\" \n @@ -11,6 +10,8 @@ import ( \n \"os/signal\" \n \"time\" \n + \"code.cloudfoundry.org/tlsconfig\" \n + \n logcache \"code.cloudfoundry.org/log-cache/pkg/client\" \n \"code.cloudfoundry.org/loggregator/metricemitter\" \n \"code.cloudfoundry.org/loggregator/plumbing\" \n @@ -89,7 +90,13 @@ func (t *TrafficController) Start() { \n Timeout: 20 * time.Second, \n PermitWithoutStream: true, \n } \n - pool := plumbing.NewPool(20, grpc.WithTransportCredentials(creds), grpc.WithKeepaliveParams(kp)) \n + \n + pool := plumbing.NewPool( \n + 20, \n + grpc.WithTransportCredentials(creds), \n + grpc.WithKeepaliveParams(kp), \n + grpc.WithDisableServiceConfig(), \n + ) \n grpcConnector := plumbing.NewGRPCConnector(1000, pool, f, t.metricClient) \n var logCacheClient proxy.LogCacheClient", "msg": "remove service config from traffic controller grpc config to reduce dns requests"}
{"diff": "a / src/rlp/main.go \n  b / src/rlp/main.go \n@@ -103,7 +103,17 @@ func main() { \n app.WithMaxEgressStreams(conf.MaxEgressStreams), \n ) \n go rlp.Start() \n - defer rlp.Stop() \n + \n + defer func() { \n + go func() { \n + // Limit the shutdown to 30 seconds \n + <-time.Tick(30 * time.Second) \n + os.Exit(0) \n + }() \n + \n + rlp.Stop() \n + }() \n + \n go profiler.New(conf.PProfPort).Start() \n killSignal := make(chan os.Signal, 1)", "msg": "Fix issue where rlp somethimes doesn't stop\nEnforce exit 30 seconds after calling stop for graceful shutdown"}
{"diff": "a / src/router/internal/server/v2/egress_server_test.go \n  b / src/router/internal/server/v2/egress_server_test.go \n@@ -236,7 +236,7 @@ var _ = Describe(\"EgressServer\", func() { \n Eventually( \n egressDropped.GetDelta, \n - 10).Should(BeNumerically(\">\", 1)) \n + 20).Should(BeNumerically(\">\", 1)) \n }) \n }) \n })", "msg": "Increase timeout for egress server dropped metric test"}
{"diff": "a / restapi/configure_weaviate.go \n  b / restapi/configure_weaviate.go \n@@ -326,6 +326,7 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { \n locationObject := &models.LocationGetResponse{} \n json.Unmarshal([]byte(locationDatabaseObject.Object), locationObject) \n locationObject.ID = strfmt.UUID(locationDatabaseObject.Uuid) \n + locationObject.Kind = getKind(locationObject) \n responseObject.Locations[i] = locationObject \n } \n @@ -505,6 +506,7 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { \n thingTemplateObject := &models.ThingTemplateGetResponse{} \n json.Unmarshal([]byte(thingTemplatesDatabaseObject.Object), thingTemplateObject) \n thingTemplateObject.ID = strfmt.UUID(thingTemplatesDatabaseObject.Uuid) \n + thingTemplateObject.Kind = getKind(thingTemplateObject) \n responseObject.ThingTemplates[i] = thingTemplateObject \n }", "msg": "Add kind to inner list response."}
{"diff": "a / test/acceptance/graphql_schema_test.go \n  b / test/acceptance/graphql_schema_test.go \n@@ -264,6 +264,27 @@ func (g *GraphQLResult) AssertKey(t *testing.T, key string) *GraphQLResult { \n return &GraphQLResult{Result: x} \n } \n +// TODO Use this function instead of AssertKey(t, \"x\").AssetKey(t, \"y\"): \n +// AssertKeys(t, []string{\"x\",\"y\",}) \n +func (g* GraphQLResult) AsserKeyPath(t *testing.T, keys []string) *GraphQLResult { \n + // currently found result. \n + r := g.Result \n + \n + for _, key := range keys { \n + m, ok := r.(map[string]interface{}) \n + if !ok { \n + t.Fatalf(\"Can't index into key %s, because this is not a map\", key) \n + } \n + \n + r, ok = m[key] \n + if !ok { \n + t.Fatalf(\"Can't index into key %s, because no such key exists\", key) \n + } \n + } \n + \n + return &GraphQLResult{Result: r} \n +} \n + \n // Assert that this is a slice. \n // Wraps a GraphQLResult over all children too. \n func (g *GraphQLResult) AssertSlice(t *testing.T) []*GraphQLResult {", "msg": "Add helper function to assert a path of keys."}
{"diff": "a / database/connectors/janusgraph/resolver.go \n  b / database/connectors/janusgraph/resolver.go \n@@ -26,7 +26,14 @@ func (j *Janusgraph) LocalGetClass(params *graphql_local_get.LocalGetClassParams \n ch := make(chan resolveResult, 1) \n go func() { \n - defer close(ch) \n + defer func() { \n + if r := recover(); r != nil { \n + // send error over the channel \n + ch <- resolveResult{err: fmt.Errorf(\"Janusgraph.LocalGetClass paniced: %#v\", r)} \n + } \n + close(ch) \n + }() \n + \n results := []interface{}{} \n className := schema.AssertValidClassName(params.ClassName)", "msg": "Capture panics so that Weaviate won't crash"}
{"diff": "a / database/schema_manager/local/validation.go \n  b / database/schema_manager/local/validation.go \n@@ -39,7 +39,7 @@ func (l *localSchemaManager) validateCanAddClass(knd kind.Kind, class *models.Se \n // Validate data type of property. \n schema := l.GetSchema() \n - err, _ := (&schema).FindPropertyDataType(property.AtDataType) \n + _, err := (&schema).FindPropertyDataType(property.AtDataType) \n if err != nil { \n return fmt.Errorf(\"Data type fo property '%s' is invalid; %v\", property.Name, err) \n }", "msg": "Update call to FindPropertyDataType to reflect change in ret types.\nMissed this one last location of call to FindPropertyDataType"}
{"diff": "a / database/connectors/janusgraph/helper_update_class.go \n  b / database/connectors/janusgraph/helper_update_class.go \n@@ -225,6 +225,9 @@ func addPrimitivePropToQuery(q *gremlin.Query, propType schema.PropertyDataType, \n default: \n return q, fmt.Errorf(\"Illegal value for property %s\", sanitizedPropertyName) \n } \n - } \n + default: \n panic(fmt.Sprintf(\"Unkown primitive datatype %s\", propType.AsPrimitive())) \n } \n + \n + return q, nil \n +}", "msg": "(refactor) fix incorrect error handling on updateClass\nThe previous refactor introduced a small issue where after successful\ntype assertion, we would still panic because we didn't return before."}
{"diff": "a / database/connectors/janusgraph/helper_add_class.go \n  b / database/connectors/janusgraph/helper_add_class.go \n@@ -51,7 +51,11 @@ type batchChunk struct { \n func (j *Janusgraph) addThingsBatch(things []*models.Thing, uuids []strfmt.UUID) error { \n chunkSize := MaximumBatchItemsPerQuery \n - chunked := make([][]batchChunk, len(things)/chunkSize) \n + chunks := len(things) / chunkSize \n + if len(things) < chunkSize { \n + chunks = 1 \n + } \n + chunked := make([][]batchChunk, chunks) \n chunk := 0 \n for i := 0; i < len(things); i++ {", "msg": "fix issue if batch size < max chunk size\nPreviously we'd create an array of length 0"}
{"diff": "a / restapi/configure_api.go \n  b / restapi/configure_api.go \n@@ -3,6 +3,7 @@ package restapi \n import ( \n \"net/http\" \n + \"github.com/creativesoftwarefdn/weaviate/models\" \n \"github.com/creativesoftwarefdn/weaviate/restapi/batch\" \n \"github.com/creativesoftwarefdn/weaviate/restapi/operations\" \n \"github.com/go-openapi/errors\" \n @@ -14,7 +15,9 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { \n api.JSONConsumer = runtime.JSONConsumer() \n - api.OidcAuth = appState.OIDC.ValidateAndExtract \n + api.OidcAuth = func(token string, scopes []string) (*models.Principal, error) { \n + return appState.OIDC.ValidateAndExtract(token, scopes) \n + } \n setupSchemaHandlers(api) \n setupThingsHandlers(api)", "msg": "set OidcAuth to anonymous closure\nOtherwise the value of appState.OIDC will forever stay null. This way we\nwill only retrieve appState.OIDC whenever the auth middleware gets\ncalled."}
{"diff": "a / database/connectors/janusgraph/fetchfuzzy/query.go \n  b / database/connectors/janusgraph/fetchfuzzy/query.go \n@@ -69,11 +69,23 @@ func (b *Query) predicates() ([]*gremlin.Query, error) { \n return nil, fmt.Errorf(\"could not get mapped names: %v\", err) \n } \n + // janusgraph does not allow more than 253 arguments, so we must abort once \n + // we hit too many \n + argsCounter := 0 \n + limit := 253 \n + \n +outer: \n for _, prop := range mappedProps { \n for _, searchterm := range b.params { \n + if argsCounter >= limit { \n + break outer \n + } \n + \n result = append(result, \n gremlin.New().Has(string(prop), gremlin.New().TextContainsFuzzy(searchterm)), \n ) \n + \n + argsCounter++ \n } \n }", "msg": "add hard-query parameter limit\njanusgraph claims the limit is 256, but trial and error has shown that\nanything above 253 will cause errors"}
{"diff": "a / test/acceptance/database_schema/add_class_test.go \n  b / test/acceptance/database_schema/add_class_test.go \n@@ -128,6 +128,17 @@ func TestDeleteSingleProperties(t *testing.T) { \n _, err = helper.Client(t).Things.WeaviateThingsList(things.NewWeaviateThingsListParams(), nil) \n assert.Nil(t, err, \"listing things should not error\") \n + t.Log(\"verifying we could re-add the property with the same name\") \n + readdParams := schema.NewWeaviateSchemaThingsPropertiesAddParams(). \n + WithClassName(randomThingClassName). \n + WithBody(&models.SemanticSchemaClassProperty{ \n + Name: \"description\", \n + DataType: []string{\"string\"}, \n + }) \n + \n + _, err = helper.Client(t).Schema.WeaviateSchemaThingsPropertiesAdd(readdParams, nil) \n + assert.Nil(t, err, \"adding the previously deleted property again should not error\") \n + \n // Now clean up this class. \n t.Log(\"Remove the class\") \n delParams := schema.NewWeaviateSchemaThingsDeleteParams().WithClassName(randomThingClassName)", "msg": "extend test to also cover\nWe had the assumption that the fix for the former might also help with\nfixing the latter. This test proves that re-adding a previously deleted\nproperty should be possible now.\ncloses"}
{"diff": "a / adapters/repos/esvector/cache_integration_test.go \n  b / adapters/repos/esvector/cache_integration_test.go \n@@ -298,7 +298,7 @@ func testEsVectorCache(t *testing.T) { \n }) \n // wait for both es indexing as well as esvector caching to be complete \n - time.Sleep(1000 * time.Millisecond) \n + time.Sleep(2000 * time.Millisecond) \n t.Run(\"all 3 (outer) things must now have a hot cache\", func(t *testing.T) { \n res, err := repo.ThingByID(context.Background(), \"18c80a16-346a-477d-849d-9d92e5040ac9\", \n @@ -426,7 +426,7 @@ func testEsVectorCache(t *testing.T) { \n refreshAll(t, client) \n // wait for both es indexing as well as esvector caching to be complete \n - time.Sleep(1000 * time.Millisecond) \n + time.Sleep(2000 * time.Millisecond) \n t.Run(\"the newly added place must have a hot cache by now\", func(t *testing.T) { \n res, err := repo.ThingByID(context.Background(), \"0f02d525-902d-4dc0-8052-647cb420c1a6\", traverser.SelectProperties{},", "msg": "increase wait for cache periods in integration tests\nas we are adding more integration tests it seems they take longer on CI,\nso we need to wait longer, too."}
{"diff": "a / usecases/classification/classifier.go \n  b / usecases/classification/classifier.go \n@@ -191,9 +191,23 @@ func (c *Classifier) setDefaultValuesForOptionalFields(params *models.Classifica \n params.Type = &defaultType \n } \n + if *params.Type == \"knn\" { \n + c.setDefaultsForKNN(params) \n + } \n + \n + if *params.Type == \"contextual\" { \n + c.setDefaultsForContextual(params) \n + } \n + \n +} \n + \n +func (c *Classifier) setDefaultsForKNN(params *models.Classification) { \n if params.K == nil { \n defaultK := int32(3) \n params.K = &defaultK \n } \n +} \n +func (c *Classifier) setDefaultsForContextual(params *models.Classification) { \n + // none at the moment \n }", "msg": "only set defaults for respective types\nfixes"}
{"diff": "a / modules/text2vec-contextionary/extensions/usecase.go \n  b / modules/text2vec-contextionary/extensions/usecase.go \n@@ -57,13 +57,13 @@ func (uc *UseCase) LoadAll() ([]byte, error) { \n _, err = buf.Write([]byte(\"\\n\")) \n if err != nil { \n - return false, errors.Wrapf(err, \"write newline separator\") \n + return false, errors.Wrap(err, \"write newline separator\") \n } \n return true, nil \n }) \n if err != nil { \n - return nil, errors.Wrapf(err, \"load all concepts\") \n + return nil, errors.Wrap(err, \"load all concepts\") \n } \n return buf.Bytes(), nil", "msg": "use Wrap instead of Wrapf where no formatting is required"}
{"diff": "a / adapters/repos/db/vector/hnsw/search.go \n  b / adapters/repos/db/vector/hnsw/search.go \n@@ -321,10 +321,17 @@ func (h *hnsw) knnSearchByVector(searchVec []float32, k int, \n if err != nil { \n return nil, errors.Wrapf(err, \"knn search: search layer at level %d\", level) \n } \n + \n + // There might be situations where we did not find a better entrypoint at \n + // that particular level, so instead we're keeping whatever entrypoint we \n + // had before (i.e. either from a previous level or even the main \n + // entrypoint) \n + if res.root != nil { \n best := res.minimum() \n entryPointID = best.index \n entryPointDistance = best.dist \n } \n + } \n eps := &binarySearchTreeGeneric{} \n eps.insert(entryPointID, entryPointDistance)", "msg": "add entrypoint nil-check to hnsw\nThis might otherwise lead to panics"}
{"diff": "a / adapters/repos/db/inverted/searcher.go \n  b / adapters/repos/db/inverted/searcher.go \n@@ -76,6 +76,11 @@ func (f *Searcher) Object(ctx context.Context, limit int, \n return errors.Wrap(err, \"merge doc ids by operator\") \n } \n + // cutoff if required, e.g. after merging unlimted filters \n + if len(pointers.docIDs) > limit { \n + pointers.docIDs = pointers.docIDs[:limit] \n + } \n + \n res, err := docid.ObjectsInTx(tx, pointers.IDs()) \n if err != nil { \n return errors.Wrap(err, \"resolve doc ids to objects\")", "msg": "provide missing test and fix ignored limit on chained filters"}
{"diff": "a / adapters/repos/db/vector/hnsw/search_with_max_dist.go \n  b / adapters/repos/db/vector/hnsw/search_with_max_dist.go \n@@ -42,7 +42,7 @@ func (h *hnsw) KnnSearchByVectorMaxDist(searchVec []float32, dist float32, \n return nil, errors.Wrapf(err, \"knn search: search layer at level %d\", level) \n } \n if res.Len() > 0 { \n - best := eps.Pop() \n + best := res.Pop() \n entryPointID = best.ID \n entryPointDistance = best.Dist \n }", "msg": "fix broken geo distance search"}
{"diff": "a / adapters/repos/db/vector/hnsw/deserializer2.go \n  b / adapters/repos/db/vector/hnsw/deserializer2.go \n@@ -266,7 +266,7 @@ func (c *Deserializer2) ReadDeleteNode(r io.Reader, res *DeserializationResult) \n func (c *Deserializer2) readUint64(r io.Reader) (uint64, error) { \n var value uint64 \n tmpBuf := make([]byte, 8) \n - if _, err := io.ReadFull(r, tmpBuf); err != nil { \n + if _, err := r.Read(tmpBuf); err != nil { \n return 0, err \n } \n @@ -278,7 +278,7 @@ func (c *Deserializer2) readUint64(r io.Reader) (uint64, error) { \n func (c *Deserializer2) readUint16(r io.Reader) (uint16, error) { \n var value uint16 \n tmpBuf := make([]byte, 2) \n - if _, err := io.ReadFull(r, tmpBuf); err != nil { \n + if _, err := r.Read(tmpBuf); err != nil { \n return 0, err \n } \n value = binary.LittleEndian.Uint16(tmpBuf)", "msg": "Replace io.ReadAll() usage with r.Read() to fix failing integration tests"}
{"diff": "a / usecases/cluster/state.go \n  b / usecases/cluster/state.go \n@@ -2,6 +2,7 @@ package cluster \n import ( \n \"fmt\" \n + \"net\" \n \"strings\" \n \"github.com/hashicorp/memberlist\" \n @@ -43,11 +44,20 @@ func Init(userConfig Config, logger logrus.FieldLogger) (*State, error) { \n } \n if len(joinAddr) > 0 { \n + \n + _, err := net.LookupIP(strings.Split(joinAddr[0], \":\")[0]) \n + if err != nil { \n + logger.WithField(\"action\", \"cluster_attempt_join\"). \n + WithField(\"remote_hostname\", joinAddr[0]). \n + WithError(err). \n + Warn(\"specified hostname to join cluster cannot be resolved. This is fine\" + \n + \"if this is the first node of a new cluster, but problematic otherwise.\") \n + } else { \n _, err := list.Join(joinAddr) \n if err != nil { \n return nil, errors.Wrap(err, \"join cluster\") \n } \n - \n + } \n } \n return &State{list: list}, nil", "msg": "try to resolve hostname, skip if not resolvable"}
{"diff": "a / modules/qna-transformers/additional/answer/answer_test.go \n  b / modules/qna-transformers/additional/answer/answer_test.go \n@@ -453,8 +453,10 @@ func TestAdditionalAnswerProvider(t *testing.T) { \n assert.Equal(t, true, answerAdditional.HasAnswer) \n answer, answerOK = in[1].AdditionalProperties[\"answer\"] \n assert.False(t, answerOK) \n + assert.Nil(t, answer) \n answer, answerOK = in[2].AdditionalProperties[\"answer\"] \n assert.False(t, answerOK) \n + assert.Nil(t, answer) \n }) \n }", "msg": "fixed tests for qna reranking feature"}
{"diff": "a / adapters/repos/db/lsmkv/bucket.go \n  b / adapters/repos/db/lsmkv/bucket.go \n@@ -353,20 +353,24 @@ func (b *Bucket) setNewActiveMemtable() error { \n } \n func (b *Bucket) Count() int { \n + b.flushLock.RLock() \n + defer b.flushLock.RUnlock() \n + \n if b.strategy != StrategyReplace { \n panic(\"Count() called on strategy other than 'replace'\") \n } \n - memtableCount := b.memtableNetCount() \n - fmt.Printf(\"memtable count is %d\\n\", memtableCount) \n + memtableCount := b.memtableNetCount(b.active.countStats()) \n + if b.flushing != nil { \n + memtableCount += b.memtableNetCount(b.flushing.countStats()) \n + } \n diskCount := b.disk.count() \n return memtableCount + diskCount \n } \n -func (b *Bucket) memtableNetCount() int { \n +func (b *Bucket) memtableNetCount(stats *countStats) int { \n netCount := 0 \n - stats := b.active.countStats() \n // TODO: this uses regular get, given that this may be called quite commonly, \n // we might consider building a pure Exists(), which skips reading the value", "msg": "make sure to include flushing memtable if flush cycle is running"}
{"diff": "a / adapters/repos/db/inverted/bm25_searcher.go \n  b / adapters/repos/db/inverted/bm25_searcher.go \n@@ -116,17 +116,6 @@ func (b *BM25Searcher) retrieveScoreAndSortForSingleTerm(ctx context.Context, \n } \n fmt.Printf(\"term %q: score ids took %s\\n\", term, time.Since(before)) \n - before = time.Now() \n - // TODO: this runtime sorting is only because the storage is not implemented \n - // in an always sorted manner. Once we have that implemented, we can skip \n - // this expensive runtime-sort \n - sort.Slice(ids.docIDs, func(a, b int) bool { \n - return ids.docIDs[a].id < ids.docIDs[b].id \n - }) \n - \n - // TODO: structured logging \n - fmt.Printf(\"term %q: sorting by doc ids took %s\\n\", term, time.Since(before)) \n - \n return ids, nil \n }", "msg": "remove temporary doc id sorting from bm25\nnow that the Map type in the lsmkv supports sorted keys and the doc ids\nare encoded is big endian, any time we retrieve doc ids, we can be\ncertain that they are already sorted."}
{"diff": "a / adapters/repos/db/lsmkv/bucket_threshold_test.go \n  b / adapters/repos/db/lsmkv/bucket_threshold_test.go \n@@ -164,9 +164,9 @@ func TestMemtableThreshold_Replace(t *testing.T) { \n // give the bucket time to fill up beyond threshold \n time.Sleep(time.Millisecond) \n - if !isSizeWithinTolerance(t, bucket.active.size, memtableThreshold, tolerance) { \n + if !isSizeWithinTolerance(t, bucket.active.Size(), memtableThreshold, tolerance) { \n t.Fatalf(\"memtable size (%d) was allowed to increase beyond threshold (%d) with tolerance of (%f)%%\", \n - bucket.active.size, memtableThreshold, tolerance) \n + bucket.active.Size(), memtableThreshold, tolerance) \n } else { \n done <- true \n }", "msg": "use .Size() instead of .size in test to prevent a race condition"}
{"diff": "a / adapters/repos/db/shard.go \n  b / adapters/repos/db/shard.go \n@@ -104,10 +104,13 @@ func NewShard(ctx context.Context, promMetrics *monitoring.PrometheusMetrics, \n case \"l2-squared\": \n distProv = distancer.NewL2SquaredProvider() \n // here I can add new cases for manhattan distance, minowski distance etc \n + case \"manhattan\": \n + // set distProv as distancer.NewManhattanProvide() \n + distProv = distancer.NewManhattanProvider() \n default: \n return nil, errors.Errorf(\"unrecognized distance metric %q,\"+ \n \"choose one of [\\\"cosine\\\", \\\"l2-squared\\\"]\", hnswUserConfig.Distance) \n - } \n + } // add manhanntan to this default as well \n vi, err := hnsw.New(hnsw.Config{ \n Logger: index.logger,", "msg": "Add code to include manhattan distance"}
{"diff": "a / adapters/repos/db/crud.go \n  b / adapters/repos/db/crud.go \n@@ -167,7 +167,7 @@ func (d *DB) Exists(ctx context.Context, class string, id strfmt.UUID) (bool, er \n } \n index := d.GetIndex(schema.ClassName(class)) \n if index == nil { \n - return false, fmt.Errorf(\"index not found for class %s\", class) \n + return false, nil \n } \n return index.exists(ctx, id) \n }", "msg": "do not return an error if index is not found while checking for object existence"}
{"diff": "a / adapters/repos/db/inverted/objects_test.go \n  b / adapters/repos/db/inverted/objects_test.go \n@@ -313,6 +313,7 @@ func TestAnalyzeObject(t *testing.T) { \n }) \n t.Run(\"with array properties\", func(t *testing.T) { \n + //nolint:gofumpt // the following block is formatted differently with go1.18 vs go1.19 \n schema := map[string]interface{}{ \n \"descriptions\": []interface{}{\"I am great!\", \"I am also great!\"}, \n \"emails\": []interface{}{\"john@doe.com\", \"john2@doe.com\"},", "msg": "temporarily ignore linting of code block that differs\nbetween 1.18 and 1.19\nThis can be removed when golangci-lint can run with go 1.19"}
{"diff": "a / adapters/repos/db/vector/hnsw/vector_cache.go \n  b / adapters/repos/db/vector/hnsw/vector_cache.go \n@@ -242,6 +242,14 @@ func (c *shardedLockCache) obtainAllLocks() { \n // dimensionality \n func (c *shardedLockCache) dimensions(id uint64) int { \n c.shardedLocks[id%shardFactor].RLock() \n + if int(id) >= len(c.cache) { \n + // if an object has no vector it could have a higher doc id than the size \n + // of the vector index, in this case we can assume it has no dimensions \n + // (since it had no vector), but cannot actually check, as this would lead \n + // to an OOM error \n + c.shardedLocks[id%shardFactor].RUnlock() \n + return 0 \n + } \n vec := c.cache[id] \n c.shardedLocks[id%shardFactor].RUnlock()", "msg": "handle out-of-range in temporary dimension logic"}
{"diff": "a / modules/backup-s3/s3/s3.go \n  b / modules/backup-s3/s3/s3.go \n@@ -28,8 +28,6 @@ import ( \n ) \n const ( \n - AWS_ROLE_ARN = \"AWS_ROLE_ARN\" \n - AWS_WEB_IDENTITY_TOKEN_FILE = \"AWS_WEB_IDENTITY_TOKEN_FILE\" \n AWS_REGION = \"AWS_REGION\" \n AWS_DEFAULT_REGION = \"AWS_DEFAULT_REGION\" \n ) \n @@ -46,10 +44,12 @@ func New(config Config, logger logrus.FieldLogger, dataPath string) (*s3, error) \n if len(region) == 0 { \n region = os.Getenv(AWS_DEFAULT_REGION) \n } \n - creds := credentials.NewEnvAWS() \n - if len(os.Getenv(AWS_WEB_IDENTITY_TOKEN_FILE)) > 0 && len(os.Getenv(AWS_ROLE_ARN)) > 0 { \n - creds = credentials.NewIAM(\"\") \n + \n + creds := credentials.NewIAM(\"\") \n + if _, err := creds.Get(); err != nil { \n + creds = credentials.NewEnvAWS() \n } \n + \n client, err := minio.New(config.Endpoint(), &minio.Options{ \n Creds: creds, \n Region: region,", "msg": "Add support for AWS IAM based authorization"}
{"diff": "a / adapters/repos/db/shard_dimension_tracking_test.go \n  b / adapters/repos/db/shard_dimension_tracking_test.go \n@@ -18,7 +18,6 @@ import ( \n \"context\" \n \"fmt\" \n \"math/rand\" \n - \"os\" \n \"testing\" \n \"time\" \n @@ -37,11 +36,7 @@ func Benchmark_Migration(b *testing.B) { \n for i := 0; i < b.N; i++ { \n rand.Seed(time.Now().UnixNano()) \n - dir := b.TempDir() \n - \n - defer os.RemoveAll(dir) \n - \n - dirName := os.TempDir() \n + dirName := b.TempDir() \n shardState := singleShardState() \n logger := logrus.New()", "msg": "Better dirname"}
{"diff": "a / usecases/cluster/state.go \n  b / usecases/cluster/state.go \n@@ -36,8 +36,6 @@ func Init(userConfig Config, logger logrus.FieldLogger) (*State, error) { \n cfg := memberlist.DefaultLocalConfig() \n cfg.LogOutput = newLogParser(logger) \n - cfg.AdvertiseAddr = \"127.0.0.1\" \n - \n if userConfig.Hostname != \"\" { \n cfg.Name = userConfig.Hostname \n }", "msg": "Remove temp hack to get Weaviate running without internet access"}
{"diff": "a / usecases/schema/get.go \n  b / usecases/schema/get.go \n@@ -27,8 +27,10 @@ func (m *Manager) GetSchema(principal *models.Principal) (schema.Schema, error) \n return schema.Schema{}, err \n } \n + m.Lock() \n + defer m.Unlock() \n return schema.Schema{ \n - Objects: m.state.ObjectSchema, \n + Objects: m.state.ObjectSchema.Deepcopy(), \n }, nil \n }", "msg": "Add locking for getting schema with public call"}
{"diff": "a / usecases/schema/get.go \n  b / usecases/schema/get.go \n@@ -54,6 +54,8 @@ func (m *Manager) getSchema() schema.Schema { \n } \n func (m *Manager) IndexedInverted(className, propertyName string) bool { \n + m.Lock() \n + defer m.Unlock() \n class := m.getClassByName(className) \n if class == nil { \n return false", "msg": "Add lock to public schemanager function"}
{"diff": "a / pkg/results/reader.go \n  b / pkg/results/reader.go \n@@ -5,6 +5,7 @@ import ( \n \"bytes\" \n \"compress/gzip\" \n \"encoding/json\" \n + \"encoding/xml\" \n \"errors\" \n \"io\" \n \"os\" \n @@ -155,8 +156,20 @@ func ExtractIntoStruct(predicate func(string) bool, path string, info os.FileInf \n if !ok { \n return errors.New(\"info.Sys() is not a reader\") \n } \n + // TODO(chuckha) there must be a better way \n + if strings.HasSuffix(path, \"xml\") { \n + decoder := xml.NewDecoder(reader) \n + err := decoder.Decode(object) \n + if err != nil { \n + return err \n + } \n + return nil \n + } \n + \n + // If it's not xml it's probably json \n decoder := json.NewDecoder(reader) \n err := decoder.Decode(object) \n + \n if err != nil { \n return err \n }", "msg": "Detect type to decode"}
{"diff": "a / pkg/discovery/discovery.go \n  b / pkg/discovery/discovery.go \n@@ -61,6 +61,10 @@ func Run(kubeClient kubernetes.Interface, cfg *config.Config) (errCount int) { \n logrus.AddHook(hook) \n + // Unset all hooks as we exit the Run function \n + defer func() { \n + logrus.StandardLogger().Hooks = make(logrus.LevelHooks) \n + }() \n // closure used to collect and report errors. \n trackErrorsFor := func(action string) func(error) { \n return func(err error) {", "msg": "Remove logrus hooks at the end of the run.\nSonobuoy has captured everything interesting and has created the results.\nNo more logs should be written to disk at this point."}
{"diff": "a / pkg/templates/manifest.go \n  b / pkg/templates/manifest.go \n@@ -38,7 +38,7 @@ kind: ClusterRoleBinding \n metadata: \n labels: \n component: sonobuoy \n - name: sonobuoy-serviceaccount \n + name: sonobuoy-serviceaccount-{{.Namespace}} \n roleRef: \n apiGroup: rbac.authorization.k8s.io \n kind: ClusterRole", "msg": "Namespace serviceaccount\nFixes The issue was that serviceaccounts would not be created by new runs,\nbut would still point to a single namespace."}
{"diff": "a / pkg/plugin/driver/daemonset/daemonset.go \n  b / pkg/plugin/driver/daemonset/daemonset.go \n@@ -102,7 +102,7 @@ func (p *Plugin) Run(kubeclient kubernetes.Interface, hostname string, cert *tls \n return errors.Wrap(err, \"couldn't fill template\") \n } \n if err := kuberuntime.DecodeInto(scheme.Codecs.UniversalDecoder(), b, &daemonSet); err != nil { \n - return errors.Wrapf(err, \"could not decode the executed template into a daemonset. Plugin name: \", p.GetName()) \n + return errors.Wrapf(err, \"could not decode the executed template into a daemonset. Plugin name: %v\", p.GetName()) \n } \n secret, err := p.MakeTLSSecret(cert)", "msg": "Fix go vet error in daemonset.go"}
{"diff": "a / pkg/discovery/queries.go \n  b / pkg/discovery/queries.go \n@@ -137,6 +137,11 @@ func QueryResources( \n ns *string, \n cfg *config.Config) error { \n + // Early exit; avoid forming query or creating output directories. \n + if len(resources) == 0 { \n + return nil \n + } \n + \n if ns != nil { \n logrus.Infof(\"Running ns query (%v)\", *ns) \n } else {", "msg": "Don't create resource directories if not querying for resources\nOrdering issue in the logic caused the paths to be created even\nif the queries were noops. This adds an empty-check before\ncreating the directories at all.\nRelated to"}
{"diff": "a / cmd/sonobuoy/app/images.go \n  b / cmd/sonobuoy/app/images.go \n@@ -59,6 +59,7 @@ func NewCmdImages() *cobra.Command { \n Run: pullImages, \n Args: cobra.ExactArgs(0), \n } \n + AddE2ERegistryConfigFlag(&imagesflags.e2eRegistryConfig, pullCmd.Flags()) \n AddKubeconfigFlag(&imagesflags.kubeconfig, pullCmd.Flags()) \n AddPluginFlag(&imagesflags.plugin, pullCmd.Flags()) \n @@ -166,7 +167,7 @@ func pullImages(cmd *cobra.Command, args []string) { \n os.Exit(1) \n } \n - upstreamImages, err := image.GetImages(defaultE2ERegistries, version) \n + upstreamImages, err := image.GetImages(imagesflags.e2eRegistryConfig, version) \n if err != nil { \n errlog.LogError(errors.Wrap(err, \"couldn't init upstream registry list\")) \n os.Exit(1)", "msg": "Add e2e-repo-config flag to images pull command"}
{"diff": "a / pkg/plugin/aggregation/status.go \n  b / pkg/plugin/aggregation/status.go \n@@ -108,7 +108,7 @@ func (s *Status) updateStatus() error { \n // is returned. \n func GetStatus(client kubernetes.Interface, namespace string) (*Status, error) { \n if _, err := client.CoreV1().Namespaces().Get(namespace, metav1.GetOptions{}); err != nil { \n - return nil, errors.Wrap(err, \"sonobuoy namespace does not exist\") \n + return nil, errors.Wrapf(err, \"failed to get namespace %v\", namespace) \n } \n // Determine sonobuoy pod name", "msg": "Improve error message from GetStatus\nThe GetStatus function assumed that any error when getting a namespace\nwas due to that namespace not existing. Instead state that it was an\nerror when getting the namespace."}
{"diff": "a / pkg/config/config.go \n  b / pkg/config/config.go \n@@ -63,7 +63,7 @@ const ( \n DefaultDNSNamespace = \"kube-system\" \n // DefaultSystemdLogsImage is the URL for the docker image used by the systemd-logs plugin \n - DefaultSystemdLogsImage = \"gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest\" \n + DefaultSystemdLogsImage = \"sonobuoy/systemd-logs:v0.3\" \n ) \n var (", "msg": "Update to use latest systemd-logs image\nThe image is now published to the sonobuoy repo on dockerhub.\nIn addition, the plugin code is now at github.com/vmware-tanzu/sonobuoy-plugins"}
{"diff": "a / pkg/buildinfo/version.go \n  b / pkg/buildinfo/version.go \n@@ -20,7 +20,7 @@ limitations under the License. \n package buildinfo \n // Version is the current version of Sonobuoy, set by the go linker's -X flag at build time \n -var Version = \"v0.20.0\" \n +var Version = \"v0.50.0\" \n // GitSHA is the actual commit that is being built, set by the go linker's -X flag at build time. \n var GitSHA string", "msg": "Bump sonobuoy version\nThe executable on github is correctly reporting v0.50.0 but not\nmaster. So developer builds are incorrectly showing v0.20.0."}
{"diff": "a / cmd/sonobuoy/app/version.go \n  b / cmd/sonobuoy/app/version.go \n@@ -18,6 +18,7 @@ package app \n import ( \n \"fmt\" \n + \"runtime\" \n \"github.com/pkg/errors\" \n \"github.com/spf13/cobra\" \n @@ -57,6 +58,8 @@ func runVersion(versionflags *versionFlags) func(cmd *cobra.Command, args []stri \n fmt.Printf(\"MinimumKubeVersion: %s\\n\", buildinfo.MinimumKubeVersion) \n fmt.Printf(\"MaximumKubeVersion: %s\\n\", buildinfo.MaximumKubeVersion) \n fmt.Printf(\"GitSHA: %s\\n\", buildinfo.GitSHA) \n + fmt.Printf(\"GoVersion: %s\\n\", runtime.Version()) \n + fmt.Printf(\"Platform: %s/%s\\n\", runtime.GOOS, runtime.GOARCH) \n // Get Kubernetes version, this is last so that the regular version information \n // will be shown even if the API server cannot be contacted and throws an error", "msg": "Improve version subcommand\nadds Go version and platform information to the output"}
{"diff": "a / test/integration/sonobuoy_integration_test.go \n  b / test/integration/sonobuoy_integration_test.go \n@@ -422,6 +422,7 @@ func checkTarballPluginForErrors(t *testing.T, tarball, plugin string, failCount \n } \n func saveToArtifacts(t *testing.T, p string) (newPath string) { \n + p = strings.TrimSpace(p) \n artifactsDir := os.Getenv(\"ARTIFACTS_DIR\") \n if artifactsDir == \"\" { \n t.Logf(\"Skipping saving artifact %v since ARTIFACTS_DIR is unset.\", p) \n @@ -439,7 +440,7 @@ func saveToArtifacts(t *testing.T, p string) (newPath string) { \n var stdout, stderr bytes.Buffer \n // Shell out to `mv` instead of using os.Rename(); the latter caused a problem due to files being on different devices. \n - cmd := exec.CommandContext(context.Background(), bash, \"-c\", fmt.Sprintf(\"mv -r %v %v\", origFile, artifactFile)) \n + cmd := exec.CommandContext(context.Background(), bash, \"-c\", fmt.Sprintf(\"mv %v %v\", origFile, artifactFile)) \n cmd.Stdout = &stdout \n cmd.Stderr = &stderr", "msg": "Remove -r option from mv command\nError with this option causing artifacts not to be moved for CI\nFixes"}
{"diff": "a / cmd/set.go \n  b / cmd/set.go \n@@ -159,13 +159,11 @@ var setCmd = &cobra.Command{ \n } \n } else { \n var vType string \n - if inlineUpdates { \n if len(updateTypes) > i { \n vType = updateTypes[i] \n } else { \n vType = \"json\" \n } \n - } \n switch vType { \n case \"json\": \n buff := new(bytes.Buffer) \n @@ -263,13 +261,11 @@ var setCmd = &cobra.Command{ \n } \n } else { \n var vType string \n - if inlineReplaces { \n if len(replaceTypes) > i { \n vType = replaceTypes[i] \n } else { \n vType = \"json\" \n } \n - } \n switch vType { \n case \"json\": \n buff := new(bytes.Buffer)", "msg": "make json a default type for set update/replace"}
{"diff": "a / cmd/subscribe.go \n  b / cmd/subscribe.go \n@@ -132,7 +132,7 @@ var subscribeCmd = &cobra.Command{ \n } \n switch resp := subscribeRsp.Response.(type) { \n case *gnmi.SubscribeResponse_Update: \n - printSubscribeResponse(nil, subscribeRsp) \n + printSubscribeResponse(map[string]interface{}{\"source\": address}, subscribeRsp) \n case *gnmi.SubscribeResponse_SyncResponse: \n logger.Printf(\"received sync response=%+v from %s\\n\", resp.SyncResponse, address) \n if subscReq.GetSubscribe().Mode == gnmi.SubscriptionList_ONCE { \n @@ -162,7 +162,7 @@ var subscribeCmd = &cobra.Command{ \n } \n switch resp := subscribeRsp.Response.(type) { \n case *gnmi.SubscribeResponse_Update: \n - printSubscribeResponse(nil, subscribeRsp) \n + printSubscribeResponse(map[string]interface{}{\"source\": address}, subscribeRsp) \n case *gnmi.SubscribeResponse_SyncResponse: \n fmt.Printf(\"%ssync response: %+v\\n\", printPrefix, resp.SyncResponse) \n }", "msg": "add source field to subscribe output"}
{"diff": "a / cmd/set.go \n  b / cmd/set.go \n@@ -400,7 +400,7 @@ func setRequest(ctx context.Context, req *gnmi.SetRequest, address, username, pa \n } \n lock.Lock() \n defer lock.Unlock() \n - logger.Printf(\"sending gNMI SetRequest: '%s' to %s\", prototext.MarshalOptions{Multiline: false}.Format(req), address) \n + logger.Printf(\"sending gNMI SetRequest: prefix='%v', delete='%v', replace='%v', update='%v', extension='%v' to %s\", req.Prefix, req.Delete, req.Replace, req.Update, req.Extension, address) \n printSetRequest(printPrefix, req) \n response, err := client.Set(nctx, req)", "msg": "reverted back to log using fields instead of proto marshaller"}
{"diff": "a / cmd/root.go \n  b / cmd/root.go \n@@ -357,7 +357,7 @@ func setupCloseHandler(cancelFn context.CancelFunc) { \n signal.Notify(c, os.Interrupt, syscall.SIGTERM, syscall.SIGINT, syscall.SIGKILL) \n go func() { \n sig := <-c \n - fmt.Printf(\"received signal '%s'. terminating...\\n\", sig.String()) \n + fmt.Printf(\"\\nreceived signal '%s'. terminating...\\n\", sig.String()) \n cancelFn() \n os.Exit(0) \n }()", "msg": "added newlined for terminating message"}
{"diff": "a / cmd/root.go \n  b / cmd/root.go \n@@ -79,7 +79,7 @@ var rootCmd = &cobra.Command{ \n logger = log.New(f, \"\", log.LstdFlags|log.Lmicroseconds) \n logger.SetFlags(log.LstdFlags | log.Lmicroseconds) \n if viper.GetBool(\"debug\") { \n - grpclog.SetLogger(logger) \n + grpclog.SetLogger(logger) //lint:ignore SA1019 see https://github.com/karimra/gnmiClient/issues/59 \n } \n }, \n PersistentPostRun: func(cmd *cobra.Command, args []string) {", "msg": "disable staticcheck linter check for SetLoggerV2"}
{"diff": "a / collector/collector.go \n  b / collector/collector.go \n@@ -144,7 +144,7 @@ func (c *Collector) Subscribe(tName string) error { \n if err != nil { \n return err \n } \n - c.Logger.Printf(\"sending gnmi SubscribeRequest: subscribe='%+v', mode='%+v', encoding='%+v', to %s\", \n + c.Logger.Printf(\"sending gNMI SubscribeRequest: subscribe='%+v', mode='%+v', encoding='%+v', to %s\", \n req, req.GetSubscribe().GetMode(), req.GetSubscribe().GetEncoding(), t.Config.Name) \n go t.Subscribe(c.ctx, req, sc.Name) \n }", "msg": "set gnmi->gNMI to make it consistent with other log msgs"}
{"diff": "a / cmd/set.go \n  b / cmd/set.go \n@@ -405,6 +405,10 @@ func createSetRequest() (*gnmi.SetRequest, error) { \n Val: value, \n }) \n } \n + if (len(req.Delete) == 0) && (len(req.Update) == 0) && (len(req.Replace) == 0) { \n + return nil, errors.New(\"no data to populate Set Request with\") \n + } \n + \n return req, nil \n }", "msg": "fix set command should stop if the --update-file references to non existent file"}
{"diff": "a / cmd/set.go \n  b / cmd/set.go \n@@ -265,10 +265,9 @@ func init() { \n } \n func createSetRequest() (*gnmi.SetRequest, error) { \n - prefix := viper.GetString(\"set-prefix\") \n - gnmiPrefix, err := xpath.ToGNMIPath(prefix) \n + gnmiPrefix, err := collector.CreatePrefix(viper.GetString(\"set-prefix\"), viper.GetString(\"set-target\")) \n if err != nil { \n - return nil, err \n + return nil, fmt.Errorf(\"prefix parse error: %v\", err) \n } \n deletes := viper.GetStringSlice(\"set-delete\") \n updates := viper.GetStringSlice(\"set-update\")", "msg": "add prefix with target to createSetRequest func"}
{"diff": "a / cmd/subscribe.go \n  b / cmd/subscribe.go \n@@ -217,9 +217,7 @@ func getOutputs() (map[string][]outputs.Output, error) { \n \"file-type\": \"stdout\", \n \"format\": viper.GetString(\"format\"), \n } \n - stdoutFile := make([]interface{}, 1) \n - stdoutFile[0] = stdoutConfig \n - outDef[\"stdout\"] = stdoutFile \n + outDef[\"stdout\"] = []interface{}{stdoutConfig} \n } \n outputDestinations := make(map[string][]outputs.Output) \n for name, d := range outDef { \n @@ -231,6 +229,10 @@ func getOutputs() (map[string][]outputs.Output, error) { \n case map[string]interface{}: \n if outType, ok := ou[\"type\"]; ok { \n if initalizer, ok := outputs.Outputs[outType.(string)]; ok { \n + format, ok := ou[\"format\"] \n + if !ok || (ok && format == \"\") { \n + ou[\"format\"] = viper.GetString(\"format\") \n + } \n o := initalizer() \n err := o.Init(ou, logger) \n if err != nil {", "msg": "get format fromviper if not set or is an empty string"}
{"diff": "a / collector/msg.go \n  b / collector/msg.go \n@@ -159,7 +159,7 @@ func getValue(updValue *gnmi.TypedValue) (interface{}, error) { \n case *gnmi.TypedValue_AnyVal: \n value = updValue.GetAnyVal() \n } \n - if value == nil { \n + if value == nil && len(jsondata) != 0 { \n err := json.Unmarshal(jsondata, &value) \n if err != nil { \n return nil, err", "msg": "check for byte slice length before unmarshaling"}
{"diff": "a / outputs/influxdb_output/influxdb_output.go \n  b / outputs/influxdb_output/influxdb_output.go \n@@ -194,6 +194,7 @@ func (i *InfluxDBOutput) health(ctx context.Context) { \n } \n func (i *InfluxDBOutput) worker(ctx context.Context, idx int) { \n + for { \n select { \n case <-ctx.Done(): \n i.logger.Printf(\"worker-%d terminating...\", idx) \n @@ -202,3 +203,4 @@ func (i *InfluxDBOutput) worker(ctx context.Context, idx int) { \n i.writer.WritePoint(influxdb2.NewPoint(ev.Name, ev.Tags, ev.Values, time.Unix(0, ev.Timestamp))) \n } \n } \n +}", "msg": "fix influxdb output worker func"}
{"diff": "a / cmd/prompt.go \n  b / cmd/prompt.go \n@@ -3,6 +3,7 @@ package cmd \n import ( \n \"fmt\" \n \"io/ioutil\" \n + \"log\" \n \"os\" \n \"strings\" \n @@ -69,11 +70,17 @@ var promptModeCmd = &cobra.Command{ \n promptHistory = make([]string, 0, 256) \n home, err := homedir.Dir() \n if err != nil { \n - return err \n + if viper.GetBool(\"debug\") { \n + log.Printf(\"failed to get home directory: %v\", err) \n + } \n + return nil \n } \n content, err := ioutil.ReadFile(home + \"/.gnmic.history\") \n if err != nil { \n - return err \n + if viper.GetBool(\"debug\") { \n + log.Printf(\"failed to read history file: %v\", err) \n + } \n + return nil \n } \n history := strings.Split(string(content), \"\\n\") \n for i := range history { \n @@ -81,7 +88,7 @@ var promptModeCmd = &cobra.Command{ \n promptHistory = append(promptHistory, history[i]) \n } \n } \n - return err \n + return nil \n }, \n PostRun: func(cmd *cobra.Command, args []string) { \n cmd.ResetFlags()", "msg": "prompt cmd does not fail if the history file cannot be read"}
{"diff": "a / cmd/subscribe.go \n  b / cmd/subscribe.go \n@@ -314,7 +314,7 @@ func getSubscriptions() (map[string]*collector.SubscriptionConfig, error) { \n } \n if len(paths) > 0 { \n sub := new(collector.SubscriptionConfig) \n - sub.Name = \"default\" \n + sub.Name = fmt.Sprintf(\"default-%d\", time.Now().Unix()) \n sub.Paths = paths \n sub.Prefix = viper.GetString(\"subscribe-prefix\") \n sub.Target = viper.GetString(\"subscribe-target\")", "msg": "change default subscription name to include a timestamp"}
{"diff": "a / cmd/prompt.go \n  b / cmd/prompt.go \n@@ -151,8 +151,6 @@ var promptModeCmd = &cobra.Command{ \n } \n } \n // \n - ctx, cancel := context.WithCancel(context.Background()) \n - defer cancel() \n debug := viper.GetBool(\"debug\") \n targetsConfig, err := createTargets() \n if err != nil { \n @@ -187,7 +185,7 @@ var promptModeCmd = &cobra.Command{ \n coll = collector.NewCollector(cfg, targetsConfig, \n collector.WithDialOptions(createCollectorDialOpts()), \n collector.WithSubscriptions(subscriptionsConfig), \n - collector.WithOutputs(ctx, outs, logger), \n + collector.WithOutputs(context.Background(), outs, logger), \n collector.WithLogger(logger), \n ) \n }", "msg": "use background ctx to initialize coll in prompt cmd"}
{"diff": "a / cmd/prompt.go \n  b / cmd/prompt.go \n@@ -1204,16 +1204,18 @@ func findSuggestions(co cmdPrompt, doc goprompt.Document) []goprompt.Suggest { \n func resolveGlobs(globs []string) ([]string, error) { \n results := make([]string, 0, len(globs)) \n for _, pattern := range globs { \n - if strings.ContainsAny(pattern, `*?[`) { \n + for _, p := range strings.Split(pattern, \",\") { \n + if strings.ContainsAny(p, `*?[`) { \n // is a glob pattern \n - matches, err := filepath.Glob(pattern) \n + matches, err := filepath.Glob(p) \n if err != nil { \n return nil, err \n } \n results = append(results, matches...) \n } else { \n // is not a glob pattern ( file or dir ) \n - results = append(results, pattern) \n + results = append(results, p) \n + } \n } \n } \n return results, nil", "msg": "split read paths by comma in case they were specified in the config file"}
{"diff": "a / collector/collector.go \n  b / collector/collector.go \n@@ -199,12 +199,14 @@ func (c *Collector) InitOutput(ctx context.Context, name string) { \n c.logger.Printf(\"starting output type %s\", outType) \n if initializer, ok := outputs.Outputs[outType.(string)]; ok { \n out := initializer() \n + if c.reg != nil { \n for _, m := range out.Metrics() { \n err := c.reg.Register(m) \n if err != nil { \n c.logger.Printf(\"failed to register output '%s' metric : %v\", name, err) \n } \n } \n + } \n go out.Init(ctx, cfg, c.logger) \n c.Outputs[name] = out \n }", "msg": "register metrics only if the prometheus Registry is initialized"}
{"diff": "a / config/config.go \n  b / config/config.go \n@@ -154,8 +154,8 @@ func (c *Config) Load(file string) error { \n if err != nil { \n return err \n } \n - c.FileConfig.AddConfigPath(home) \n c.FileConfig.AddConfigPath(\".\") \n + c.FileConfig.AddConfigPath(home) \n c.FileConfig.AddConfigPath(xdg.ConfigHome) \n c.FileConfig.AddConfigPath(xdg.ConfigHome + \"/gnmic\") \n c.FileConfig.SetConfigName(configName) \n @@ -174,7 +174,6 @@ func (c *Config) Load(file string) error { \n return nil \n } \n - \n func (c *Config) SetLogger() { \n if c.Globals.LogFile != \"\" { \n f, err := os.OpenFile(c.Globals.LogFile, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666)", "msg": "changed order of config path search\ncheck the local (\".\") directory before the home directory.  this would\nallow for local override in a given project, etc."}
{"diff": "a / app/app.go \n  b / app/app.go \n@@ -139,8 +139,10 @@ func (a *App) PreRun(_ *cobra.Command, args []string) error { \n func (a *App) PrintMsg(address string, msgName string, msg proto.Message) error { \n a.printLock.Lock() \n defer a.printLock.Unlock() \n + if a.Config.PrintRequest { \n fmt.Fprint(os.Stderr, msgName) \n fmt.Fprintln(os.Stderr, \"\") \n + } \n printPrefix := \"\" \n if len(a.Config.TargetsList()) > 1 && !a.Config.NoPrefix { \n printPrefix = fmt.Sprintf(\"[%s] \", address)", "msg": "print the response msg name only if the request is printed"}
{"diff": "a / collector/subscription.go \n  b / collector/subscription.go \n@@ -108,7 +108,7 @@ func (sc *SubscriptionConfig) CreateSubscribeRequest(target string) (*gnmi.Subsc \n subscriptions[i].SampleInterval = uint64(sc.SampleInterval.Nanoseconds()) \n } \n subscriptions[i].SuppressRedundant = sc.SuppressRedundant \n - if subscriptions[i].SuppressRedundant { \n + if subscriptions[i].SuppressRedundant && sc.HeartbeatInterval != nil { \n subscriptions[i].HeartbeatInterval = uint64(sc.HeartbeatInterval.Nanoseconds()) \n } \n }", "msg": "fix subscribe request creation when suppress-redundant is set but not heartbeat-interval"}
{"diff": "a / formatters/event_convert/event_convert.go \n  b / formatters/event_convert/event_convert.go \n@@ -224,7 +224,13 @@ func convertToUint(i interface{}) (uint, error) { \n func convertToFloat(i interface{}) (float64, error) { \n switch i := i.(type) { \n case []uint8: \n + if len(i) == 4 { \n ij := math.Float32frombits(binary.BigEndian.Uint32([]byte(i))) \n + } else if len(i) == 8 { \n + ij := math.Float64frombits(binary.BigEndian.Uint64([]byte(i))) \n + } else { \n + return 0, nil \n + } \n return float64(ij), nil \n case string: \n iv, err := strconv.ParseFloat(i, 64)", "msg": "Add check len for Binary Float on 32 bits and 64 bits"}
{"diff": "a / app/clustering.go \n  b / app/clustering.go \n@@ -19,7 +19,7 @@ import ( \n const ( \n defaultClusterName = \"default-cluster\" \n - retryTimer = 2 * time.Second \n + retryTimer = 10 * time.Second \n lockWaitTime = 100 * time.Millisecond \n apiServiceName = \"gnmic-api\" \n ) \n @@ -132,12 +132,14 @@ START: \n } \n ctx, cancel := context.WithCancel(a.ctx) \n defer cancel() \n + go func() { \n go a.watchMembers(ctx) \n a.Logger.Printf(\"leader waiting %s before dispatching targets\", a.Config.Clustering.LeaderWaitTimer) \n time.Sleep(a.Config.Clustering.LeaderWaitTimer) \n a.Logger.Printf(\"leader done waiting, starting loader and dispatching targets\") \n go a.startLoader(ctx) \n go a.dispatchTargets(ctx) \n + }() \n doneCh, errCh := a.locker.KeepLock(a.ctx, leaderKey) \n select {", "msg": "run cluster leader wait inside a goroutine to avoid consul session expiry"}
{"diff": "a / target/target.go \n  b / target/target.go \n@@ -3,6 +3,7 @@ package target \n import ( \n \"context\" \n \"fmt\" \n + \"os\" \n \"strings\" \n \"sync\" \n @@ -76,6 +77,17 @@ func (t *Target) CreateGNMIClient(ctx context.Context, opts ...grpc.DialOption) \n if err != nil { \n return err \n } \n + \n + // TODO: add flag to enable tls pre master key log dump \n + if true { \n + logPath := t.Config.Name + \".tlskey.log\" \n + w, err := os.Create(logPath) \n + if err != nil { \n + return err \n + } \n + tlsConfig.KeyLogWriter = w \n + } \n + \n tOpts = append(tOpts, grpc.WithTransportCredentials(credentials.NewTLS(tlsConfig))) \n if t.Config.Token != nil && *t.Config.Token != \"\" { \n tOpts = append(tOpts,", "msg": "prototype for tls premaster key logging"}
{"diff": "a / target/target.go \n  b / target/target.go \n@@ -68,7 +68,7 @@ func (t *Target) CreateGNMIClient(ctx context.Context, opts ...grpc.DialOption) \n return err \n } \n opts = append(opts, tOpts...) \n - \n + opts = append(opts, grpc.WithBlock()) \n // create a gRPC connection \n addrs := strings.Split(t.Config.Address, \",\") \n numAddrs := len(addrs)", "msg": "add grpc withBlock option to grpc Dial options"}
{"diff": "a / outputs/influxdb_output/influxdb_output.go \n  b / outputs/influxdb_output/influxdb_output.go \n@@ -36,7 +36,7 @@ const ( \n func init() { \n outputs.Register(\"influxdb\", func() outputs.Output { \n return &InfluxDBOutput{ \n - Cfg: &Config{GnmiCacheConfig: &cache.GnmiCacheConfig{}}, \n + Cfg: &Config{}, \n eventChan: make(chan *formatters.EventMsg), \n reset: make(chan struct{}), \n startSig: make(chan struct{}), \n @@ -151,10 +151,10 @@ func (i *InfluxDBOutput) Init(ctx context.Context, name string, cfg map[string]i \n i.Cfg.HealthCheckPeriod = defaultHealthCheckPeriod \n } \n if i.Cfg.GnmiCacheConfig != nil { \n - i.initCache() \n if i.Cfg.CacheFlushTimer == 0 { \n i.Cfg.CacheFlushTimer = defaultCacheFlushTimer \n } \n + i.initCache() \n } \n iopts := influxdb2.DefaultOptions().", "msg": "minor influx cache bugfixes\nEnsure cache config is nil at init\nSet the cache flush timer prior to cache init"}
{"diff": "a / cmd/nodelink-controller/main.go \n  b / cmd/nodelink-controller/main.go \n@@ -375,7 +375,6 @@ func (c *Controller) processNode(node *corev1.Node) error { \n } \n if matchingMachine == nil { \n - glog.Warning(\"No machine was found for node %q\", node.Name) \n return fmt.Errorf(\"no machine was found for node: %q\", node.Name) \n }", "msg": "Remove printing of Warning message\nWe return an error and the error is printed. Additionally, the Warning\nmessage had little value as it did not indicate which node (by name)\nwas not found. This removes a lot of duplicated output from the logs."}
{"diff": "a / cmd/machine-api-operator/start.go \n  b / cmd/machine-api-operator/start.go \n@@ -13,9 +13,9 @@ import ( \n \"github.com/openshift/machine-api-operator/pkg/metrics\" \n \"github.com/openshift/machine-api-operator/pkg/operator\" \n \"github.com/openshift/machine-api-operator/pkg/version\" \n - \"github.com/spf13/cobra\" \n - \n \"github.com/prometheus/client_golang/prometheus\" \n + \"github.com/prometheus/client_golang/prometheus/promhttp\" \n + \"github.com/spf13/cobra\" \n v1 \"k8s.io/api/core/v1\" \n \"k8s.io/apimachinery/pkg/runtime\" \n \"k8s.io/client-go/kubernetes\" \n @@ -151,8 +151,7 @@ func startMetricsCollectionAndServer(ctx *ControllerContext) { \n func startHTTPMetricServer(metricsPort string) { \n mux := http.NewServeMux() \n - //TODO(vikasc): Use promhttp package for handler. This is Deprecated \n - mux.Handle(\"/metrics\", prometheus.Handler()) \n + mux.Handle(\"/metrics\", promhttp.Handler()) \n server := &http.Server{ \n Addr: metricsPort,", "msg": "Update to use promhttp.Handler() as prometheus.Handler() was dropped support after adding kube 1.18 deps"}
{"diff": "a / pkg/operator/baremetal_config.go \n  b / pkg/operator/baremetal_config.go \n@@ -132,8 +132,12 @@ func getIronicInspectorEndpoint(baremetalConfig BaremetalProvisioningConfig) *st \n } \n func getProvisioningDHCPRange(baremetalConfig BaremetalProvisioningConfig) *string { \n + // When the DHCP server is external, it is OK for the DHCP range in the CR \n + // to be empty. \n if baremetalConfig.ProvisioningDHCPRange != \"\" { \n return &(baremetalConfig.ProvisioningDHCPRange) \n + } else if baremetalConfig.ProvisioningDHCPExternal { \n + return &(baremetalConfig.ProvisioningDHCPRange) \n } \n return nil \n }", "msg": "Fix External DHCP range values for Baremetal configuration\nAllow an empty string to be passed in as DHCP range when the DHCP\nserver is external in a Baremetal IPI installation."}
{"diff": "a / cmd/vsphere/main.go \n  b / cmd/vsphere/main.go \n@@ -45,7 +45,7 @@ func main() { \n leaderElectLeaseDuration := flag.Duration( \n \"leader-elect-lease-duration\", \n - 15*time.Second, \n + 90*time.Second, \n \"The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\", \n )", "msg": "Increate leader election lease time for vsphere\nThe machine-api-controller components are refreshing their lease more\nthan all other components combined. Bringing this to 90s each, will\ndecrease etcd writes at idle."}
{"diff": "a / cmd/machineset/main.go \n  b / cmd/machineset/main.go \n@@ -75,7 +75,7 @@ func main() { \n leaderElectLeaseDuration := flag.Duration( \n \"leader-elect-lease-duration\", \n - 15*time.Second, \n + 90*time.Second, \n \"The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\", \n )", "msg": "Increate leader election lease time for machineset\nThe machine-api-controller components are refreshing their lease more\nthan all other components combined. Bringing this to 90s each, will\ndecrease etcd writes at idle."}
{"diff": "a / pkg/apis/machine/v1beta1/machine_webhook.go \n  b / pkg/apis/machine/v1beta1/machine_webhook.go \n@@ -95,7 +95,7 @@ const ( \n defaultWebhookServiceName = \"machine-api-operator-webhook\" \n defaultWebhookServiceNamespace = \"openshift-machine-api\" \n - defaultUserDataSecret = \"worker-user-data\" \n + defaultUserDataSecret = \"worker-user-data-managed\" \n defaultSecretNamespace = \"openshift-machine-api\" \n // AWS Defaults", "msg": "Update default user data to be worker-user-data-managed\nThis changed the name of the userdata generated by the installer to satisfy MCO ign v2 -> v3 migration and let the new secret be under mco management."}
{"diff": "a / cmd/machineset/main.go \n  b / cmd/machineset/main.go \n@@ -39,6 +39,12 @@ const ( \n defaultWebhookCertdir = \"/etc/machine-api-operator/tls\" \n ) \n +// The default durations for the leader electrion operations. \n +var ( \n + retryPeriod = 30 * time.Second \n + renewDealine = 90 * time.Second \n +) \n + \n func main() { \n flag.Set(\"logtostderr\", \"true\") \n klog.InitFlags(nil) \n @@ -102,6 +108,9 @@ func main() { \n LeaderElectionNamespace: *leaderElectResourceNamespace, \n LeaderElectionID: \"cluster-api-provider-machineset-leader\", \n LeaseDuration: leaderElectLeaseDuration, \n + // Slow the default retry and renew election rate to reduce etcd writes at idle: BZ 1858400 \n + RetryPeriod: &retryPeriod, \n + RenewDeadline: &renewDealine, \n } \n mgr, err := manager.New(cfg, opts)", "msg": "Slow the default lease retry and renew rate for MachineSet controller\nPrevent machine controllers from writing in etcd at idle too often\nby setting 30s retry and 90s deadline on all renewals.\nBZ"}
{"diff": "a / cmd/nodelink-controller/main.go \n  b / cmd/nodelink-controller/main.go \n@@ -15,6 +15,12 @@ import ( \n \"sigs.k8s.io/controller-runtime/pkg/runtime/signals\" \n ) \n +// The default durations for the leader electrion operations. \n +var ( \n + retryPeriod = 30 * time.Second \n + renewDealine = 90 * time.Second \n +) \n + \n func printVersion() { \n klog.Infof(\"Go Version: %s\", runtime.Version()) \n klog.Infof(\"Go OS/Arch: %s/%s\", runtime.GOOS, runtime.GOARCH) \n @@ -65,6 +71,9 @@ func main() { \n LeaderElectionNamespace: *leaderElectResourceNamespace, \n LeaderElectionID: \"cluster-api-provider-nodelink-leader\", \n LeaseDuration: leaderElectLeaseDuration, \n + // Slow the default retry and renew election rate to reduce etcd writes at idle: BZ 1858400 \n + RetryPeriod: &retryPeriod, \n + RenewDeadline: &renewDealine, \n } \n if *watchNamespace != \"\" { \n opts.Namespace = *watchNamespace", "msg": "Slow the default lease retry and renew rate for nodelink controller\nPrevent machine controllers from writing in etcd at idle too often\nby setting 60s retry and delay on all renewals.\nBZ"}
{"diff": "a / pkg/operator/baremetal_pod.go \n  b / pkg/operator/baremetal_pod.go \n@@ -450,6 +450,11 @@ func createContainerMetal3IronicConductor(config *OperatorConfig, baremetalProvi \n Value: ironicUsername, \n }, \n setIronicPassword(\"IRONIC_HTTP_BASIC_PASSWORD\"), \n + { \n + Name: \"INSPECTOR_HTTP_BASIC_USERNAME\", \n + Value: ironicUsername, \n + }, \n + setIronicPassword(\"INSPECTOR_HTTP_BASIC_PASSWORD\"), \n }, \n } \n return container", "msg": "baremetal: Pass Inspector credentials to Ironic pod\nThe ironic image doesn't yet support this, but we can pass them."}
{"diff": "a / pkg/controller/vsphere/reconciler.go \n  b / pkg/controller/vsphere/reconciler.go \n@@ -475,9 +475,10 @@ func clone(s *machineScope) (string, error) { \n vmTemplate, err := s.GetSession().FindVM(*s, \"\", s.providerSpec.Template) \n if err != nil { \n + const multipleFoundMsg = \"multiple templates found, specify one in config\" \n const notFoundMsg = \"template not found, specify valid value\" \n defaultError := fmt.Errorf(\"unable to get template %q: %w\", s.providerSpec.Template, err) \n - return \"\", handleVSphereError(\"\", notFoundMsg, defaultError, err) \n + return \"\", handleVSphereError(multipleFoundMsg, notFoundMsg, defaultError, err) \n } \n var snapshotRef *types.ManagedObjectReference", "msg": "Return a valid error message when multiple templates are found"}
